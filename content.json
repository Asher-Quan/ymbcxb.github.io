{"meta":{"title":"ymbcxb|个人博客","subtitle":null,"description":"专注 Java 开发的技术博客","author":"ymbcxb","url":"http://yoursite.com","root":"/"},"pages":[{"title":"关于","date":"2018-12-22T21:57:25.000Z","updated":"2019-08-19T06:17:34.248Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"archives","date":"2018-12-22T21:59:15.000Z","updated":"2019-08-19T01:21:42.559Z","comments":true,"path":"archives/index.html","permalink":"http://yoursite.com/archives/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-08-18T12:15:43.284Z","updated":"2019-08-18T12:15:43.284Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-08-18T12:15:43.290Z","updated":"2019-08-18T12:15:43.290Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"JVM总结","slug":"JVM总结","date":"2019-12-19T09:28:43.000Z","updated":"2020-01-11T12:34:35.063Z","comments":true,"path":"2019/12/19/JVM总结/","link":"","permalink":"http://yoursite.com/2019/12/19/JVM总结/","excerpt":"","text":"谈谈你对Java的理解Java有两个显著的特性： 平台无关性，也就是一次编译，多处运行 通过垃圾回收机制管理内存的分配和回收 还有一些别的特性，语言特性：泛型，反射 类库丰富：网络编程，集合，并发 Java如何实现平台无关Java分为编译期和解释期 首先将Java文件编译成.class文件，这个class文件则可以在多个平台下解释运行 现在JDK的有一个名叫JIT（Just in time）的编译器,这个编译器在运行期间会判断是否存在热点代码，如果有热点代码则使用JIT进行编译处理，而不是解释处理 JVM的三种启动方式： -Xint：只进行解释执行，不执行编译，屏蔽JIT -Xcomp：关闭解释器，只进行编译处理 AOT：在运行前直接编译成机器码，避免JIT预热 JVM如何加载.class文件 JVM大致分为四部分： ClassLoader：依据特定格式加载class文件到内存，满足格式则可以加载 运行时数据区：Java虚拟机在运行程序的时候的数据区域 Execution Engine：对命令进行解析，提交给操作系统执行 Native Interface ： 融合不同开发语言的原生库为Java使用 Java的反射Java的反射机制是在运行状态中，对于任意一个类，都能够知道类的所有属性和方法，对于任意一个对象，都能够调用它的任意方法和属性，这种动态获取信息以及调用对象的方法的功能称为Java语言的反射机制。 ClassLoaderClassLoader的主要工作是在Class装载的时候，从系统外部获得Class的二进制数据流，所有的Class都是由ClassLoader进行加载的，ClassLoader负责通过将Class文件里的二进制数据流装载进系统，然后交给Java虚拟机进行连接，初始化等操作。 ClassLoader的种类： BoostrapClassLoader：C++编写，加载核心库java.* ExtClassLoader：Java编写，加载扩展库 javax.* AppClassLoader：Java编写，加载程序的所在目录 双亲委派机制 为什么要使用双亲委派机制去加载类，因为这样子做可以避免多份同样字节码的加载，比如我们手写一个String类，也是不会被加载的，String这个类会被BoostrapClassLoader加载 loadClass和forName的区别首先类的装载过程如下： 加载：将字节码加载到内存 校验：分析是否存在不满足Java规范 准备：将类的变量即静态变量分配内存并设置类变量的初始值（举个例子 public static int i = 3; 在这个过程之后赋值成0，使用初始值） 解析：将虚拟机常量池的符号引用替换为直接引用（举个例子,int b = a; 这个可以理解为符号引用，直接引用是什么意思呢，将这种符号转换成对应的内存地址） 初始化：执行类构造器方法(区分开构造器方法)，(举个例子，上面在准备的阶段，将i的值赋值为0，此时才赋值为3) 其中 2，3，4的三个步骤也叫做链接 Class.forName得到的class是已经初始化完成的 Classloader.loadClass得到的class是还没有链接 元空间和永久代的区别JDK8开始把类的元信息存放空间叫做元空间，JDK8以前，这个存储空间叫做永久代 元空间和永久代都是属于方法区的实现，实现方法不同，方法区是Java虚拟机的一种规范 元空间使用的是本地内存，而永久代使用的是JVM内存 使用永久代的缺点： 字符串常量池存在永久代中，容易出现性能问题和内存溢出 类和方法的信息大小难易确定，给永久代的大小指定带来了困难 永久代会为GC带来不必要的复杂性 方便HotSpot与其他JVM的集成","categories":[{"name":"Java虚拟机","slug":"Java虚拟机","permalink":"http://yoursite.com/categories/Java虚拟机/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}]},{"title":"设计模式-装饰者模式","slug":"设计模式-装饰者模式","date":"2019-11-26T11:23:38.000Z","updated":"2019-11-26T17:07:06.133Z","comments":true,"path":"2019/11/26/设计模式-装饰者模式/","link":"","permalink":"http://yoursite.com/2019/11/26/设计模式-装饰者模式/","excerpt":"","text":"装饰者模式在不改变原有对象的基础之上，将功能附加到对象，提供了比继承更具有弹性 Demo： 1234//产品public interface Product &#123; public void produce();&#125; 1234567//蛋糕public class Cake implements Product &#123; @Override public void produce() &#123; System.out.println(\"普通的蛋糕\"); &#125;&#125; 1234567891011121314//装饰者public class Decorator implements Product &#123; protected Product product; public Decorator(Product product)&#123; this.product = product; &#125; @Override public void produce() &#123; product.produce(); &#125;&#125; 123456789101112//颜色public class Color extends Decorator &#123; public Color(Product product) &#123; super(product); &#125; @Override public void produce()&#123; System.out.println(\"上颜色\"); product.produce(); &#125;&#125; 123456//测试类public class TestDemo &#123; public static void main(String[] args) &#123; new Color(new Cake()).produce(); &#125;&#125; 当我需要添加新的装饰类的时候，比如说，水果的装饰： 123456789101112//水果public class Fruit extends Decorator &#123; public Fruit(Product product) &#123; super(product); &#125; @Override public void produce() &#123; System.out.println(\"添加水果\"); product.produce(); &#125;&#125; 123456//测试类public class TestDemo &#123; public static void main(String[] args) &#123; new Fruit(new Color(new Cake())).produce(); &#125;&#125; UML类图： 装饰者模式使用的场景： 扩展一个类的功能或给一个类添加附加职责 动态的给一个对象添加功能，这些功能可以再动态的撤销 优点： 比继承灵活，不改变原有对象的情况下给一个对象扩展功能 通过使用不同装饰类以及装饰类的 排列组合可以实现不同的效果 符合开闭原则 缺点： 会出现更多的类，增加程序的复杂性 动态装饰与多层装饰会更加复杂","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"设计模式-外观模式","slug":"设计模式-外观模式","date":"2019-11-18T16:42:14.000Z","updated":"2019-11-26T03:21:00.984Z","comments":true,"path":"2019/11/19/设计模式-外观模式/","link":"","permalink":"http://yoursite.com/2019/11/19/设计模式-外观模式/","excerpt":"","text":"外观模式外观模式又叫做门面模式，提供了 一个统一的接口，用来访问子系统中的一群接口 外观模式定义了一个高层接口，让子系统更容易使用 Demo： 123456//物流系统public class Logistics &#123; public void send()&#123; System.out.println(\"发出快递\"); &#125;&#125; 123456//支付系统public class Pay &#123; public void pay()&#123; System.out.println(\"支付\"); &#125;&#125; 123456//产品系统public class Product &#123; public void produce()&#123; System.out.println(\"生产产品\"); &#125;&#125; 123456789101112//测试类public class TestDemo &#123; public static void main(String[] args) &#123; Product product = new Product(); Pay pay = new Pay(); Logistics logistics = new Logistics(); product.produce(); pay.pay(); logistics.send(); &#125;&#125; 像这样子使用，特别不好，我每次购买一个商品，调用者做了很多个步骤，然而这些步骤进行封装的，其次，如果购买商品的逻辑进行一个增加，需要在每一个调用的时候都重改代码，比较麻烦 其实门面模式，就是把子系统进行一个封装，放在一个更大的系统里面，比如，我这里新建一个Mall 123456789101112//商城类public class Mall &#123; private Product product = new Product(); private Pay pay = new Pay(); private Logistics logistics = new Logistics(); public void buy()&#123; product.produce(); pay.pay(); logistics.send(); &#125;&#125; 1234567//测试类public class TestDemo &#123; public static void main(String[] args) &#123; Mall mall = new Mall(); mall.buy(); &#125;&#125; 调用者不再关心细节的问题，而关心我需要做的那件事 UML类图 外观模式的优点： 简化了调用过程 减少系统依赖 符合迪米特法则 缺点： 增加子系统，容易引入风险 不符合开闭原则","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"Netty(一）","slug":"Netty(一)","date":"2019-09-10T20:09:14.000Z","updated":"2019-09-17T08:56:34.458Z","comments":true,"path":"2019/09/11/Netty(一)/","link":"","permalink":"http://yoursite.com/2019/09/11/Netty(一)/","excerpt":"","text":"Netty初识简单来说：Netty就是一个框架，为了使我们开发网络编程的时候可以更加简单 接下来介绍传统和Netty的对比，首先上场的是我们传统的网络编程 BIO (Blocking IO)BIO是一种同步阻塞的编程方式，由服务端开启线程来管理socket连接，优点就是简单 在JDK1.4之前通常都是这样子使用 缺点是支持的并发量不大，浪费线程资源 图示： 代码： 12345678910111213public class Client &#123; public static void main(String[] args) throws IOException &#123; Socket socket = new Socket(\"127.0.0.1\",8080); while (true)&#123; Scanner scanner = new Scanner(System.in); String content = scanner.nextLine(); socket.getOutputStream().write(content.getBytes()); if (\"exit\".equals(content))&#123; socket.close(); &#125; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031public class BioServer &#123; public static void main(String[] args) throws IOException &#123; ServerSocket socket = new ServerSocket(); socket.bind(new InetSocketAddress(8080)); while(true)&#123; //阻塞 Socket accept = socket.accept(); new HandlerThread(accept).start(); &#125; &#125; static class HandlerThread extends Thread&#123; private Socket accept; public HandlerThread(Socket accept)&#123; this.accept = accept; &#125; @Override public void run() &#123; try &#123; while (true)&#123; byte[] bytes = new byte[1024]; //阻塞 accept.getInputStream().read(bytes); System.out.println(new String(bytes)); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; NIO (New IO)NIO是同步非阻塞的IO，JDK1.4后开始支持 这里大概说一下思路：解决非阻塞的问题，主要在两个，将两个阻塞的问题解决就可以了 NIO的核心是，将连接注册到多路复用器，然后轮询多路复用器是否有可用的连接，有则派线程来执行 所以线程准确来说是处理一个可用的（活跃的）Socket，大大提升效率 缺点： 当并发请求数量增大的时候，后端负载增大，虽然是非阻塞，但是是同步的，每个请求的等待时间就很长了 图示： 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class NioServer &#123; public static void main(String[] args) throws IOException &#123; ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.bind(new InetSocketAddress(8080)); //非阻塞 serverSocketChannel.configureBlocking(false); //开启多路复用器 Selector selector = Selector.open(); // 注册，并标记当前服务通道状态 /* * register(Selector, int) * int - 状态编码 * OP_ACCEPT ： 连接成功的标记位。 * OP_READ ： 可以读取数据的标记 * OP_WRITE ： 可以写入数据的标记 * OP_CONNECT ： 连接建立后的标记 */ SelectionKey selectionKey = serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); while (true)&#123; // 阻塞方法，当至少一个通道被选中，此方法返回。 // 通道是否可选择，由注册到多路复用器中的通道标记决定 int count = selector.select(); if(count &gt; 0)&#123; Iterator&lt;SelectionKey&gt; iterator = selector.selectedKeys().iterator(); while (iterator.hasNext())&#123; SelectionKey key = iterator.next(); iterator.remove(); //判断通道是否合法 if(key.isValid())&#123; if(key.isAcceptable())&#123; ServerSocketChannel socketChannel = (ServerSocketChannel) key.channel(); SocketChannel accept = socketChannel.accept(); accept.configureBlocking(false); accept.register(selector,SelectionKey.OP_READ); &#125; //判断通道是否可读 if(key.isReadable())&#123; byteBuffer.clear(); SocketChannel socketChannel = (SocketChannel) key.channel(); int readLength = socketChannel.read(byteBuffer); if(readLength == -1)&#123; key.channel().close(); key.channel(); return; &#125; byteBuffer.flip(); String content = Charset.forName(\"utf-8\").decode(byteBuffer).toString(); System.out.println(content); &#125; &#125; &#125; &#125; &#125; &#125;&#125; AIO（Asynchronous IO）异步非阻塞的IO，JDK1.7后开始支持 与NIO不同，read和write的操作都是异步的，当完成后，会主动调用回调函数， 服务器实现模式为一个有效请求一个线程，客户端的 I/O 请求都是由 OS先完成了再通知服务器应用去启动线程进行处理 图示： 代码： 12345678910111213141516171819public class AioServer &#123; private AsynchronousServerSocketChannel channel; public static void main(String[] args) throws IOException, InterruptedException &#123; new AioServer().Aio(); &#125; public void Aio() throws IOException, InterruptedException &#123; ExecutorService executorService = Executors.newFixedThreadPool(6); channel = AsynchronousServerSocketChannel.open(); channel.bind(new InetSocketAddress(8080)); channel.accept(this,new AioServerHandler()); Thread.sleep(Integer.MAX_VALUE); &#125; public AsynchronousServerSocketChannel getChannel() &#123; return channel; &#125;&#125; 1234567891011121314151617181920212223242526272829public class AioServerHandler implements CompletionHandler&lt;AsynchronousSocketChannel, AioServer&gt; &#123; @Override public void completed(AsynchronousSocketChannel result, AioServer attachment) &#123; attachment.getChannel().accept(attachment,this); doRead(result); &#125; private void doRead(AsynchronousSocketChannel channel) &#123; ByteBuffer byteBuffer = ByteBuffer.allocate(1024); channel.read(byteBuffer, byteBuffer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() &#123; @Override public void completed(Integer result, ByteBuffer attachment) &#123; attachment.flip(); System.out.println(Charset.forName(\"utf-8\").decode(byteBuffer).toString()); doRead(channel); &#125; @Override public void failed(Throwable exc, ByteBuffer attachment) &#123; exc.printStackTrace(); &#125; &#125;); &#125; @Override public void failed(Throwable exc, AioServer attachment) &#123; exc.printStackTrace(); &#125;&#125; 接下来就使用Netty来开发BIO和NIO BIO（Netty版本）1234567891011121314151617181920212223242526public class BioServer &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; EventLoopGroup group = new OioEventLoopGroup(); try&#123; ServerBootstrap bootstrap = new ServerBootstrap(); bootstrap.group(group).channel(OioServerSocketChannel.class).localAddress(new InetSocketAddress(8080)) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel socketChannel) &#123; socketChannel.pipeline().addLast(new ChannelInboundHandlerAdapter()&#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf in = (ByteBuf)msg; System.out.println(\"Server received: \" + in.toString(CharsetUtil.UTF_8)); ctx.write(in); &#125; &#125;); &#125; &#125;); ChannelFuture f = bootstrap.bind().sync(); f.channel().closeFuture().sync(); &#125;finally &#123; group.shutdownGracefully().sync(); &#125; &#125;&#125; NIO (Netty版本)12345678910111213141516171819202122232425262728public class NioServer &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; //修改处1 NioEventLoopGroup EventLoopGroup group = new NioEventLoopGroup(); try&#123; ServerBootstrap bootstrap = new ServerBootstrap(); //修改处2 NioServerSocketChannel bootstrap.group(group).channel(NioServerSocketChannel.class).localAddress(new InetSocketAddress(8080)) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel socketChannel) &#123; socketChannel.pipeline().addLast(new ChannelInboundHandlerAdapter()&#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf in = (ByteBuf)msg; System.out.println(\"Server received: \" + in.toString(CharsetUtil.UTF_8)); ctx.write(in); &#125; &#125;); &#125; &#125;); ChannelFuture f = bootstrap.bind().sync(); f.channel().closeFuture().sync(); &#125;finally &#123; group.shutdownGracefully().sync(); &#125; &#125;&#125; 你会发现一个问题：就是从BIO和NIO的之间切换很简单的，这就是Netty封装后的效果","categories":[{"name":"Netty","slug":"Netty","permalink":"http://yoursite.com/categories/Netty/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"http://yoursite.com/tags/Netty/"}]},{"title":"Synchronized详解","slug":"Synchronized详解","date":"2019-09-03T14:28:44.000Z","updated":"2019-09-06T13:18:18.719Z","comments":true,"path":"2019/09/03/Synchronized详解/","link":"","permalink":"http://yoursite.com/2019/09/03/Synchronized详解/","excerpt":"","text":"Synchronized详解先看一个程序： 12345678910111213public class T03 &#123; private static int count = 100; public void consume()&#123; if(count &gt; 0) count--; &#125; public static void main(String[] args) throws InterruptedException &#123; T03 t = new T03(); for (int i = 0; i &lt; 1000; i++) &#123; new Thread(()-&gt;t.consume()).start(); &#125; &#125;&#125; 出现的结果，是不符合预期的 这就是多线程下产生的问题，解决的方案也很简单，加锁，这里讲解Synchronized 123456//将方法改成这样子就可以了public void consume()&#123; synchronized (this)&#123; if(count &gt; 0) count--; &#125;&#125; 加锁的目的：使得临界资源被线程同步互斥访问，就是某一个时刻只允许一个线程访问资源 重要的是下面： 深入字节码javap -c T03.class 可以查看一下字节码指令，我之前收录了一下 https://blog.csdn.net/qq_36457148/article/details/100521282 monitorenter 进入并获得对象监视器。 monitorexit 释放并退出对象监视器。 如果你使用的是同步方法，就是将synchronized修饰在方法上的，对应的是invokevirtual 所以说Synchronized是通过Monitor来实现锁的 JVM对象加锁原理首先引入的是 对象的内存布局 其中对象头是比较重要的：这里再贴张图描述一下对象头 关于对象头我们可以通过代码验证一下 引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jol&lt;/groupId&gt; &lt;artifactId&gt;jol-core&lt;/artifactId&gt; &lt;version&gt;0.7&lt;/version&gt;&lt;/dependency&gt; 1234567ClassLayout.parseInstance(new Object()).toPrintable()//查看对象的布局OFFSET SIZE TYPE DESCRIPTION VALUE //00011010中最后的10代表锁的标记位 0 4 (object header) 1a 05 7f 1a (00011010 00000101 01111111 00011010) (444531994) 4 4 (object header) 00 00 00 00 (00000000 00000000 00000000 00000000) (0) 8 4 (object header) 05 c1 00 f8 (00000101 11000001 00000000 11111000) (-134168315) 12 4 (loss due to the next object alignment) 以下是HotSpot虚拟机的描述 12345678910111213// 32 bits:// --------// hash:25 ------------&gt;| age:4 biased_lock:1 lock:2 (normal object)// JavaThread*:23 epoch:2 age:4 biased_lock:1 lock:2 (biased object)// size:32 ------------------------------------------&gt;| (CMS free block)// PromotedObject*:29 ----------&gt;| promo_bits:3 -----&gt;| (CMS promoted object)//// 64 bits:// --------// unused:25 hash:31 --&gt;| unused:1 age:4 biased_lock:1 lock:2 (normal object)// JavaThread*:54 epoch:2 unused:1 age:4 biased_lock:1 lock:2 (biased object)// PromotedObject*:61 ---------------------&gt;| promo_bits:3 -----&gt;| (CMS promoted object)// size:64 -----------------------------------------------------&gt;| (CMS free block) 这里要注意一下，Java默认是使用大端模式，所谓的大端模式（Big-endian），是指数据的高字节，保存在内存的低地址中，而数据的低字节，保存在内存的高地址中，这样的存储模式有点儿类似于把数据当作字符串顺序处理：地址由小向大增加，而数据从高位往低位放 就是最后1个字节，其实打印出来应该是放在第一个字节 锁的状态锁的状态有四种 无锁 偏向锁，只有一个线程的情况下 轻量级锁，多线程下竞争不激烈的情况下 重量级锁，多线程竞争的情况下 随着场景的不同，锁的状态是会发生变化的 根据上述对象头的最后三位就能确定现在是有锁还是无锁 如果是有锁是什么状态的锁，都是更具对象投来进行判断的 我上一张图来描述一下锁转换的过程 如何验证图中所说的，我们可以结合上述打印对象头来确认 123456789101112131415161718192021222324252627public class T04 &#123; public synchronized void test01()&#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"hello\"); &#125; public static void main(String[] args) throws InterruptedException &#123; T04 t = new T04(); //无锁 System.out.println(ClassLayout.parseInstance(t).toPrintable()); Thread.sleep(6000); new Thread(t::test01).start(); //轻量锁 Thread.sleep(200); System.out.println(ClassLayout.parseInstance(t).toPrintable()); Thread.sleep(6000); for (int i = 0; i &lt; 3; i++) &#123; new Thread(t::test01).start(); &#125; //重量级锁 Thread.sleep(500); System.out.println(ClassLayout.parseInstance(t).toPrintable()); &#125;&#125; 这里面缺失了个偏向锁，也不太清楚原因 为什么要设计锁的升级过程synchronized轻量级锁没有使用互斥量，如果学过操作系统，应该能够理解互斥量的概念，使用互斥量是比较重的一个操作，有一个上下文切换，系统的内核态和用户态的切换，比较重 CAS指令在Synchronized里面设计了很多CAS（Compare-and-Swap） 这里稍微介绍一下，CAS的作用是用来保证原子性，保证该操作不会被打断 在使用上，通常会记录下某块内存中的旧值，通过对旧值进行一系列的操作后得到新值，然后通过CAS操作将新值与旧值进行交换。如果这块内存的值在这期间内没被修改过，则旧值会与内存中的数据相同，这时CAS操作将会成功执行使内存中的数据变为新值。如果内存中的值在这期间内被修改过，则一般来说旧值会与内存中的数据不同，这时CAS操作将会失败，新值将不会被写入内存 这里用自己的话说一下，A的值为1，在主内存当中，将A的值设置为2的时候，设置一个副本记录A修改前的值（这里叫A~），如果A~与主内存的值，就将A的值去进行覆盖，否则则不覆盖 CAS指令很简单，但是会有以下三个问题 循环时间长开销很大 如果CAS指令判断失败，会继续尝试，增加CPU的负担 只能保证一个共享变量的原子操作 ABA问题 举个例子，A的值是1，修改成2再修改成1，用CAS判断是误以为没有其余线程修改过内存的值，其实是已经被修改过了 ABA导致的问题，引入维基百科的例子： 12345 top | V 0x0014| Node A | --&gt; | Node X | --&gt; …… 有一个堆(先入后出)中有top和节点A，节点A目前位于堆顶top指针指向A。现在有一个进程P1想要pop一个节点，因此按照如下无锁操作进行 12345678pop()&#123; do&#123; ptr = top; // ptr = top = NodeA next_prt = top-&gt;next; // next_ptr = NodeX &#125; while(CAS(top, ptr, next_ptr) != true); return ptr; &#125; 而进程P2在执行CAS操作之前打断了P1，并对堆进行了一系列的pop和push操作，使堆变为如下结构： 12345 top | V 0x0014| Node C | --&gt; | Node B | --&gt; | Node X | --&gt; …… 进程P2首先pop出NodeA，之后又Push了两个NodeB和C，由于内存管理机制中广泛使用的内存重用机制，导致NodeC的地址与之前的NodeA一致。 这时P1又开始继续运行，在执行CAS操作时，由于top依旧指向的是NodeA的地址(实际上已经变为NodeC)，因此将top的值修改为了NodeX，这时堆结构如下： 1234 top | 0x0014 V| Node C | --&gt; | Node B | --&gt; | Node X | --&gt; …… 经过CAS操作后，top指针错误的指向了NodeX而不是NodeB。","categories":[{"name":"并发","slug":"并发","permalink":"http://yoursite.com/categories/并发/"}],"tags":[{"name":"并发","slug":"并发","permalink":"http://yoursite.com/tags/并发/"}]},{"title":"Volatile详解","slug":"Volatile详解","date":"2019-08-27T20:04:13.000Z","updated":"2019-09-03T06:33:52.534Z","comments":true,"path":"2019/08/28/Volatile详解/","link":"","permalink":"http://yoursite.com/2019/08/28/Volatile详解/","excerpt":"","text":"在介绍Volatile之前，我们要来先看一段程序(抛砖引玉),最好自己跑一遍程序 12345678910111213141516public class T01 &#123; public static void main(String[] args) throws InterruptedException &#123; MyThread myThread = new MyThread(); myThread.start(); Thread.sleep(1000); myThread.flag = false; &#125;&#125;class MyThread extends Thread&#123; public boolean flag = true; @Override public void run() &#123; while (flag)&#123; &#125; System.out.println(\"end\"); &#125;&#125; 测试结果是不会输出end 很奇怪吧，不符合逻辑，接下来我们就引入我们的第一个知识点： Java内存模型 简单描述，就是每个线程会有一个私有的本地内存，存放主内存的副本，这样子可以加快线程读的速度 那么线程之间如何进行通信呢？引出我们第二个知识点 Java线程之间的通信 关于主内存与工作内存之间的具体交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步到主内存之间的实现细节，Java内存模型定义了以下八种操作以及八种规则来完成 这里面其实会有个隐含的问题，假设线程A修改一个值后存进内存，B线程同时去读主内存会发生什么呢？ 1.A修改执行assign操作 2.B读执行read操作和load操作 3.A存进内存执行store操作和write操作 数据不一致 解决这个问题我们很自然能够想到的就是：锁 由于锁是相对比较重量级，能不能不用锁呢？这里就到第三个知识点了： VolatileVolatile有一个特性：可见性 什么是可见性，简单来说当一个线程修改了一个值，这个值对别的线程都是可以看见的，去读的时候都能读到这个新的值 为什么能够保证可见性 第四个知识点： Happens-before原则 Two actions can be ordered by a happens-before relationship. If one action happens-before another, then the first is visible to and ordered before the second. If we have two actions x and y, we write hb(x, y) to indicate that x happens-before y —Java8官方文档 Java8关于happens-before文档：https://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html#jls-17.4.5 如果一个事件的发生在另一个事件之前，其结果必须反应出来，记住这句话就好了 举个例子 12int a = 1;//事件1int b = a + 1;//事件2 事件1发生在事件2之前，所以其结果是什么？a=1,b=2，,不管如何优化,这个结果一定要反映出来，所以一定要先执行1后执行2才可以有这个结果 举个反例 12int a = 1;//事件1int b = 2;//事件2 事件1发生在事件2之前，所以其结果是什么？a=1,b=2,这个结果一定要反映出来,此时就有两种可能，基于优化的考虑，我可能先执行1后执行2，或者反过来，其结果都是不会发生改变的 在官方文档里面有描述happens before遵循的规则 其中一条是： A write to a volatile field (§8.3.1.4) happens-before every subsequent read of that field 对于一个volatile字段进行写操作会 happens-before 任意操作 讲到这里我们再去扒一扒底层，字节码？不，是汇编 我这里将程序修改成如下 123456789101112131415161718192021//java -server -Xcomp -XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly -XX:CompileCommand=compileonly,*T01.setData//只查看setData的汇编public class T01 &#123; public static MyThread myThread = new MyThread(); public static void main(String[] args) throws InterruptedException &#123; myThread.start(); Thread.sleep(1000); setData(); &#125; public static void setData()&#123; myThread.flag = false; &#125;&#125;class MyThread extends Thread&#123; public volatile boolean flag = true; @Override public void run() &#123; while (flag)&#123;&#125; System.out.println(\"end\"); &#125;&#125; 输出的结果如下： 120x0000000002c9dbc9: lock add dword ptr [rsp],0h ;*putfield flag ; - example.t09.T01::setData@4 (line 21) 这句话的意思呢，是指使得本CPU的Cache写入了内存，该写入动作也会引起别的CPU失效其Cache，所以通过这样一个操作，可让前面volatile变量的修改对其他CPU立即可见 简单来说： 锁内存 写内存 让其余的缓存该值失效 讲到这里就是为了讲解volatile有可见性 返回到我们的第一个程序为什么会死循环，也是因为一个线程的修改不能使得另一个线程立刻知道，所以才会导致死循环 但是严谨来说其实是因为JIT编译的优化，大概是觉得一直循环读值，所以就优化成直接从缓存取值，所以才会导致这种问题 大家可以试试 1java -Djava.compiler=NONE &lt;class&gt; 这样子其实也不会有问题的，但是少了JIT优化，还是不太好的 只需要加上volatile修饰就可以解决问题了 但是volatile不单纯是解决可见性问题 第五个知识点： 指令重排指令重排就是为了提升程序执行的效率，会按照一定的规则将指令的顺序进行重排，但是不能影响语义 比如说 123int a = 1;int b = 2;int c = a+b; 前两条指令是可以重排的，无论谁先都可以，第三条指令必须在这两条指令的后面，因为第三条指令会依赖前两条指令，所以第三条指令是不会被重排的 来看一段程序，参考《Java并发编程实战》 1234567891011121314151617public class T02 &#123; private static boolean ready = true; private static int number; private static class MyThread extends Thread&#123; @Override public void run() &#123; while (ready)&#123;Thread.yield(); &#125; System.out.println(number); &#125; &#125; public static void main(String[] args)&#123; new MyThread().start(); number = 42; ready = false; &#125;&#125; 看似结果输出是42，但是实则不然，答案应该有两个，42或者0 为什么会输出0，这就是因为指令重排，number = 42 和 ready = false 没有依赖关系，所以这两条指令有可能会互换位置（编译器基于性能的考虑），假设互换位置后，则输出的结果就应该是0了 其实还有另一种可能就是死循环了，上面也提到了，这是可见性问题 volatile的第二个作用就是可以防止指令重排 内存屏障：使CPU或编译器对屏障指令之前和之后发出的内存操作执行一个排序约束。 这通常意味着在屏障之前发布的操作被保证在屏障之后发布的操作之前执行 内存屏障可以被分为以下几种类型： LoadLoad屏障：对于这样的语句Load1; LoadLoad; Load2，在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。 StoreStore屏障：对于这样的语句Store1; StoreStore; Store2，在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见。 LoadStore屏障：对于这样的语句Load1; LoadStore; Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。 StoreLoad屏障：对于这样的语句Store1; StoreLoad; Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。它的开销是四种屏障中最大的。在大多数处理器的实现中，这个屏障是个万能屏障，兼具其它三种内存屏障的功能。 很多文章都有说到内存屏障但是都没有出处，我也找了很久终于找到了： http://gee.cs.oswego.edu/dl/jmm/cookbook.html 在一个变量被volatile修饰后，JVM会为我们做两件事： 1.在每个volatile写操作前插入StoreStore屏障，在写操作后插入StoreLoad屏障 2.在每个volatile读操作前插入LoadLoad屏障，在读操作后插入LoadStore屏障 贴一张图来说明一下 最后一个特性是： double和long的非原子性处理 某些 Java内存模型实现可能发现将对 64 位 long 或 double 值的写操作分成两次相邻的 32位值写操作更方便。为了效率起见，这种行为是实现可以自行决定的。JavaTM 虚拟机可以自由地决定是原子性的对待 long 和 double 值的写操作还是一分为二的对待。鉴于本内存模型的目的，我们将对非 volatile long 或 double 值的单次写操作视作两次分开的写操作：每次 32 位。这可能会导致一种情况，某个线程会看到某次写操作中 64 位的前 32 位，以及另外一次写操作的后 32 位。读写 volatile 的 long 和double 总是原子的。读写引用也总是原子的，而不管引用的实现采用的是 32 位还是 64 位。我们鼓励 VM 的实现者尽可能避免将 64 位值的写操作分开。鼓励编码人员将共享的 64 位值声明为 volaitle 的或将其程序正确同步以避免可能的并发问题","categories":[{"name":"并发","slug":"并发","permalink":"http://yoursite.com/categories/并发/"}],"tags":[{"name":"并发","slug":"并发","permalink":"http://yoursite.com/tags/并发/"}]},{"title":"JVM---字节码指令","slug":"JVM---字节码指令操作","date":"2019-08-22T12:25:33.000Z","updated":"2019-08-25T01:10:46.135Z","comments":true,"path":"2019/08/22/JVM---字节码指令操作/","link":"","permalink":"http://yoursite.com/2019/08/22/JVM---字节码指令操作/","excerpt":"","text":"123456iconst_m1 将int型的m1推送至栈顶iload_m 将指定的第m个int型本地变量推送至栈顶以上使用i来代表intd:double,f:floata:Reference,l:longps:当int 取值-1~5 采用iconst 指令，取值-128~127 采用bipush 指令，取值-32768~32767 采用sipush 指令，取值-2147483648~2147483647 采用ldc 指令 上一个Demo： 12345678public class Demo &#123; public static void main(String[] args) &#123; int i = 11; int j = 12; int c = i + j; System.out.println(c); &#125;&#125; 12使用javap来分解一下javap -verbose Demo.class &gt; Demo.txt 打开Demo.txt,截取一部分 JVM执行字节码指令的时候基于栈结构的 我们重点看的是Code后面的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//常量池Constant pool: #1 = Methodref #5.#14 // java/lang/Object.\"&lt;init&gt;\":()V #2 = Fieldref #15.#16 // java/lang/System.out:Ljava/io/PrintStream; #3 = Methodref #17.#18 // java/io/PrintStream.println:(I)V #4 = Class #19 // Demo #5 = Class #20 // java/lang/Object #6 = Utf8 &lt;init&gt; #7 = Utf8 ()V ......&#123; public Demo(); descriptor: ()V flags: ACC_PUBLIC Code: //操作数栈最大深度，本地变量表最大长度（64位操作系统为2其余为1），参数个数 stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.\"&lt;init&gt;\":()V 4: return LineNumberTable: line 7: 0 public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC //字节码 Code: stack=2, locals=4, args_size=1 0: bipush 11 //将11入栈 2: istore_1 //取出栈顶元素存进本地变量1 3: bipush 12 //将12入栈 5: istore_2 //取出栈顶元素存进本地变量2 6: iload_1 //把本地变量1入栈 7: iload_2 //把本地变量2入栈 8: iadd //把栈顶取两个元素进行加法，进行入栈 9: istore_3 //取出栈顶元素存进本地变量3 10: getstatic #2 //调用System.out // Field java/lang/System.out:Ljava/io/PrintStream; 13: iload_3 //把本地变量3入栈 14: invokevirtual #3//print语句 // Method java/io/PrintStream.println:(I)V 17: return //返回值 //行号表，第一个数字代表代码的行号，第二个数字代表字节码的行号 LineNumberTable: line 9: 0 line 10: 3 line 11: 6 line 12: 10 line 13: 17&#125;","categories":[{"name":"Java虚拟机","slug":"Java虚拟机","permalink":"http://yoursite.com/categories/Java虚拟机/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}]},{"title":"JVM---bTrace","slug":"bTrace","date":"2019-08-21T17:35:09.000Z","updated":"2019-08-25T01:03:27.164Z","comments":true,"path":"2019/08/22/bTrace/","link":"","permalink":"http://yoursite.com/2019/08/22/bTrace/","excerpt":"","text":"BTraceBTrace可用于动态跟踪正在运行的Java程序（类似于OpenSolaris应用程序和操作系统的DTrace）。BTrace动态检测目标应用程序的类以注入跟踪代码（“字节码跟踪”） 不用修改源代码，既可以追踪Java程序 如果使用远程Debug，会对线上的项目产生巨大影响 编写BTrace 脚本时，也有如下约定： 不能创建对象，数组 不能throw，catch 异常 不能有循环(for，while，do..while) 不能实现接口 不能有同步块及同步方法 不能有断言语句 不能有外部，内部，嵌套 或本地类 不能进行任何实例好或静态方法调用，只能从com.sun.btrace.BtraceUtils类的公共静态方法 …… 下载地址：https://github.com/btraceio/btrace/releases/tag/v1.3.11.3 下载Btrace对应的版本后进行解压 在环境变量中自行配置（我这里写的是Windows下） BTRACE_HOME=bTrace的安装路径 %BTRACE_HOME%\\bin 简单使用demo： 12345678910111213public class BTraceDemo &#123; //public static void test02(User user)&#123;&#125; 在打印复杂类型的例子里开启 public static String test01(String name) throws InterruptedException &#123; new User(name); test02(user); //int i = 1 / 0;在Kind.THROW的例子里开启 return name; &#125; public static void main(String[] args) throws Exception &#123; TimeUnit.SECONDS.sleep(15); System.out.println(test01(\"aaa\")); &#125;&#125; 12345678910111213@BTracepublic class BTraceMonitor &#123; @OnMethod( clazz = \"BTraceDemo\",//类名 method = \"test01\",//方法名 location = @Location(Kind.ENTRY)//在方法进入的时候进行拦截 ) public static void test(@ProbeClassName String className, @ProbeMethodName String methodName, AnyType args[])&#123; BTraceUtils.println(\"className: \"+className);//打印类名 BTraceUtils.println(\"methodName: \"+methodName);//打印方法名 BTraceUtils.printArray(args);//打印参数 &#125;&#125; 一：首先运行BTraceDemo 然后使用jps - l查询到pid，进行追踪 一般为了在IDE工具上打bTrace代码更加方便，通常需要引入一下jar包 贴一下我的项目结构： Maven的依赖： 1234567891011121314151617181920212223&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.sun.tools.btrace&lt;/groupId&gt; &lt;artifactId&gt;btrace-boot&lt;/artifactId&gt; &lt;version&gt;1.3.11.3&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;basedir&#125;/src/main/resources/lib/btrace-boot.jar&lt;/systemPath&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.sun.tools.btrace&lt;/groupId&gt; &lt;artifactId&gt;btrace-client&lt;/artifactId&gt; &lt;version&gt;1.3.11.3&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;basedir&#125;/src/main/resources/lib/btrace-boot.jar&lt;/systemPath&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.sun.tools.btrace&lt;/groupId&gt; &lt;artifactId&gt;btrace-agent&lt;/artifactId&gt; &lt;version&gt;1.3.11.3&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;basedir&#125;/src/main/resources/lib/btrace-boot.jar&lt;/systemPath&gt; &lt;/dependency&gt; &lt;/dependencies&gt; btrace的一些用法：拦截方法：普通方法:@OnMethod(clazz=””,method=””) 上面的Demo已经写过了 构造函数:@OnMethod(clazz=””,method=”“) 1234567@BTracepublic class BTraceMonitor &#123; @OnMethod(clazz = \"User\",method = \"&lt;init&gt;\",location = @Location(Kind.ENTRY)) public static void test(@ProbeClassName String className, @ProbeMethodName String methodName, String name)&#123; BTraceUtils.print(name); &#125;&#125; 123456public class User &#123; private String name; public User(String name)&#123; this.name = name; &#125;&#125; 拦截时机1234Kind.ENTRY ：入口Kind.RETURN：返回Kind.THROW：异常Kind.Line：行 Kind.ENTRY上面已经演示过了 Kind.RETURN1234567@BTracepublic class BTraceMonitor &#123; @OnMethod(clazz = \"BTraceDemo\", method = \"test01\", location = @Location(Kind.RETURN)) public static void test(@ProbeClassName String className, @ProbeMethodName String methodName,@Return String value)&#123; BTraceUtils.print(value); &#125;&#125; Kind.THROW1234567891011121314151617181920212223242526272829@BTracepublic class BTraceMonitor &#123; @TLS static Throwable currentException; @OnMethod(clazz=\"java.lang.Throwable\", method=\"&lt;init&gt;\") public static void onthrow(@Self Throwable self) &#123; //new Throwable() currentException = self; &#125; @OnMethod(clazz=\"java.lang.Throwable\", method=\"&lt;init&gt;\") public static void onthrow1(@Self Throwable self, String s) &#123; //new Throwable(String msg) currentException = self; &#125; @OnMethod(clazz=\"java.lang.Throwable\", method=\"&lt;init&gt;\") public static void onthrow1(@Self Throwable self, String s, Throwable cause) &#123; //new Throwable(String msg, Throwable cause) currentException = self; &#125; @OnMethod(clazz=\"java.lang.Throwable\", method=\"&lt;init&gt;\") public static void onthrow2(@Self Throwable self, Throwable cause) &#123; //new Throwable(Throwable cause) currentException = self; &#125; @OnMethod(clazz=\"java.lang.Throwable\", method=\"&lt;init&gt;\", location=@Location(Kind.RETURN)) public static void onthrowreturn() &#123; if (currentException != null) &#123; BTraceUtils.Threads.jstack(currentException); BTraceUtils.println(\"=====================\"); currentException = null; &#125; &#125;&#125; Kind.LINE 1234567@BTracepublic class BTraceMonitor &#123; @OnMethod(clazz = \"BTraceDemo\", method = \"test01\", location = @Location(value = Kind.LINE,line = N//自己写自己要监控的行数)) public static void test(int line)&#123; BTraceUtils.print(line); &#125;&#125; 打印复杂类型12345678910@BTracepublic class BTraceMonitor &#123; @OnMethod(clazz = \"BTraceDemo\", method = \"test02\", location = @Location(value = Kind.ENTRY)) public static void test(User user)&#123; BTraceUtils.printFields(user); Field field = BTraceUtils.field(\"User\", \"name\"); BTraceUtils.printFields(BTraceUtils.get(field,user)); &#125;&#125;//如果报错找不到User类则 执行脚本修改为：bTrace -cp \"字节码存放的路径\" pid BTraceMonitor.java 注意BTrace对字节码的修改不可逆 默认只能监控本地Java进程 如果想要监控远程Java进程，需要修改源代码","categories":[{"name":"Java虚拟机","slug":"Java虚拟机","permalink":"http://yoursite.com/categories/Java虚拟机/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}]},{"title":"Spring Security源码分析(一)","slug":"SpringSecurity---源码分析(一)","date":"2019-08-20T00:49:27.000Z","updated":"2019-08-25T13:24:09.497Z","comments":true,"path":"2019/08/20/SpringSecurity---源码分析(一)/","link":"","permalink":"http://yoursite.com/2019/08/20/SpringSecurity---源码分析(一)/","excerpt":"","text":"Spring Security源码分析(一)SpringSecurity基本原理是基于一个过滤器链我们启动一个SpringSecurity项目，日志会打印过滤器 一共有十二个过滤器，大致介绍一下 WebAsyncManagerIntegrationFilter将Security上下文与Spring Web中用于处理异步请求映射的 WebAsyncManager 进行集成 SecurityContextPersistenceFilter 两个主要职责：请求来临时，创建SecurityContext安全上下文信息，请求结束时清空SecurityContextHolder。 HeaderWriterFilter (文档中并未介绍，非核心过滤器) 用来给http响应添加一些Header,比如X-Frame-Options, X-XSS-Protection*，X-Content-Type-Options. CsrfFilter 在spring4这个版本中被默认开启的一个过滤器，用于防止csrf攻击 LogoutFilter 顾名思义，处理注销的过滤器UsernamePasswordAuthenticationFilter 这个会重点分析，表单提交了username和password，被封装成token进行一系列的认证，便是主要通过这个过滤器完成的，在表单认证的方法中，这是最最关键的过滤器。 RequestCacheAwareFilter (文档中并未介绍，非核心过滤器) 内部维护了一个RequestCache，用于缓存request请求 SecurityContextHolderAwareRequestFilter 此过滤器对ServletRequest进行了一次包装，使得request具有更加丰富的API AnonymousAuthenticationFilter 匿名身份过滤器 UsernamePasswordAuthenticationFilter 用户名密码过滤器，检验用户名和密码的 SessionManagementFilter 是和session相关的过滤器，内部维护了一个SessionAuthenticationStrategy，两者组合使用，常用来防止session-fixation protection attack，以及限制同一用户开启多个会话的数量 ExceptionTranslationFilter 直译成异常翻译过滤器，还是比较形象的，这个过滤器本身不处理异常，而是将认证过程中出现的异常交给内部维护的一些类去处理，具体是那些类下面详细介绍 FilterSecurityInterceptor 这个过滤器决定了访问特定路径应该具备的权限，访问的用户的角色，权限是什么？访问的路径需要什么样的角色和权限？这些判断和处理都是由该类进行的。 最常用的就是UsernamePasswordAuthenticationFilter 了接下来我们就对UsernamePasswordAuthenticationFilter进行一个源码分析 UsernamePasswordAuthenticationFilter核心方法:attemptAuthentication做了以下几件事情 判断是不是post请求，不是则抛异常 获取用户名和密码，判空 构建UsernamePasswordAuthenticationToken（将权限和用户名和密码进行一个封装） 执行setDetails，将请求的信息存入token里 通过AuthenticationManager的authenticate()去进行校验 1234567891011121314151617181920public Authentication attemptAuthentication(HttpServletRequest request,HttpServletResponse response) throws AuthenticationException &#123; if (postOnly &amp;&amp; !request.getMethod().equals(\"POST\")) &#123; throw new AuthenticationServiceException( \"Authentication method not supported: \" + request.getMethod()); &#125; String username = obtainUsername(request); String password = obtainPassword(request); if (username == null) &#123; username = \"\"; &#125; if (password == null) &#123; password = \"\"; &#125; username = username.trim(); UsernamePasswordAuthenticationToken authRequest = new UsernamePasswordAuthenticationToken( username, password); // Allow subclasses to set the \"details\" property setDetails(request, authRequest); return this.getAuthenticationManager().authenticate(authRequest); &#125; 然而AuthenticationManager是一个接口，对应的实现类是ProviderManager,查看以下authenticate的实现方法 ProviderManager 中的List，会依照次序去认证，认证成功则立即返回，若认证失败则返回null，下一个AuthenticationProvider会继续尝试认证，如果所有认证器都无法认证成功，则ProviderManager 会抛出一个ProviderNotFoundException异常。 12345678910111213141516public Authentication authenticate(Authentication authentication)throws AuthenticationException &#123; ...... for (AuthenticationProvider provider : getProviders()) &#123; if (!provider.supports(toTest)) &#123; continue; &#125; try &#123; result = provider.authenticate(authentication); if (result != null) &#123; copyDetails(authentication, result); break; &#125; &#125; &#125;&#125; 所以实现校验的过程的是AuthenticationProvider的实现类，比如使用表单验证的情况下，默认是使用DaoAuthenticationProviderDaoAuthenticationProvider继承了AbstractUserDetailsAuthenticationProvider，authenticate方法是在AbstractUserDetailsAuthenticationProviderauthenticate方法做了什么呢？ 123456789101112//1.从缓存里取UserDetails信息UserDetails user = this.userCache.getUserFromCache(username);//2.如果缓存没有则通过UserDetailsService去调取loadUserByUsername获取UserDeatails对象user = retrieveUser(username,(UsernamePasswordAuthenticationToken) authentication);//3.预检查,对UserDetails进行提前的检查，判断是否锁定，是否过期，是否可用preAuthenticationChecks.check(user);//4.附加检查,判断是否有证书和是否有用passwordEncoder加密additionalAuthenticationChecks(user,(UsernamePasswordAuthenticationToken) authentication);//5.最后检查,判断证书是否过期postAuthenticationChecks.check(user);//6.返回认证成功的信息return createSuccessAuthentication(principalToReturn, authentication, user); createSuccessAuthentication(principalToReturn, authentication, user)源码： 重写组装了一个UsernamePasswordAuthenticationToken，和第一次的组装不一样的地方在于 添加了权限和设置了已验证的信息 12345678910//第一次的组装super(null);this.principal = principal;this.credentials = credentials;setAuthenticated(false);//第二次的组装super(authorities);this.principal = principal;this.credentials = credentials;super.setAuthenticated(true); // must use super, as we override 12345678910111213protected Authentication createSuccessAuthentication(Object principal, Authentication authentication, UserDetails user) &#123; // Ensure we return the original credentials the user supplied, // so subsequent attempts are successful even with encoded passwords. // Also ensure we return the original getDetails(), so that future // authentication events after cache expiry contain the details UsernamePasswordAuthenticationToken result = new UsernamePasswordAuthenticationToken( principal, authentication.getCredentials(), authoritiesMapper.mapAuthorities(user.getAuthorities())); result.setDetails(authentication.getDetails()); return result; &#125; 整个过程 一旦任何一处发送异常，都会被捕获AbstractAuthenticationProcessingFilter类里面捕获 异常则调用unsuccessfulAuthentication() failureHandler.onAuthenticationFailure(request, response, failed);会调用自己写的failureHandler 成功则调用successfulAuthentication(), successHandler.onAuthenticationSuccess(request, response, authResult);会调用自己写的successHandler 12345678910111213141516171819202122232425262728293031try &#123; //这里是UsernamePasswordAuthenticationFilter的最后返回值 authResult = attemptAuthentication(request, response); if (authResult == null) &#123; // return immediately as subclass has indicated that it hasn't completed // authentication return; &#125; sessionStrategy.onAuthentication(authResult, request, response);&#125;catch (InternalAuthenticationServiceException failed) &#123; logger.error( \"An internal error occurred while trying to authenticate the user.\", failed); unsuccessfulAuthentication(request, response, failed); return;&#125;catch (AuthenticationException failed) &#123; // Authentication failed unsuccessfulAuthentication(request, response, failed); return;&#125;// Authentication successif (continueChainBeforeSuccessfulAuthentication) &#123; chain.doFilter(request, response);&#125;//调用登陆成功的处理器successfulAuthentication(request, response, chain, authResult);","categories":[{"name":"Spring Security","slug":"Spring-Security","permalink":"http://yoursite.com/categories/Spring-Security/"}],"tags":[{"name":"源码分析","slug":"源码分析","permalink":"http://yoursite.com/tags/源码分析/"}]},{"title":"JVM---内存溢出","slug":"JVM---内存溢出","date":"2019-08-05T16:28:57.000Z","updated":"2019-08-25T01:00:30.513Z","comments":true,"path":"2019/08/06/JVM---内存溢出/","link":"","permalink":"http://yoursite.com/2019/08/06/JVM---内存溢出/","excerpt":"","text":"Java虚拟机结构模型： 先普及几个指令 12-Xmx等价于-XX:InitialHeapSize 初始化堆内存-Xms等价于-XX:MaxHeapSize 最大堆内存 程序计数器此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域 Java虚拟机栈如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常 123456789101112131415//-Xmx1M -Xms1Mpublic class JVMStackOOM extends Thread &#123; @Override public void run()&#123; byte[]arr = new byte[1024]; run(); &#125; public static void main(String[] args) &#123; while (true)&#123; new JVMStackOOM().start(); &#125; &#125;&#125;//\"Thread-135\" Exception in thread \"Thread-130\" java.lang.OutOfMemoryError: Java heap space 注明：单线程下无论是栈帧太大还是虚拟机容量太小，都是抛出StackOverflowError 本地方法栈与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常 堆如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。 12345678910// -Xmx32M -Xms32Mpublic class HeapOOM &#123; public static void main(String[] args) &#123; List&lt;HeapOOM&gt; list = new ArrayList&lt;&gt;(); while(true)&#123; list.add(new HeapOOM()); &#125; &#125;&#125;//Exception in thread \"main\" java.lang.OutOfMemoryError: Java heap space 方法区根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常 这里使用的JDK8，JDK8以前是永久代，JDK8是元空间 12345678910111213141516171819/** * JDK8:-XX:MetaspaceSize=8m -XX:MaxMetaspaceSize=8m * JDK7以下:-XX:PermSize=8m -XX:MaxPermSize=8m * 非堆内存溢出 */public class noHeapOOM &#123; public static void main(String[] args) &#123; while (true) &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(OOMObject.class); enhancer.setUseCache(false); enhancer.setCallback((MethodInterceptor) (obj, method, args1, proxy) -&gt; proxy.invokeSuper(obj, args1)); //无限创建动态代理，生成Class对象 enhancer.create(); &#125; &#125; static class OOMObject &#123; &#125;&#125; 解决内存溢出的办法1.导出内存映像文件 1-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=./ 1使用jmap -dump:format=b,file=heap.prof pid (pid可以用jps查看)(这个方法只能记录当前的堆栈信息) 2.使用MAT分析内存溢出下载链接： https://www.eclipse.org/mat/downloads.php 打开软件左上角File—&gt;Open Heap Dump—&gt;选择对应的hprof文件 怀疑的一个问题：主线程占了百分之97.3的内存，内存在“Java.Lang.Objult[]”的一个实例中累积 如果觉得不够清楚可以接着看别的地方： 这里可以看得更加清楚是哪个位置的哪个类的哪个变量引起的内存溢出","categories":[{"name":"Java虚拟机","slug":"Java虚拟机","permalink":"http://yoursite.com/categories/Java虚拟机/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}]},{"title":"JVM---jstack","slug":"JVM---jstack","date":"2019-08-05T16:27:15.000Z","updated":"2019-08-25T01:00:23.851Z","comments":true,"path":"2019/08/06/JVM---jstack/","link":"","permalink":"http://yoursite.com/2019/08/06/JVM---jstack/","excerpt":"","text":"JVM之Jstack使用jstack是jdk自带的线程堆栈分析工具，使用该命令可以查看或导出Java 应用程序中线程堆栈信息 首先需要了解以下线程的几个状态 解决CPU飙高问题12345678910111213public class Demo &#123; private static int i = 0; private static void test()&#123; while (true)&#123; i++; &#125; &#125; public static void main(String[] args) &#123; new Thread(() -&gt; test()).start(); new Thread(() -&gt; test()).start(); &#125;&#125; top指令能够实时显示系统中各个进程的资源占用状况 运行前的情况 运行后的情况 执行jstack命令 1jstack 4468 &gt; 4468.txt 查看该Pid下的线程 1top -p 4468 -H 发现有两个线程的CPU利用率比较高 下载下来4468.txt查看一下 怎么去定位到我们的线程呢？ 我们刚才可以查看到有两个线程的CPU利用率比较高分别的pid对应为4479和4478 要转为16进制，分别为：117f 和 117e 在4468.txt查看这两个数字去进行定位到该线程 根据Demo.test(Demo.java:12)可以明白问题的原因 解决死锁问题12345678910111213141516171819202122232425262728293031public class Demo &#123; private Object object1 = new Object(); private Object object2 = new Object(); public void test01()&#123; synchronized (object1)&#123; try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (object2)&#123; &#125; &#125; &#125; public void test02()&#123; synchronized (object2)&#123; try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (object1)&#123; &#125; &#125; &#125; public static void main(String[] args) &#123; Demo demo = new Demo(); new Thread(() -&gt; demo.test01()).start(); new Thread(() -&gt; demo.test02()).start(); &#125;&#125; 方法还是一样，我们直接来看生成的txt文件 查看txt的末尾 找到一个死锁问题，并且对应的类和方法都写得很清楚，很容易就能排除这个问题","categories":[{"name":"Java虚拟机","slug":"Java虚拟机","permalink":"http://yoursite.com/categories/Java虚拟机/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}]},{"title":"HashMap源码分析","slug":"HashMap源码分析","date":"2019-08-03T22:58:38.000Z","updated":"2019-08-25T00:59:48.946Z","comments":true,"path":"2019/08/04/HashMap源码分析/","link":"","permalink":"http://yoursite.com/2019/08/04/HashMap源码分析/","excerpt":"","text":"HashMap源码分析最简单的一个例子 12Map&lt;String,String&gt; map = new HashMap&lt;String,String&gt;();map.put(&quot;a&quot;,&quot;a&quot;); 从构造函数开始分析： 1234static final float DEFAULT_LOAD_FACTOR = 0.75f;public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125; 接下来执行put函数 123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; 先看一下hash函数做了什么 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 分析下hash算法(h = key.hashCode()) ^ (h &gt;&gt;&gt; 16） hashCode的值是32位二进制 12345678假设h得到的值是： 10010001 10010101 10110000 11110001右移16位得到的值是: 00000000 00000000 10010001 10010101异或(同为0,不同为1)： 10010001 10010101 00100001 01100100假设h得到的值是： 10010001 00000000 00000000 00000000右移16位得到的值是: 00000000 00000000 00000000 10010001异或(同为0,不同为1)： 10010001 00000000 00000000 10010001 通过这种方式，可以提高1的分布变得相对均匀一些 为什么要变得相对均匀呢?我们接着看 查看putVal的函数 由于代码比较长，我这里分段来显示且直接在代码里进行解释 12345678910final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; //定义了一个Node数组，Node这个类里面包含了四个属性，hash值，Key，Value，指向下一个Node的指针 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //对table进行一个判空操作，table是一个Node数组 if ((tab = table) == null || (n = tab.length) == 0) //如果为空则走resize() n = (tab = resize()).length; ......（省略其余代码） &#125; resize()函数 1234567891011121314151617181920212223242526final Node&lt;K,V&gt;[] resize() &#123; //将table赋值给oldTab Node&lt;K,V&gt;[] oldTab = table; //oldCap为0 int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //目前oldCap为0 ....（省略部分代码） else &#123; //赋值新的空间和新的阈值 //static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 //DEFAULT_LOAD_FACTOR = 0.75f //新的空间是16，新的阈值是12 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; ....（省略部分代码） threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) //申请16大小的空间 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; ....（省略部分代码） return newTab; &#125; 回到putVal函数 此时n得到的值为16 1234....（省略部分代码）//进行取余if ((p = tab[i = (n - 1) &amp; hash]) == null)....（省略部分代码） tab[i = (n - 1) &amp; hash]单独抽出来解释 1234n = 16 二进制码: 0001 0000n-1=15： 0000 1111假设hash值是: 0000 0001取余运算后的结果是： 0000 0001 当n是2的幂次方，减一后再去取余一定能保证，前面的都是0 这里就可以解决了一个问题，节约内存 而且进行与h操作的速度会很快 上面提到过为什么hash值要尽可能地均匀，如果尽可能地均匀，在这里进行取余运算的时候，hash冲突会降低 继续看源码回到putVal函数 12345678910111213141516171819202122if ((p = tab[i = (n - 1) &amp; hash]) == null) //如果不为空，就插值 tab[i] = newNode(hash, key, value, null);else&#123; //产生了Hash冲突 Node&lt;K,V&gt; e; K k; //判断hash值，key，是否一致 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); ....（省略部分代码） //如果一致，将原先的key对应的值做一个替换 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125;&#125; 继续看源码回到putVal函数 假设hash值，key不是一致，另外一种冲突怎么办呢？ 123456789101112131415161718 //判断p是不是树形节点，至于TreeNode是什么呢？就是红黑树 else if (p instanceof TreeNode) //如果是树形节点的，就按照红黑树的方式，插入节点 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value);else &#123;//如果不是树形节点,以链表的方式，插入节点//这里有一个binCount计数，计算这个链表的长度 for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); //如果这个链表的长度大于8-1 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st //接下来我们来看下treeifyBin treeifyBin(tab, hash); break; &#125; ....（省略部分代码）&#125; 接下来我们来看下treeifyBin() 123456789101112final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; //判断是否为空和容量是不是太小 if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) //如果是，则要重新调整容量 resize(); //否则 else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; //将单链表转换成红黑树 ....（省略部分代码） &#125; &#125; 再回到我们的putVal函数 12345678910111213if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue;&#125;//修改的数量++modCount;//判断size（实际容量）是否大于阈值，初始值为12if (++size &gt; threshold) resize();afterNodeInsertion(evict); 再次回到resize() 12345678910111213141516171819202122232425262728final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; //判断是否大于最大值，若已经大于最大值，则将阈值设置为Integer的最大值 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; //否则，将阈值扩大一倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; ....（省略部分代码） threshold = newThr; //生成新得一个容量数组 @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; //将oldTab的节点插入到newTab中 if (oldTab != null) &#123; ....（省略部分代码） &#125; return newTab;&#125; 至于get和remove方法在这里不过多讲解，相对于set的构建过程比较简单 总结 HashMap包括了数组，链表，红黑树 数组容量为2的幂： 提高运算速度 增加散列度，降低冲突 减少内存碎片 hash函数：hashcode的高16位和低16位进行异或求模，增加散列度，降低冲突 插入冲突：通过单链表解决冲突，如果链表长度超过8，进行链表和红黑树的转换，以提高查询速度 扩容的条件：实际节点数大于容量的四分之三，扩容后进行数据排布","categories":[{"name":"技术分享","slug":"技术分享","permalink":"http://yoursite.com/categories/技术分享/"}],"tags":[{"name":"源码分析","slug":"源码分析","permalink":"http://yoursite.com/tags/源码分析/"}]},{"title":"SpringBoot整合支付宝-当面付(二)","slug":"SpringBoot整合支付宝-当面付(二)","date":"2019-08-02T09:51:10.000Z","updated":"2019-08-25T00:59:38.117Z","comments":true,"path":"2019/08/02/SpringBoot整合支付宝-当面付(二)/","link":"","permalink":"http://yoursite.com/2019/08/02/SpringBoot整合支付宝-当面付(二)/","excerpt":"","text":"在上一篇简单将一个支付宝Demo跑起来后，现在需要整合到我们的springBoot项目，虽然说是整合SpringBoot，但是，你SSM也都是可以的 我先简单写一个SpringBoot的Demo maven导入了一些与alipay相关的其余依赖 1234567891011121314151617181920212223242526272829303132333435&lt;dependency&gt; &lt;groupId&gt;commons-codec&lt;/groupId&gt; &lt;artifactId&gt;commons-codec&lt;/artifactId&gt; &lt;version&gt;1.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-configuration&lt;/groupId&gt; &lt;artifactId&gt;commons-configuration&lt;/artifactId&gt; &lt;version&gt;1.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-lang&lt;/groupId&gt; &lt;artifactId&gt;commons-lang&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.zxing&lt;/groupId&gt; &lt;artifactId&gt;core&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hamcrest&lt;/groupId&gt; &lt;artifactId&gt;hamcrest-core&lt;/artifactId&gt; &lt;version&gt;1.3&lt;/version&gt; &lt;/dependency&gt; 这里注意一下： 支付宝给的Demo的Gson是2.3.1版本，我这里使用SpringBoot2.x的时候需要修改为2.6版本以上 主要是在··AlipayController里面 其实就是把支付宝的Demo里面的test_trade_precreate()函数引入进来而已 这里为了防止小伙伴出错，我这里贴一下我的代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151package com.example.demo.controller;import com.alipay.api.AlipayResponse;import com.alipay.api.response.AlipayTradePrecreateResponse;import com.alipay.demo.trade.config.Configs;import com.alipay.demo.trade.model.ExtendParams;import com.alipay.demo.trade.model.GoodsDetail;import com.alipay.demo.trade.model.builder.AlipayTradePrecreateRequestBuilder;import com.alipay.demo.trade.model.result.AlipayF2FPrecreateResult;import com.alipay.demo.trade.service.AlipayTradeService;import com.alipay.demo.trade.service.impl.AlipayTradeServiceImpl;import com.alipay.demo.trade.utils.ZxingUtils;import org.apache.commons.lang.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import java.util.ArrayList;import java.util.List;/** * @author ymbcxb * @title * @Package com.example.demo * @date 2019/8/2 10:10 */@RestControllerpublic class AlipayController &#123; private static AlipayTradeService tradeService; private static final Logger log = LoggerFactory.getLogger(AlipayController.class); @RequestMapping(&quot;/pay&quot;) public String pay()&#123; test_trade_precreate(); return &quot;ok&quot;; &#125; static &#123; /** 一定要在创建AlipayTradeService之前调用Configs.init()设置默认参数 * Configs会读取classpath下的zfbinfo.properties文件配置信息，如果找不到该文件则确认该文件是否在classpath目录 */ Configs.init(&quot;zfbinfo.properties&quot;); /** 使用Configs提供的默认参数 * AlipayTradeService可以使用单例或者为静态成员对象，不需要反复new */ tradeService = new AlipayTradeServiceImpl.ClientBuilder().build(); &#125; public void test_trade_precreate() &#123; // (必填) 商户网站订单系统中唯一订单号，64个字符以内，只能包含字母、数字、下划线， // 需保证商户系统端不能重复，建议通过数据库sequence生成， String outTradeNo = &quot;tradeprecreate&quot; + System.currentTimeMillis() + (long) (Math.random() * 10000000L); // (必填) 订单标题，粗略描述用户的支付目的。如“xxx品牌xxx门店当面付扫码消费” String subject = &quot;xxx品牌xxx门店当面付扫码消费&quot;; // (必填) 订单总金额，单位为元，不能超过1亿元 // 如果同时传入了【打折金额】,【不可打折金额】,【订单总金额】三者,则必须满足如下条件:【订单总金额】=【打折金额】+【不可打折金额】 String totalAmount = &quot;0.01&quot;; // (可选) 订单不可打折金额，可以配合商家平台配置折扣活动，如果酒水不参与打折，则将对应金额填写至此字段 // 如果该值未传入,但传入了【订单总金额】,【打折金额】,则该值默认为【订单总金额】-【打折金额】 String undiscountableAmount = &quot;0&quot;; // 卖家支付宝账号ID，用于支持一个签约账号下支持打款到不同的收款账号，(打款到sellerId对应的支付宝账号) // 如果该字段为空，则默认为与支付宝签约的商户的PID，也就是appid对应的PID String sellerId = &quot;&quot;; // 订单描述，可以对交易或商品进行一个详细地描述，比如填写&quot;购买商品2件共15.00元&quot; String body = &quot;购买商品3件共20.00元&quot;; // 商户操作员编号，添加此参数可以为商户操作员做销售统计 String operatorId = &quot;test_operator_id&quot;; // (必填) 商户门店编号，通过门店号和商家后台可以配置精准到门店的折扣信息，详询支付宝技术支持 String storeId = &quot;test_store_id&quot;; // 业务扩展参数，目前可添加由支付宝分配的系统商编号(通过setSysServiceProviderId方法)，详情请咨询支付宝技术支持 ExtendParams extendParams = new ExtendParams(); extendParams.setSysServiceProviderId(&quot;2088100200300400500&quot;); // 支付超时，定义为120分钟 String timeoutExpress = &quot;120m&quot;; // 商品明细列表，需填写购买商品详细信息， List&lt;GoodsDetail&gt; goodsDetailList = new ArrayList&lt;GoodsDetail&gt;(); // 创建一个商品信息，参数含义分别为商品id（使用国标）、名称、单价（单位为分）、数量，如果需要添加商品类别，详见GoodsDetail GoodsDetail goods1 = GoodsDetail.newInstance(&quot;goods_id001&quot;, &quot;xxx小面包&quot;, 1000, 1); // 创建好一个商品后添加至商品明细列表 goodsDetailList.add(goods1); // 继续创建并添加第一条商品信息，用户购买的产品为“黑人牙刷”，单价为5.00元，购买了两件 GoodsDetail goods2 = GoodsDetail.newInstance(&quot;goods_id002&quot;, &quot;xxx牙刷&quot;, 500, 2); goodsDetailList.add(goods2); // 创建扫码支付请求builder，设置请求参数 AlipayTradePrecreateRequestBuilder builder = new AlipayTradePrecreateRequestBuilder() .setSubject(subject).setTotalAmount(totalAmount).setOutTradeNo(outTradeNo) .setUndiscountableAmount(undiscountableAmount).setSellerId(sellerId).setBody(body) .setOperatorId(operatorId).setStoreId(storeId).setExtendParams(extendParams) .setTimeoutExpress(timeoutExpress) // .setNotifyUrl(&quot;http://www.test-notify-url.com&quot;)//支付宝服务器主动通知商户服务器里指定的页面http路径,根据需要设置 .setGoodsDetailList(goodsDetailList); AlipayF2FPrecreateResult result = tradeService.tradePrecreate(builder); switch (result.getTradeStatus()) &#123; case SUCCESS: log.info(&quot;支付宝预下单成功: )&quot;); AlipayTradePrecreateResponse response = result.getResponse(); dumpResponse(response); // 需要修改为运行机器上的路径 String filePath = String.format(&quot;F:/qr-%s.png&quot;, response.getOutTradeNo()); log.info(&quot;filePath:&quot; + filePath); ZxingUtils.getQRCodeImge(response.getQrCode(), 256, filePath); break; case FAILED: log.error(&quot;支付宝预下单失败!!!&quot;); break; case UNKNOWN: log.error(&quot;系统异常，预下单状态未知!!!&quot;); break; default: log.error(&quot;不支持的交易状态，交易返回异常!!!&quot;); break; &#125; &#125; // 简单打印应答 private void dumpResponse(AlipayResponse response) &#123; if (response != null) &#123; log.info(String.format(&quot;code:%s, msg:%s&quot;, response.getCode(), response.getMsg())); if (StringUtils.isNotEmpty(response.getSubCode())) &#123; log.info(String.format(&quot;subCode:%s, subMsg:%s&quot;, response.getSubCode(), response.getSubMsg())); &#125; log.info(&quot;body:&quot; + response.getBody()); &#125; &#125;&#125; 这里调用函数的时候，因为我这里只是个Demo，没做太多操作，请小伙伴们根据自己的实际情况进行传参 强调一点 ZxingUtils.getQRCodeImge(response.getQrCode(), 256, filePath);是生成的二维码 要扫这个工具生成的二维码才可以 重点： 上支付宝官方的图 支付成功后的回调！当我们支付成功的时候，支付宝会根据我们事先定义好的接口，发起回调，前提是这个接口必须是公网可以访问的才行，有两种方法，申请一个公网Ip地址，或者使用内网穿透，这里我来使用一下内网穿透，内网穿透的工具很多，关于内网穿透的知识，可以参看我的另外一篇博客 内网穿透 这里我设置一下我的回调接口 写个Controller进行Debug 运行项目，进行一次支付，可以接收到回调 官方文档：https://docs.open.alipay.com/203/105286/有关异步回调的一些说明 我把一些注意点强调一下： 程序执行完后必须打印输出“success”（不包含引号）。如果商户反馈给支付宝的字符不是success这7个字符，支付宝服务器会不断重发通知，直到超过24小时22分钟。一般情况下，25小时以内完成8次通知（通知的间隔频率一般是：4m,10m,10m,1h,2h,6h,15h） 程序执行完成后，该页面不能执行页面跳转。如果执行页面跳转，支付宝会收不到success字符，会被支付宝服务器判定为该页面程序运行出现异常，而重发处理结果通知 cookies、session等在此页面会失效，即无法获取这些数据 按照步骤做就可以了 我这里直接贴代码 1234567891011121314151617181920212223242526272829303132333435@RequestMapping(&quot;/alipay_callback&quot;) public Object alipayCallback(HttpServletRequest request) &#123; Map&lt;String, String&gt; params = new HashMap&lt;&gt;(); Map requestParams = request.getParameterMap(); for (Iterator iter = requestParams.keySet().iterator(); iter.hasNext(); ) &#123; String name = (String) iter.next(); String[] values = (String[]) requestParams.get(name); String valueStr = &quot;&quot;; for (int i = 0; i &lt; values.length; i++) &#123; valueStr = (i == values.length - 1) ? valueStr + values[i] : valueStr + values[i] + &quot;,&quot;; &#125; params.put(name, valueStr); &#125; if(params == null || params.size() == 0)&#123; return &quot;failed&quot;; &#125; log.info(&quot;支付宝回调,sign: &#123;&#125;, trade_status: &#123;&#125;, 参数: &#123;&#125;&quot;, params.get(&quot;sign&quot;), params.get(&quot;trade_status&quot;), params.toString()); //验证回调的正确性 params.remove(&quot;sign_type&quot;); try &#123; boolean alipayRSACheck = AlipaySignature.rsaCheckV2(params, Configs.getAlipayPublicKey(), &quot;utf-8&quot;, Configs.getSignType()); if (!alipayRSACheck) &#123; //非法请求 return null; &#125; &#125; catch (AlipayApiException e) &#123; log.error(&quot;支付宝验证回调异常&quot;, e); e.printStackTrace(); &#125; //验证各种数据 //业务代码 return &quot;success&quot;; &#125; 这里注意一点，用SpringBoot打包的时候，如果按照平时的方法打包，会报错，找不到Alipay的类（SSM同理） 在maven里面 加上下面的配置 外部jar包根据实际情况自行进行修改 12345678910111213141516&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;compilerArguments&gt; &lt;extdirs&gt;$&#123;project.basedir&#125;/src/main/resources/lib&lt;/extdirs&gt; &lt;/compilerArguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;","categories":[{"name":"技术分享","slug":"技术分享","permalink":"http://yoursite.com/categories/技术分享/"}],"tags":[{"name":"支付宝当面付","slug":"支付宝当面付","permalink":"http://yoursite.com/tags/支付宝当面付/"}]},{"title":"SpringBoot整合支付宝-当面付(一)","slug":"SpringBoot整合支付宝-当面付(一)","date":"2019-08-02T08:10:15.000Z","updated":"2019-08-25T00:59:29.248Z","comments":true,"path":"2019/08/02/SpringBoot整合支付宝-当面付(一)/","link":"","permalink":"http://yoursite.com/2019/08/02/SpringBoot整合支付宝-当面付(一)/","excerpt":"","text":"闲话不多讲直接上干货 以下是当面付的Demo https://docs.open.alipay.com/54/104506 这里我们先下载Java版的下来看看 这是一个Eclipse版的Web项目，我这里使用的工具是IDEA，需要转换一下 点击File—&gt;Project Structure 添加好之后点击右下角Apply，项目会多一个web目录 选择好项目的模块右下角点击Apply 将原先的webroot下面jsp和图片放到web下 启动项目,进入二维码支付 启动项目会报错，查看控制台，会看到乱码，产生问题原因是我们没有在zfbinfo.properties写对应的配置 我们需要去蚂蚁金服开放平台登陆入驻， 做好登陆入驻后： 在右上角点击管理中心—–&gt;开发中心——&gt;研发服务 我们使用沙箱环境 里面有APPID等信息，对应填上我们的配置文件里面就可以了 我们现在是开发环境：（需要修改的配置项） 把支付宝网关的地址https://openapi.alipay.com/gateway.do修改为https://openapi.alipaydev.com/gateway.do appid：对应的APPID pid：商户UID 密钥生成： 将生成的商户应用密钥的私钥和公钥对应配置到zfbinfo.properties中的private_key和public_key中 我们使用：SHA256withRsa对应支付宝公钥 在沙箱应用的 点击查看应用公钥，点击修改，将刚才生成的商户应用公钥粘贴进去，修改完成后，点击查看支付宝公钥 将支付宝公钥配置到zfbinfo.properties的 12#SHA256withRsa对应支付宝公钥alipay_public_key=xxxxxxxxx 重启项目 成功了之后用我们的支付宝扫一下 停！我们要使用沙箱专用的支付宝才行 我们在沙箱工具里面https://openhome.alipay.com/platform/appDaily.htm?tab=tool 下载沙箱版的钱包，账号密码和支付密码，在对应的沙箱账号里面有显示 本文篇幅有点过长，决定使用两篇文章来写","categories":[{"name":"技术分享","slug":"技术分享","permalink":"http://yoursite.com/categories/技术分享/"}],"tags":[{"name":"支付宝当面付","slug":"支付宝当面付","permalink":"http://yoursite.com/tags/支付宝当面付/"}]},{"title":"设计模式-建造者","slug":"设计模式-建造者","date":"2019-08-01T09:20:11.000Z","updated":"2019-11-26T03:19:35.924Z","comments":true,"path":"2019/08/01/设计模式-建造者/","link":"","permalink":"http://yoursite.com/2019/08/01/设计模式-建造者/","excerpt":"","text":"建造者模式将一个复杂对象的构建与它的表现分离，使得同样的构建过程可以创建不同的表示 优点如果一个对象有非常复杂的内部结构，可以把复杂对象的创建和适用分离 扩展性好，建造类之间独立 缺点产生多余的Builder 产生的内部发生变化，建造者都要修改，成本较大 适用于规模固定的场景 实例创建一个产品类 12345public class Product &#123; private String name; private Double price; //隐藏了set方法和toString方法&#125; 创建一个产品的一个构建者 12345public interface ProductBuilder &#123; void buildProductName(String name); void buildProductPrice(Double price); Product makeProduct();&#125; 实现产品构建者的实现类 12345678910111213141516public class ProductActualBuilder implements ProductBuilder &#123; private Product product = new Product(); public void buildProductName(String name) &#123; product.setName(name); &#125; public void buildProductPrice(Double price) &#123; product.setPrice(price); &#125; public Product makeProduct() &#123; return product; &#125;&#125; 上一张图以表示它们之间的关系 测试Demo 123456789public class TestDemo &#123; public static void main(String[] args) &#123; ProductBuilder productBuilder = new ProductActualBuilder(); productBuilder.buildProductName(&quot;aaa&quot;); productBuilder.buildProductPrice(100.0); Product product = productBuilder.makeProduct(); System.out.println(product); &#125;&#125; 这样子我们就完成了建造者设计模式的一个完整demo了 点评这一个设计模式下来，看上去，和我们直接调用set方法没啥区别，的确，就是一样样的 buildProductName和setProductName，从某个角度，可以看似一样的东西，但是，一般来讲，我们从名字上可以区分set就是设值操作，build是一个更为抽象的东西，我build一个房子，里面包含了许多build步骤，而set操作只是对对象的某个属性赋值而已，由于是Demo原因，这里看上去build和set的功能一样罢了，但是实际上build里面可以包含很多复杂的操作 而本次案例当中build方法比较简单，只是为了来说明一下构建者模式仅此而已 接下来我们来写一个链式调用的构建者模式 一样是Product类 123456789101112131415161718192021222324252627282930313233343536public class Product &#123; private String name; private Double price; private Product(ProductBuilder productBuilder)&#123; this.name = productBuilder.name; this.price = productBuilder.price; &#125; public static class ProductBuilder&#123; private String name; private Double price; public ProductBuilder buildProductName(String name)&#123; this.name = name; return this; &#125; public ProductBuilder buildProductPrice(Double price)&#123; this.price = price; return this; &#125; public Product build()&#123; return new Product(this); &#125; &#125; @Override public String toString() &#123; return &quot;Product&#123;&quot; + &quot;name=&apos;&quot; + name + &apos;\\&apos;&apos; + &quot;, price=&quot; + price + &apos;&#125;&apos;; &#125;&#125; 区别的地方在于，在Product类里面实现了构建者 TestDemo: 1234567public class TestDemo &#123; public static void main(String[] args) &#123; Product product = new Product.ProductBuilder().buildProductName(&quot;build&quot;) .buildProductPrice(100.2).build(); System.out.println(product); &#125;&#125; 这样子，代码更加精简 总结建造者，用于对复杂对象的构造、初始化，与工厂模式不同的是，建造者的目的在于把复杂构造过程从不同对象展现中抽离出来，使得同样的构造工序可以展现出不同的产品对象。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"内网穿透","slug":"内网穿透","date":"2019-07-13T13:55:00.000Z","updated":"2019-08-25T00:59:15.384Z","comments":true,"path":"2019/07/13/内网穿透/","link":"","permalink":"http://yoursite.com/2019/07/13/内网穿透/","excerpt":"","text":"什么是内网和外网所谓内网就是内部建立的局域网络或办公网络。举个例：一家公司或一个家庭有多台计算机，他们利用不同网络布局将这一台或多台计算机或其它设备连接起来构成一个局部的办公或者资源共享网络，我们就称它为内部网络，也叫内网。 所谓外网就是通过一个网关或网桥与其它网络系统连接，相对于自己的内网来说，连接的其它网络系统就称为外部网络，也叫外网。举例说明：当一家公司或一个家庭的所有电脑网络想要与公司或家庭以外的网络连接（比如连接互连网），相对于这家公司或家庭，其它网络（或互连网）就称为外网！ 为什么需要内网穿透简单来说： 当内网中的主机没有静态IP地址却要被外网稳定访问时可以使用内网穿透 在互联网中唯一定位一台主机的方法是通过公网的IP地址，但固定IP是一种非常稀缺的资源，不可能给每个公司都分配一个，且许多中小公司不愿意为高昂的费用买单，多数公司直接或间接的拨号上网，电信部门会给接入网络的用户分配IP地址，以前上网用户少的时候基本分配的都是临时的静态IP地址，租约过了之后可能会更换成另一个IP地址，这样外网访问就不稳定，因为内网的静态IP地址一直变化，为了解决这个问题可以使用动态域名解析的办法变换域名指向的静态IP地址。但是现在越来越多的上网用户使得临时分配的静态IP地址也不够用了，电信部门开始分配一些虚拟的静态IP地址，这些IP是公网不能直接访问的，如以125开头的一些IP地址，以前单纯的动态域名解析就不好用了。 内网穿透的定义与障碍当内网中的主机没有静态IP地址要被外网稳定访问时可以使用内网穿透 障碍一：位于局域网内的主机有两套 IP 地址，一套是局域网内的 IP 地址，通常是动态分配的，仅供局域网内的主机间通信使用；一套是经过网关转换后的外网 IP 地址，用于与外网程序进行通信。 障碍二：位于不同局域网内的两台主机，即使是知道了对方的 IP 地址和端口号，“一厢情愿”地将数据包发送过去，对方也是接收不到的。 因为出于安全起见，除非是主机主动向对方发出了连接请求（这时会在该主机的数据结构中留下一条记录），否则，当主机接收到数据包时，如果在其数据结构中查询不到对应的记录，那些不请自来的数据包将会被丢弃。 解决办法：要想解决以上两大障碍，我们需要借助一台具有公网 IP 的服务器进行桥接。 ngrokngrok是一个反向代理，通过在公共的端点和本地运行的Web服务器之间建立一个安全的通道。ngrok可捕获和分析所有通道上的流量，便于后期分析与响应。 没有ngrok的时候 作为一个Web开发者，我们有时候会需要临时地将一个本地的Web网站部署到外网，以供他人体验评价或协助调试等等，通常我们会这么做： 找到一台运行于外网的Web服务器 服务器上有网站所需要的环境，否则自行搭建 将网站部署到服务器上 调试结束后，再将网站从服务器上删除 有ngrok的时候 首先注册并下载ngrok，得到一串授权码 ngrok -authtoken 你的授权码 80，80是你本地Web服务的端口，而之后ngrok会记住你的授权码，直接ngrok 80就OK了 你会得到一串网址，通过这个网址就可以访问你本地的Web服务了 去官方注册个账号或者使用github账号登陆一下,有步骤介绍 下载ngrok，这里以Windows为例子 下载的是一个压缩包，解压后，用CMD进入到该目录下执行以下命令,后面是一个token值，随便打即可 1$ ngrok authtoken (authtoken看自己的token值，登陆之后在Auth下可以查看) 1$ ngrok http 8080 解析来我们就可以拿着对应的域名来访问了","categories":[{"name":"技术分享","slug":"技术分享","permalink":"http://yoursite.com/categories/技术分享/"}],"tags":[]},{"title":"布隆过滤器","slug":"布隆过滤器","date":"2019-06-23T14:51:43.000Z","updated":"2019-08-25T00:59:04.686Z","comments":true,"path":"2019/06/23/布隆过滤器/","link":"","permalink":"http://yoursite.com/2019/06/23/布隆过滤器/","excerpt":"","text":"先探讨一个问题？ 现在有50亿个电话号码，现在要快速准确判断这些电话号码是否已经存在？ 数据库查询？太慢了 数据使用集合存放在内存？内存浪费 布隆过滤器！ 布隆过滤器什么是布隆过滤器它实际上是一个很长的二进制向量和一系列随机映射函数 布隆过滤器的实现原理当一个元素被加入集合时，通过K个散列函数将这个元素映射成一个位数组中的K个点，把它们置为1。检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了：如果这些点有任何一个0，则被检元素一定不在；如果都是1，则被检元素很可能在。这就是布隆过滤器的基本思想。 画个图描述一下 布隆过滤器的误差率直观因素：m/n的比率，hash函数的个数 1个元素，1个hash函数，任意一个比特为1的概率为1/m，依然为0的概率为1-1/m k给函数，依然为0的概率为（1-1/m）^k，n个元素，依然为0的概率为(1-1/m)^nk 被设置为1的概率为1-(1-1/m)^nk 新元素权重的概率为（1-(1-1/m)^nk）^k 本地布隆过滤器使用Guava来实现 123456789101112131415161718192021public class BloomFilterDemo &#123; public static void main(String[] args) &#123; //构建布隆过滤器 BloomFilter&lt;String&gt; bloomFilter = BloomFilter.create(new Funnel&lt;String&gt;() &#123; @Override public void funnel(String s, PrimitiveSink primitiveSink) &#123; primitiveSink.putString(s, Charsets.UTF_8); &#125; &#125;,10000,0.0001); //插入数据 for (int i = 0 ; i &lt; 1000 ; i++)&#123; bloomFilter.put(&quot;i_&quot;+i); &#125; //测试结果 for (int i = 0 ; i &lt; 1005 ; i++)&#123; if(!bloomFilter.mightContain(&quot;i_&quot;+i))&#123; System.out.println(&quot;no&quot;); &#125; &#125; &#125;&#125; 基于Redis的布隆过滤器redis 在 4.0 的版本中加入了 module 功能，布隆过滤器可以通过 module 的形式添加到 redis 中，所以使用 redis 4.0 以上的版本可以通过加载 module 来使用 redis 中的布隆过滤器 在Docker中 12345678&gt; docker run -d -p 6379:6379 --name bloomfilter redislabs/rebloom&gt; docker exec -it bloomfilter redis-cli# redis-cli# 127.0.0.1:6379&gt;# 127.0.0.1:6379&gt; BF.ADD newFilter foo(integer) 1# 127.0.0.1:6379&gt; BF.EXISTS newFilter foo(integer) 1","categories":[{"name":"技术分享","slug":"技术分享","permalink":"http://yoursite.com/categories/技术分享/"}],"tags":[]},{"title":"Redis-缓存设计","slug":"Redis-缓存设计","date":"2019-05-27T14:49:16.000Z","updated":"2019-08-25T00:58:51.110Z","comments":true,"path":"2019/05/27/Redis-缓存设计/","link":"","permalink":"http://yoursite.com/2019/05/27/Redis-缓存设计/","excerpt":"","text":"缓存有什么作用？先来看一幅图 缓存主要带来了什么好处呢？ 加速读写 因为缓存通常都是全内存的（例如Redis、Memcache），而存储层通常读写性能不够强悍（例如MySQL），通过缓存的使用可以有效地加速读写，优化用户体验 降低后端负载 帮助后端减少访问量和复杂计算（例如很复杂的SQL语句），在很大程度降低了后端的负载 当然，缓存能够给我们带来好的方便，但是也加大了开发者的开发成本 接下来我们就来学习下如何使用Redis来设计缓存，以及注意的问题： 缓存更新策略缓存中的数据通常都是有生命周期的，需要在指定时间后被删除或更新，这样可以保证缓存空间在一个可控的范围。 三种策略： LRU/LFU/FIFO算法剔除通常用于缓存使用量超过了预设的最大值时候，如何对现有的数据进行剔除。例如Redis使用maxmemory-policy这个配置作为内存最大值后对于数据的剔除策略,还要设置maxmemory（最大内存） maxmemory 默认为0（代表不限制Redis的内存使用） maxmemory-policy有六种策略 noeviction: 不进行置换，表示即使内存达到上限也不进行置换，所有能引起内存增加的命令都会返回error allkeys-lru: 优先删除掉最近最不经常使用的key，用以保存新数据 volatile-lru: 只从设置失效（expire set）的key中选择最近最不经常使用的key进行删除，用以保存新数据 allkeys-random: 随机从all-keys中选择一些key进行删除，用以保存新数据 volatile-random: 只从设置失效（expire set）的key中，选择一些key进行删除，用以保存新数据 volatile-ttl: 只从设置失效（expire set）的key中，选出存活时间（TTL）最短的key进行删除，用以保存新数据 优点使用简单，只用修改对应的配置文件就可以 缺点数据的清理由选择算法决定，开发人员只能选择置换内存的算法策略，所以数据的一致性是最差的 超时剔除超时剔除通过给缓存数据设置过期时间，让其在过期时间后自动删除，Redis提供了expire命令 1$ EXPIRE key time(s) 优点使用简单，只需要设置对应key的过期时间即可 能够允许在一段时间内的数据不一致或者是数据对实际业务影响不大的场景，此方案还是很好的解决方案 缺点数据不一致：如果，数据对实际业务影响比较大的场景，谨慎使用，特别涉及到金钱交易的情况下 主动更新应用方对于数据的一致性要求高，需要在真实数据更新后，立即更新缓存数据。例如可以利用消息系统或者其他方式通知缓存更新 优点可以根据自己的业务需求来完成缓存的更新，解决了单纯使用expire设置过期时间的局限性 一致性最高，但如果主动更新发生了问题，那么这条数据很可能很长时间不会更新，所以建议结合超时剔除一起使用效果会更好 缺点维护成本会比较高，开发者需要自己来完成更新，并保证更新操作的正确性 有两个建议： 低一致性业务建议配置最大内存和淘汰策略的方式使用 高一致性业务可以结合使用超时剔除和主动更新，这样即使主动更新出了问题，也能保证数据过期时间后删除脏数据 缓存粒度很多时候我们缓存的值是后端Mysql或者别的数据库查询出来的值，因为一般来说，复杂的查询比较费时，所以查询出来放进redis是一个比较明智的做法 那么我们要缓存多少，缓存到一个什么层度呢？ 比如说：缓存所有列 1set user:&#123;id&#125; &apos;select * from user where id=&#123;id&#125;&apos; 缓存部分列 12set user:&#123;id&#125; &apos;select &#123;importantColumn1&#125;, &#123;important Column2&#125; ... &#123;importantColumnN&#125;from user where id=&#123;id&#125;&apos; 这个问题就是缓存粒度问题 缓存全部数据要比部分数据占用更多的空间，可能存在以下问题： 全部数据会造成内存的浪费 全部数据可能每次传输产生的网络流量会比较大，耗时相对较大，在极端情况下会阻塞网络 全部数据的序列化和反序列化的CPU开销更大 缓存粒度问题是一个容易被忽视的问题，如果使用不当，可能会造成很多无用空间的浪费，网络带宽的浪费，代码通用性较差等情况，需要综合数据通用性、空间占用比、代码维护性三点进行取舍 缓存穿透 缓存穿透是指查询一个根本不存在的数据，缓存层和存储层都不会命中，当被别人恶意访问查询一个不存在的数据的时候，请求就会访问到存储层（Mysql），当请求量很大的情况下，Mysql是承受不了的，可能造成后端存储宕掉，而且缓存层也失去了保护存储层的意义 解决办法： 缓存空对象 当第2步存储层不命中后，仍然将空对象保留到缓存层中，之后再访问这个数据将会从缓存中获取，这样就保护了后端数据源，缺点： 缓存层中存了更多的键，需要更多的内存空间(也是一种危害) 措施：是将这类的Key设置一个较短的过期时间 缺点：缓存层和存储层的短暂不一致 措施：可以利用消息系统或者其他方式清除掉缓存层中的空对象 布隆过滤器 在访问缓存层之前，将存在的key用布隆过滤器提前保存起来,当访问不存在的key的时候，布隆过滤器直接返回，可以避免访问缓存层和存储层 使用互斥锁：根据key获取value值为空时，锁上，从数据库中load数据后再释放锁（适合并发量比较低的情况，因为并发量很大的情况下，会导致阻塞和死锁的情况） 缓存雪崩由于缓存层承载大量请求，当cache服务异常/脱机，流量打向了后端的存储层，造成存储层也会级联宕机的情况，除了缓存服务异常和脱机的情况，还有另外一种情况：缓存key大片面积失效（时间到期）也会导致缓存雪崩 关于解决缓存服务异常和脱机需要注意的几个问题: 保证缓存层服务高可用性 如果缓存层设计成高可用的，即使个别节点、个别机器、甚至是机房宕掉，依然可以提供服务 依赖隔离组件为后端限流并降级 我们需要对重要的资源（例如Redis、MySQL、HBase、外部接口）都进行隔离，让每种资源都单独运行在自己的线程池中，即使个别资源出现了问题，对其他服务没有影响 提前演练 演练缓存层宕掉后，应用以及后端的负载情况以及可能出现的问题，在此基础上做一些预案设定 解决方案： 在设置缓存时候，不设置统一的过期时间，而是在一个范围内随机设置过期时间，以防止大量的key同时过期 使用互斥锁：根据key获取value值为空时，锁上，从数据库中load数据后再释放锁（适合并发量比较低的情况，因为并发量很大的情况下，会导致阻塞和死锁的情况） 提前使用互斥锁，比如key10S过期，在内部使用一个标识key 8S过期，假设标识key过期的时候，就给对应的Key延长过期时间 双缓存，缓存A和缓存B，A设置超时时间，B不设值超时时间，先从A读缓存，如果A没有则读B，并且更新A缓存和B缓存（对存储的消耗比较大）","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"设计模式-抽象工厂","slug":"设计模式-抽象工厂","date":"2019-05-27T10:25:23.000Z","updated":"2019-11-18T06:50:57.818Z","comments":true,"path":"2019/05/27/设计模式-抽象工厂/","link":"","permalink":"http://yoursite.com/2019/05/27/设计模式-抽象工厂/","excerpt":"","text":"抽象工厂抽象工厂模式提供一个创建一系列相关或相互依赖对象的接口 客户端（应用层）不依赖于产品类实例如何被创建，实现等细节 强调一系列相关的产品对象（属于同一产品族）一起使用创建对象需要大量重复的代码 具体产品在应用层代码隔离，无需关系创建细节 将一个系列的产品族一起创建 产品族：这里画个图来帮助理解 比如说，小米的所有子产品都属于一个产品族 前面学的工厂方法所关注的就是产品等级结构：就如图中的小米电脑和苹果电脑，都属于电脑 这里的抽象工厂关注的是一个产品族:我们只需要指出一个产品所处于的产品族以及所属的等级结构就能唯一确定这个产品（举个例子：从小米工厂取出的手机——》就是小米手机） Demo工厂接口 1234567/** * 品牌工厂 */public interface BrandFactory &#123; Phone getPhone();//获得手机 LapTop getLapTop();//获得手提电脑&#125; 两个抽象实体： 123456/** * 手提电脑 */public abstract class LapTop &#123; public abstract void produce();//生产手提电脑&#125; 123456/** * 手机 */public abstract class Phone &#123; public abstract void produce();//生产手机&#125; 两个具体的工厂: 1234567891011121314/** * 苹果工厂 */public class AppleFactory implements BrandFactory &#123; @Override public Phone getPhone() &#123; return new ApplePhone(); &#125; @Override public LapTop getLapTop() &#123; return null; &#125;&#125; 1234567891011121314/** * 小米工厂 */public class MiFactory implements BrandFactory &#123; @Override public Phone getPhone() &#123; return new MiPhone(); &#125; @Override public LapTop getLapTop() &#123; return new MiLapTop(); &#125;&#125; 两个手机的实现类： 123456789/** * 小米手机 */public class MiPhone extends Phone &#123; @Override public void produce() &#123; System.out.println(&quot;生产小米手机&quot;); &#125;&#125; 123456789/** * 苹果手机 */public class ApplePhone extends Phone &#123; @Override public void produce() &#123; System.out.println(&quot;生产苹果手机&quot;); &#125;&#125; 两个笔记本的实现类 123456789/** * 苹果笔记本 */public class AppleLapTop extends LapTop &#123; @Override public void produce() &#123; System.out.println(&quot;生产苹果笔记本&quot;); &#125;&#125; 123456789/** * 小米笔记本 */public class MiLapTop extends LapTop &#123; @Override public void produce() &#123; System.out.println(&quot;生产小米笔记本&quot;); &#125;&#125; 看一下类图 写个测试类 123456789public class TestDemo &#123; public static void main(String[] args) &#123; BrandFactory brandFactory = new MiFactory(); Phone phone = brandFactory.getPhone(); LapTop lapTop = brandFactory.getLapTop(); phone.produce();//输出生产小米手机 lapTop.produce();//输出小米笔记本 &#125;&#125; 结论在抽象工厂里面，对于应用层，根本不关心手机和笔记本的实现过程，只关心对应的一个工厂，在哪个工厂取得的产品也就是什么工厂的产品，还是上面的例子（小米工厂拿到的手机就是小米手机） 应用层代码和具体的手机是解耦的，和具体的笔记本电脑也是解耦的 工厂方法和抽象工厂的最大区别是： 工厂方法关注的是同一产品等级，抽象工厂关注的是产品族 抽象工厂 优点： 应用层代码不和具体的产品发生依赖，只和具体的产品族工厂发生依赖 从某个工厂取出的产品，一定是属于这个工厂的产品 扩展性好，比如增加一个华为的品牌，只用增加一个工厂，一个手机，一个笔记本即可，其余不用发生改变（符合开闭原则）产品等级稳定的情况下 缺点： 新增产品等级的时候，会在原有的结构上进行较大的改动（比如说，增加一个小米生产的别的产品）此时就违背了开闭原则，所以要注意场景使用 举个真实的例子：在java.sql这个包下Connection这个接口里面PrepareStatement和Statement都是属于一个产品族在这里，就使用到了抽象工厂的设计模式","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"Redis-Cluster常见问题","slug":"Redis-Cluster常见问题","date":"2019-05-26T13:00:30.000Z","updated":"2019-08-25T00:58:28.202Z","comments":true,"path":"2019/05/26/Redis-Cluster常见问题/","link":"","permalink":"http://yoursite.com/2019/05/26/Redis-Cluster常见问题/","excerpt":"","text":"集群完整性12# 默认为yescluster-require-full-coverage yes 默认的配置保证了两个问题： 集群中的161384个槽全部可用，保证了集群的完整性 节点故障或者故障转移的时候，进行set操作会报 (error) CLUSTERRDOWN The cluster is down 一个节点坏了，整个集群都不可用了，这在实际业务上是有问题的 所以我们通常需要将它修改为no 带宽消耗RedisCluster会定期进行Ping/Pong进行心跳检测，也会进行信息的一个交互 官方建议RedisCluster的节点数量不要超过1000个，因为RedisCluster对带宽的消耗很高 主要来自下面三个方面 消息发送频率，节点发现与其他节点最后通信时间超过cluster-node-timeout/2时会直接发送ping消息 消息数据量：slots槽数组（2KB空间）和整个集群1/10的状态数据（10个节点的状态数据约1KB） 节点部署的机器规模：集群分布的机器越多且每台机器划分的节点数越均匀，则集群内整体的可用带宽就越高 举个例子： 规模：节点200个，20台物理机（每台10个节点） 1cluster-node-timeout = 15000 ping/pong的带宽为25Mb 1cluster-node-timeout = 20000 ping/pong带宽会低于15Mb 优化： 避免多业务使用一个集群，大业务可以多集群 cluster-node-timeout:带宽和故障转移速度的均衡 尽量均匀分配到多机器上，保证高可用和带宽 Pub/Sub广播问题：publish在集群每个节点广播：加重带宽 解决：针对这种情况建议使用sentinel结构专门用于Pub/Sub功能，从而规避这一问题。 数据倾斜集群内特定节点数据量过大将导致节点之间负载不均，影响集群均衡和运维成本。 数据倾斜主要分为以下几种： ·节点和槽分配严重不均。 当节点对应槽数量不均匀时，可以使用redis-trib.rb rebalance命令进行平衡 ·不同槽对应键数量差异过大。 通过命令：cluster countkeysinslot{slot}可以获取槽对应的键数量，识别出哪些槽映射了过多的键。再通过命令clustergetkeysinslot{slot}{count}循环迭代出槽下所有的键。 ·集合对象包含大量元素。 对于大集合对象的识别可以使用redis-cli–bigkeys命令识别 ·内存相关配置不一致。 当集群大量使用hash、set等数据结构时，如果内存压缩数据结构配置不一致，极端情况下会相差数倍的内存，从而造成节点内存量倾斜。 请求倾斜集群内特定节点请求量/流量过大将导致节点之间负载不均，影响集群均衡和运维成本。 避免方式如下： 合理设计键，热点大集合对象做拆分或使用hmget替代hgetall避免整体读取。 不要使用热键作为hash_tag，避免映射到同一槽。 对于一致性要求不高的场景，客户端可使用本地缓存减少热键调用。 读写分离只读连接集群模式下从节点不接受任何读写请求，发送过来的键命令会重定向到负责槽的主节点上（其中包括它的主节点） 当需要使用从节点分担主节点读压力时，可以使用readonly命令打开客户端连接只读状态。 注意：readonly是连接级别生效，每次连接都要开启一遍才可以 读写分离问题：复制延迟，读取过期数据，从节点故障 集群模式下读写分离涉及对客户端修改如下： 维护每个主节点可用从节点列表 针对读命令维护请求节点路由 从节点新建连接开启readonly转态 集群模式下读写分离成本比较高，可以直接扩展主节点数量提高集群性能，一般不建议集群模式下做读写分离 集群读写分离有时用于特殊业务场景如： 利用复制的最终一致性使用多个从节点做跨机房部署降低读命令网络延迟。 主节点故障转移时间过长，业务端把读请求路由给从节点保证读操作可用。 数据迁移应用Redis集群时，常需要把单机Redis数据迁移到集群环境。redis-trib.rb工具提供了导入功能，用于数据从单机向集群环境迁移的场景，命令如下： 1$ redis-trib.rb import host:port --from &lt;arg&gt; --copy --replace redis-trib.rb import命令内部采用批量scan和migrate的方式迁移数据。这种迁移方式存在以下缺点： 迁移只能从单机节点向集群环境导入数据。 不支持在线迁移数据，迁移数据时应用方必须停写，无法平滑迁移数据。 迁移过程中途如果出现超时等错误，不支持断点续传只能重新全量导入。 使用单线程进行数据迁移，大数据量迁移速度过慢 正因为这些问题，社区开源了很多迁移工具，这里推荐一款唯品会开发的redis-migrate-tool，该工具可满足大多数Redis迁移需求，特点如下： ·支持单机、Twemproxy、Redis Cluster、RDB/AOF等多种类型的数据迁移 工具模拟成从节点基于复制流迁移数据，从而支持在线迁移数据，业务方不需要停写 采用多线程加速数据迁移过程且提供数据校验和查看迁移状态等功能 集群VS单机集群限制 key批量操作支持有限：mget，mset必须在一个slot上 key事务和Lua支持有限：操作的key必须在一个节点上 key是数据分区的最小粒度，不支持bigkey 不支持多个数据库：集群下只有一个db0 复制只支持复制一层，不支持树形复制结构 思考分布式Redis RedisCluster:用来满足容量和性能的扩展性，但是要看业务来使用 大多数情况客户端性能会降低 命令无法跨节点使用 Lua和事务无法跨节点使用 客户端维护更加复杂","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"Redis-Cluster(集群)","slug":"Redis-Cluster(集群)","date":"2019-05-20T12:03:44.000Z","updated":"2019-08-25T00:58:13.818Z","comments":true,"path":"2019/05/20/Redis-Cluster(集群)/","link":"","permalink":"http://yoursite.com/2019/05/20/Redis-Cluster(集群)/","excerpt":"","text":"Redis集群为什么要引入Redis集群 单机会造成什么问题，官方说法：单机Redis支持十万的并发量，如果我需要二十万呢？三十万呢？这时候就要用到Redis集群，撇开Redis集群不说，集群，就是用来分担单机的压力，无论是访问的压力，查询的压力还是流量的压力，为什么通常都要使用集群来解决呢？比如电脑内存不够往往会想着买一条内存条，但是还不够呢？一台电脑不断加装，成本越来越贵，很不划算，当成本大于我重新买一台电脑的时候，我当然选择两台电脑就是这个意思，更何况通常集群都是上百台呢 RedisCluster的安装原生安装配置节点启动6个redis，配置的端口从7000-7005，以下是配置文件 12345678port 7000daemonize yesdir &quot;/redis/data&quot;logfile &quot;7000.log&quot;dbfilename &quot;dump-7000.rdb&quot;cluster-enabled yescluster-config-file nodes-7000.confcluster-require-full-coverage no 配置meet12345$ redis-cli -p 7000 cluster meet 127.0.0.1 7001$ redis-cli -p 7000 cluster meet 127.0.0.1 7002$ redis-cli -p 7000 cluster meet 127.0.0.1 7003$ redis-cli -p 7000 cluster meet 127.0.0.1 7004$ redis-cli -p 7000 cluster meet 127.0.0.1 7005 分配槽编写一个脚本addslots.sh 1234567start=$1end=$2port=$3for slot in `seq $&#123;start&#125; $&#123;end&#125;`do redis-cli -p $&#123;port&#125; cluster addslots $&#123;slot&#125;done 分配槽给节点，这里我有六个redis，但是我只分配将槽分成三份，另外三份来用做slave 123$ sh addslots.sh 0 5461 7000$ sh addslots.sh 5462 10922 7001$ sh addslots.sh 10923 16383 7002 主从分配上面将槽分配给了7000，7001，7002，将其对应的slave是7003，7004，7005 1234#replicate后面接对应节点的id号，id号可以看我上面那张图，一一对应的，在redis-cli下通过cluster nodes可以查询$ redis-cli -p 7003 cluster replicate 84b0e3f7ba4d7a709d8b25fe4844b66296af71d8$ redis-cli -p 7004 cluster replicate 6b17f1a9bb7ee845c1d8c012c2805df76fb9b943$ redis-cli -p 7005 cluster replicate e32dd59b6b1205ca88050e2124c40ba37200a674 安装结束： 123$ redis-cli -p 7000$ set k1 v1OK 可以进入redis-cli后能够进行set操作了 官方工具安装首先按照步骤按照一下官方工具 1234567wget https://cache.ruby-lang.org/pub/ruby/2.6/ruby-2.6.3.tar.gzcd ruby-2.6.3./configure -prefix=/usr/local/rubymake &amp;&amp; make installwget http://rubygems.org/downloads/redis-3.3.0.gemgem install -l redis-3.3.0.gemgem list -- check redis gem 然后和上面一样，跑六个节点，全部启动起来 8000-8005端口 12345678port 8005daemonize yesdir &quot;/redis/data&quot;logfile &quot;8005.log&quot;dbfilename &quot;dump-8005.rdb&quot;cluster-enabled yescluster-config-file nodes-8005.confcluster-require-full-coverage no 进入redis下的src目录 123$ cd /redis/src#使用工具来创建集群,其中数量1代表一个主节点配置1个从节点$ ./redis-trib.rb create --replicas 1 127.0.0.1:8000 127.0.0.1:8001 127.0.0.1:8002 127.0.0.1:8003 127.0.0.1:8004 127.0.0.1:8005 对上面没问题可以输入yes 可以通过以下命令，查看集群是否搭建成功 12$ redis-cli -p 8000 cluster nodes$ redis-cli -p 8000 cluster info 集群扩容 启动两个独立的redis，其中8006和8007是独立的 将集群种的节点对独立的节点进行meet操作 12$ redis-cli -p 8000 cluster meet 127.0.0.1 8006$ redis-cli -p 8000 cluster meet 127.0.0.1 8007 主从的分配 1$ redis-cli -p 8006 cluster replicate 8007的runId 分配槽，使用redis-trib的工具 进入 src目录下， 1$ ./redis-trib.rb reshard 127.0.0.1:8000 分配完成 集群缩容./redis-trib.rb reshard – from 不要的节点的runId –to 分配的节点的runId –slots 多少个槽 1366 让哪个端口去执行示例： 1$ ./redis-trib.rb reshard --from 9334fcdbc2453637e86f1b6664a8c86413eb87d4 --to d576410d1da0cd11f29035550ef0eaf81a811ea7 --slots 1366 127.0.0.1:8007 按照这个方法可以把槽分配给其他节点 把所有槽都分配完成后，可以去删除节点 ./redis-trib.rb del-node 分配一个Ip地址:端口 运行的runId 示例 1$ ./redis-trib.rb del-node 127.0.0.1:8000 9334fcdbc2453637e86f1b6664a8c86413eb87d4 客户端首先简单讲一下ASK重定向和Moved重定向 1234进入redis-cli$ redis-cli -p 8000$ set k1 v1(error) MOVED 12706 192.168.25.155:8002 这个就是MOVED异常 什么意思呢？就是说，当你要设置一个key的时候，如果当前这个key的hash值计算出来的槽不在当前节点，就会报错，错误会告诉你应该去哪个节点里面进行一个set操作 12345进入集群下的redis-cli$ redis-cli -c -p 8000$ set k1 v1-&gt; Redirected to slot [12706] located at 192.168.25.155:8002OK 这就是Moved重定向，会自动帮我们进行一个重定向到8002端口的Redis然后进行一个set操作 接下来讲讲ASK重定向 什么是ASK重定向呢？ ask异常是在槽的迁移过程才会发生的 什么意思呢，比如访问k1这个槽的节点的时候，节点报ASK异常，说明这个k1当前在我这个节点，但k1这个槽即将迁移到新的节点当中，ask的重定向会返回一个迁移到的目标节点的信息 Smart客户端,会识别ask异常和moved异常来进行一个对节点的切换 JavaDemo： 123456789101112131415161718192021222324252627282930313233343536373839public class JedisClusterDemo &#123; private Logger logger = LoggerFactory.getLogger(JedisClusterDemo.class); private Set&lt;HostAndPort&gt; nodeSet = new HashSet&lt;&gt;(); private JedisCluster jedisCluster = null; private List&lt;String&gt; hostPortList = new ArrayList&lt;&gt;(); @Before public void setup()&#123; hostPortList.add(&quot;192.168.25.155:8000&quot;); hostPortList.add(&quot;192.168.25.155:8001&quot;); hostPortList.add(&quot;192.168.25.155:8002&quot;); hostPortList.add(&quot;192.168.25.155:8003&quot;); hostPortList.add(&quot;192.168.25.155:8004&quot;); hostPortList.add(&quot;192.168.25.155:8005&quot;); JedisPoolConfig jedisPoolConfig = new JedisPoolConfig(); for(String hostPort : hostPortList)&#123; String[] arr = hostPort.split(&quot;:&quot;); if(arr.length != 2)&#123; continue; &#125; nodeSet.add(new HostAndPort(arr[0],Integer.parseInt(arr[1]))); &#125; try &#123; jedisCluster = new JedisCluster(nodeSet,1000,7,jedisPoolConfig); &#125; catch (Exception e) &#123; logger.error(e.getMessage(),e); &#125; &#125; @Test public void testDemo()&#123; jedisCluster.set(&quot;k1&quot;,&quot;v1&quot;); System.out.println(jedisCluster.get(&quot;k1&quot;)); &#125; @After public void destroy()&#123; if (jedisCluster != null)&#123; jedisCluster.close(); &#125; &#125;&#125;","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"Redis-Sentinel(哨兵)","slug":"Redis-Sentinel(哨兵)","date":"2019-05-19T10:29:57.000Z","updated":"2019-08-25T00:58:08.494Z","comments":true,"path":"2019/05/19/Redis-Sentinel(哨兵)/","link":"","permalink":"http://yoursite.com/2019/05/19/Redis-Sentinel(哨兵)/","excerpt":"","text":"主从复制的问题手动故障的转移 假设master出现故障了，两个slave都出现了问题 这时候应该怎么解决呢？ 将上面的slave做一个slaveof no one的操作，将其变为独立的master 将客户端指向新的master 将下面的slave指向新的master 这个过程实际操作起来还是比较麻烦的，虽然说我们是可以使用脚本来进行执行，但是，编写脚本的所需要的能力还是比较大，怎么判断redis节点出现问题，怎么通知客户端去重新指向等等的问题 写能力和存储能力受限在主从复制的架构上，master节点负责写，slave节点负责读，即便有多个从节点，但这些从节点存储的数据也只是主节点的数据副本，实际上也就相当于数据只存储在主节点一台机器中 Redis Sentinel 架构 首先，你可以把一个sentinel想象是一个redis的进程，不同的是sentinel不负责存储数据，它是负责对redis的一个故障判断、故障转移以及通知客户端的功能。另外，由上图可以看出sentinel不是一个而是多个，这样一来可以保证我们判断故障的一个公平性（后面可以设置几个sentinel认为节点有故障才算数），同时也保证了我们的高可用（即当一个sentinel节点挂了，仍然可以保证我们这个sentinel机制是完美的）。 ​ 那对客户端来说就再也不会直接从redis中获取信息，也就是说在我们客户端中不会记录redis的地址（某个IP），而是记录sentinel的地址，这样我们可以直接从sentinel获取的redis地址，因为sentinel会对所有的master、slave进行监控，它是知道到底谁才是真正的master的，例如我们故障转移，这时候对于sentinel来说，master是变了的，然后通知客户端。而客户端根本不用关心到底谁才是真正的master，只关心sentinel告知的master。 Redis Sentinel故障转移的步骤 多个sentinel发现并确认master有问题。 选举出一个sentinel作为领导。（因为故障转移一系列操作只需要一个sentinel就可以完成） 从多个slave中选出一个slave作为新的master 通知其余slave成为新的master的slave 通知客户端主从变化（这样客户端就不会有读取失败的问题） 等待老的master复活成为新的master的slave（sentinel依然会对老的master进行监控是否复活） 这里简单提一下：我们的一套sentinel是可以监听多套master+slave的组合，这样可以有效节省资源，其中每套master+slave会使用一个master-name作为一个标识。 Redis Sentinel演示首先配置两个Redis，一主二从 主：port：8000 12345port 8000daemonize yespidfile /var/run/redis-8000.pidlogfile &quot;8000.log&quot;dir &quot;/redis/data&quot; 从节点：port:8001和8002 12345678910111213port 8001daemonize yespidfile /var/run/redis-8001.pidlogfile &quot;8001.log&quot;dir &quot;/redis/data&quot;slaveof 127.0.0.1 8000-----------------------------------port 8002daemonize yespidfile /var/run/redis-8002.pidlogfile &quot;8002.log&quot;dir &quot;/redis/data&quot;slaveof 127.0.0.1 8000 启动redis 123$ redis-server redis-8000.conf$ redis-server redis-8001.conf$ redis-server redis-8002.conf 配置sentinel 先从redis文件夹下拷贝sentinel.conf配置文件 1$ cat sentinel.conf | grep -v &quot;#&quot; | grep -v &quot;^$&quot; &gt; redis-sentinel-26379.conf 进行修改配置文件redis-sentinel-26379.conf 12345678910111213141516#端口号port 26379#以守护进程的方式启动daemonize yes#哨兵sentinel的工作目录dir /redis/data#日志文件名logfile &quot;26379.log&quot;# 监控名字为mymaster 的master 主机号为127.0.0.1 端口号为8000 当有两个sentinel认为这个master有问题就会执行相应的措施sentinel monitor mymaster 127.0.0.1 8000 2# 指定多少毫秒之后 主节点没有应答哨兵sentinel 此时 哨兵主观上认为主节点下线 默认30秒sentinel down-after-milliseconds mymaster 30000# 这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行同步sentinel parallel-syncs mymaster 1# 故障转移的超时时间 failover-timeoutsentinel failover-timeout mymaster 180000 启动sentinel 1$ redis-sentinel redis-sentinel-26379.conf 再去查看一下配置文件 发生配置文件发生了改变，删除掉一些默认配置，而且加上了一些配置信息，比如从节点的信息（因为我们一开始指定主节点的时候，sentinel会获取主节点的信息，从而知道了slave的信息） 在创建两个sentinel 12345$ sed &quot;s/26379/26380/g&quot; redis-sentinel-26379.conf &gt; redis-sentinel-26380.conf$ sed &quot;s/26379/26381/g&quot; redis-sentinel-26379.conf &gt; redis-sentinel-26381.conf#开启redis-sentinel$ redis-sentinel redis-sentinel-26380.conf$ redis-sentinel redis-sentinel-26381.conf 可以查看redis-sentinel 26381的信息或者26382的信息都是可以发现sentinel的数量已经变成了3个 1$ redis-cli -p 26381 info sentinel 使用客户端观察Java代码： 这里要注意一点，如果是本机则没事，如果是用虚拟机的朋友要把上面的127.0.0.1写成具体的虚拟机地址，比如我这里就是192.168.25.155，如果用的是Docker的朋友们，可以先commit然后再在启动的时候请使用–network=host（使用宿主机的IP地址） 1234567891011121314151617181920212223242526272829303132public class JedisSentinelPoolDemo &#123; private static Logger logger = LoggerFactory.getLogger(JedisSentinelPoolDemo.class); private String masterName = &quot;mymaster&quot;; private Set&lt;String&gt; sentine = new HashSet&lt;String&gt;(); private JedisSentinelPool jedisSentinelPool; @Before public void setup()&#123; sentine.add(&quot;192.168.25.155:26379&quot;); sentine.add(&quot;192.168.25.155:26380&quot;); sentine.add(&quot;192.168.25.155:26381&quot;); jedisSentinelPool = new JedisSentinelPool(masterName,sentine); &#125; @Test public void test01()&#123; int counter = 0; while(true)&#123; counter++; try(Jedis jedis = jedisSentinelPool.getResource())&#123; int index = new Random().nextInt(100000); String key = &quot;k-&quot;+index; String value = &quot;v-&quot;+index; jedis.set(key,value); if(counter % 100 == 0)&#123; logger.info(&quot;&#123;&#125; value is &#123;&#125;&quot;,key,jedis.get(key)); &#125; TimeUnit.MILLISECONDS.sleep(10); &#125;catch (Exception e)&#123; logger.error(e.getMessage(),e); &#125; &#125; &#125;&#125; 这个时候，我们的master是8000，如果停止掉master（模仿宕机） 1$ redid-cli -p 8000 shutdown Java客户端会报错，稍等一会，会发现，日志继续输出？？？ 这就是我们的Redis的Sentinel的机制 Sentinel会去检测Redis中的master是否故障，从而自动的将故障进行一个转移 这都是我们Sentinel去做的，省下了人工去操控的一个麻烦的过程 Sentinel的三个定时任务 每十秒对每个sentinel对master和slave执行info，可以发现slave节点和确认主从节点的关系 每两秒每个sentinel通过master节点的channel交换信息 每一秒每个sentinel对其他sentinel和redis执行ping（心跳检测） 主观下线和客观下线配置文件 12# 节点多久没响应，就对这个节点做主观下线sentinel down-after-milliseconds mymaster 30000 主观下线和客观下线的区别在于，主观下线仅仅是依赖于单独的一个sentinel，而客观下线相当于，超过多少个quorum 都认为这个节点是下线了，那么，这个节点从客观的角度，就是下线了，此时就叫做客观下线 领导者选举只需要一个sentinel节点就可以完成故障的转移 每个主观下线的Sentinel节点向其他Sentinel节点发送命令，要求将它设置为领导者 收到命令的Sentinel节点如果没有同意通过其他Sentinel节点发送的命令，那么将会同意该请求，否则拒绝 如果该Sentinel节点发现自己的票数已经超过Sentinel集合的半数而且超过quorum，那么将成为领导者 如果该过程有多个Sentinel节点成为了领导者，将进行一次重新选举 总结Redis Sentinel是Redis的高可用实现方案： 故障发现 故障自动转移 配置中心 客户端通知 Redis Sentinel从Redis2.8版本开始才正式生产使用，之前版本生产不可用 Redis Sentinel的Sentinel节点个数应该为奇数 Redis Sentinel节点不负责读写，负责监控，和管理节点","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"Redis-主从复制","slug":"Redis-主从复制","date":"2019-05-18T14:52:43.000Z","updated":"2019-08-25T00:58:01.098Z","comments":true,"path":"2019/05/18/Redis-主从复制/","link":"","permalink":"http://yoursite.com/2019/05/18/Redis-主从复制/","excerpt":"","text":"单机版的Redis单机版的Redis能够支持十万的QPS（QPS是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准） 假设需要一百万怎么办？那就多台Redis实现集群 单机版的Redis，万一坏了，怎么办，数据没了怎么办？那就多台Redis进行数据的一个复制 所以有了Redis的主从复制的概念 Redis主从复制主从复制可以理解为，一主多从或者一主一丛（一个主节点可以有多个从节点，一个从节点只能有一个主节点，数据流向是单向的，只能从master到slave） 可以做数据副本，也可以提高扩展读的性能 还可以做读写分离 Redis主从复制的配置实现主从复制有以下两种方式 slaveof命令 配置文件 上demo，首先是用slaveof 先走一遍主节点的配置文件redis-6379.conf 12345daemonize yeslogfile &quot;6379.log&quot;dbfilename dump-6379.rdbdir /redis/data#关闭RDB的save配置 再走一遍从节点的配置文件redis-6380.conf 123456daemonize yeslogfile &quot;6380.log&quot;dbfilename dump-6380.rdbdir /redis/dataslaveof 127.0.0.1 6379#关闭RDB的save配置 启动两个redis 先查看6379端口的redis 1$ info replication 然后查看6380端口的redis 根据上面的两个图，可以看到6379的redis是显示master也就是主节点 而6380的redis是现实slave也就是从节点 接下来是实例操作 1234# 在6379端口的redis进行set$ set k1 v1# 在6380端口的redis进行get$ get k1 会发现6380的redis是可以get到的 如果想将6380的redis变为单独的节点，也就是从节点的 可以动态修改 123$ slaveof no one#再查看信息的分片，会发现，变成master节点了$ info replication 原理主从复制的时候，我们并没有开启RDB和AOP，但是为什么从节点能够把主节点的数据来进行一个记录呢？ 这是因为主从复制的时候会自动触发RDB的产生，实质是主节点fork出一个子进程来进行数据的一个传输（相当于bgsave） 全量复制 内部使用psync ? -1 这个命令第一个参数是runId，第二个参数是偏移量，而由于是第一次复制，slave不知道master的runId，也不知道自己偏移量，这时候会传一个问号和-1，告诉master节点是第一次同步 当master接受到psync ？ -1 时，就知道slave是要全量复制，就会将自己的runID和offset告知slave slave会将master信息保存 master这时会做一个RDB的生成（bgsave） 将RDB发送给slave 将复制缓冲区记录的操作也发送给slave slave清空自己的所有老数据 slave这时就会加载RDB文件以及复制缓冲区数据，完成同步。 全量复制的开销 bgsave的开销，每次bgsave需要fork子进程，对内存和CPU的开销很大 RDB文件网络传输的时间（网络带宽） 从节点清空数据的时间 从节点加载RDB的时间 可能的AOF重写时间（如果我们的从节点开启了AOF，则加载完RDB后会对AOF进行一个重写，保证AOF是最新的） 主从复制的问题读写分离读写分离在数据库层面还是用得挺多的，大多数业务都是读多写少的场景，所以，进行读写分离，能够将读的流量分摊到别的节点中 当然也会存在一些问题： 复制数据延迟（数据复制的延迟，当主节点发生阻塞的时候，会导致读写不一致） 读到过期的数据（主节点删除过期key的时候还没同步到从节点的时候，从节点会有很多脏数据） 从节点故障（从节点宕机之后，如何将访问这个节点的请求进行一个转移） 主从配置不一致 mamemory不一致会导致丢失数据 数据结构优化参数的不一致，导致主从内存不一致 规避全量复制 第一次全量复制 不可避免（但是可以减少危害，小主节点的情况下和在低峰值的情况下做第一次全量复制） 节点运行ID不匹配（主节点重启，运行ID发生了变化） 不可避免 可以使用故障转移的方式，当主节点发生故障的时候，从节点去接替当作主节点 复制积压缓冲区不足（网络中断，部分复制无法满足） 增大复制缓冲区配置rel_backlog_size 规避复制风暴 单主节点复制风暴（主节点重启，或者更换主节点的情况下，会进行大批量的全量复制） 更换节点的拓扑图（没有一个完美的拓扑图，应该结合自己的实际场合来考虑） 单机器的复制风暴（主节点所在的机器挂掉，换一个新的机器，也会进行大批量的全量复制） 主节点分散多台机器上面","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"Redis-持久化","slug":"Redis-持久化","date":"2019-05-17T15:36:07.000Z","updated":"2019-08-25T00:59:59.776Z","comments":true,"path":"2019/05/17/Redis-持久化/","link":"","permalink":"http://yoursite.com/2019/05/17/Redis-持久化/","excerpt":"","text":"什么是持久化redis所有数据都是保存在内存当中的，对数据的更新将保存到磁盘上，称之为数据的持久化 如果没有持久化，那断电，宕机，都会导致一个数据的缺失 持久化的两种方式RDBRedis的数据是存储在内存当中的，RDB是通过快照的方式，将内存中的数据存储到硬盘当中的一个文件，这个文件就是RDB文件 RDB的三种触发机制 save(同步)：占用的内存比较小，对性能开销没那么大，但是一旦数据比较多的时候，就会导致阻塞 bgsave(异步)：fork出子进程，子进程来进行一个save操作，不会导致阻塞，但是相对内存的消耗会比较大 自动 全量复制 debug reload 在演示之前，我们需要对我们redis.conf配置文件来进行一个配置 123456789101112131415161718# 以守护进程的方式启动daemonize yes# 当运行多个redis服务时，需要指定不同的pid文件和端口pidfile /var/run/redis:6379.pid# 以端口号为6379启动port 6379# 配置日志文件的存放位置logfile &quot;6379.log&quot;# 存储至本地数据库时（持久化到rdb文件）是否压缩数据，默认为yesrdbcompression yes#当bgsave执行失败的时候，是否停止redis的工作stop-writes-on-bgsave-error yes#是否校验格式rdbchecksum yes#rdb文件名dbfilename dump6379.rdb# 工作目录（按照自己的电脑进行一个配置）dir /redis/data save（同步机制）1234#执行set方法$ set k1 v1#手动save保存$ save 查看data目录下，会发现有一个文件dump6379.rdb 删除文件再进行下一个实验 bgsave（异步机制）1234#执行set方法$ set k1 v1#手动bgsave保存$ bgsave 同样的，再data目录下，会发现有一个文件dump6379.rdb 删除文件再进行下一个实验 自动（根据配置文件的配置）我们查看一下redis.conf配置文件 12345678# 比如默认配置文件中的设置，就设置了三个条件## save 900 1 900秒内至少有1个key被改变# save 300 10 300秒内至少有300个key被改变# save 60 10000 60秒内至少有10000个key被改变save 900 1save 300 10save 60 10000 我这里将值修改如下 1save 10 1 重启redis redis-cli shutdown redis-server redis.conf 12#完成一次set$ set k1 v1 同样的，再data目录下，会发现有一个文件dump6379.rdb 全量复制全量复制的内容会在下一篇文章里描述，这里大致说一下，就是当有主从节点的情况下，从节点会复制主节点的数据，此时主节点会进行一个RDB的生成（与上述说的三种配置无关） debug reload 和 shutdown123# 在redis-cli模式下，以下两条命令都可以使redis发生持久化，生成RDB文件$ debug reload$ shutdown RDB的问题耗时，耗性能耗时，全部数据都要写入一个新的文件，O（N） fork(),，数据量很大的情况下，写的时候会占用很大内存 硬盘的IO性能 不可控，丢失数据这是最最重要的问题，你耗时，我大不了久一点，你好性能，大不了就堆配置 丢数据可不行啊，这是最大的问题 为什么会丢数据呢？？？ save和bgsave都是要手动触发 而配置文件的自动生成rdb文件也是有间隔的，总会存在丢失数据的可能性 AOFAOP就解决了上述一个问题，AOP的原理是这样子的，我每次执行一个set k1 v1的时候就回去AOF文件中去进行一个记录，说白了就是一个日志功能，万一，我执行完set后宕机了，只需要把日志里的动作再做一遍就可以保证数据的恢复 AOF的工作流程 命令写入:所有的写入命令会追加到aof_buf(缓冲区)中 文件同步:AOF缓冲区根据对应的策略向硬盘做同步操作 文件重写:随着AOF文件越来越大，需要定期对AOF文件进行重写，达到压缩的目的 重启加载:当Redis服务器重启时，可以加载AOF文件进行数据恢复 AOF的三种策略always：每次都写进everysec：每一秒写进一次no：根据操作系统决定一般来说，折中考虑，会选择everysec，即可保证数据的丢失不会太大，也可以保证IO的开销会没那么大 AOF重写什么是AOF重写，举个例子 123$ set k1 1$ incr k1$ incr k1 AOF会记录三条命令，但是，其实三条命令可以简化成一条命令的，会大大减少我们的AOF的文件，大大加速我们恢复的速度 1$ set k1 3 AOF重写的两种方式 bgrewriteaof，会fork出一个子进程来进行AOF的重写 配置文件 auto-aof-rewrite-min-size：AOF文件需要重写的大小是多少 auto-aof-rewrite-percentage：下一次重写距离这一次重写需要在文件提升多少的百分比的时候进行 aof_current_size：统计当前AOF的大小 aof_base_size：AOF上次启动和重写的尺寸 上demo 首先还是要修改配置文件 12345678910111213#开启append only模式之后，redis会把所接收到的每一次写操作请求都追加到appendonly.aof文件中appendonly yes#AOF的文件名appendfilename &quot;appendonly.aof&quot;#默认使用everysec策略appendfsync everysec#是否会在append的时候，由于请求过长，而阻止no-appendfsync-on-rewrite no#默认的aof的最小文件大小，以及增长率auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb#当aof文件出错的时候，重启的时候是否忽略错误aof-load truncated yes 123# 插入两条数据$ set k1 v1$ set k2 v2 去data目录下会发现有aof的文件生成 把文件删除掉 12# 手动生成AOF文件$ bgrewriteaof 可以看到也是有AOF文件生成的 RDB与AOF的区别 命令 RDB AOF 启动优先级 低 高 体积 小 大 恢复速度 快 慢 数据安全性 丢数据 根据策略决定 轻重 重 轻 RDB的选择 在单机操作大多数情况下，建议关闭 当数据恢复的量级比较大的情况下建议开启 在集群的情况下，建议从节点开 AOF的最佳选择 在单机大多数操作情况下，建议开启 AOF重写集中管理（防止Redis自动做重写操作而导致fork太多引起的内存不足等问题） 建议使用everysec策略 数据恢复AOF和RDB文件都可以用于服务器重启时的数据恢复","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"设计模式-工厂方法","slug":"设计模式-工厂方法","date":"2019-05-14T09:34:58.000Z","updated":"2019-11-18T06:35:19.892Z","comments":true,"path":"2019/05/14/设计模式-工厂方法/","link":"","permalink":"http://yoursite.com/2019/05/14/设计模式-工厂方法/","excerpt":"","text":"首先在介绍工厂方法之前，先介绍一个新的概念，简单工厂 简单工厂不是一种设计模式 简单工厂 由一个工厂对象来决定创建出哪一种产品类的示例 工厂类负责适合创建的对象比较少的场景 应用层只用传入工厂类的参数，对于如何创建，不关心创建对象的细节 工厂类的职责较重，因为所有对象创建都由工厂类，万一工厂类出了问题，会影响很大 先上Demo,比如我这里是一个玩具厂，生产的玩具有蜘蛛侠和钢铁侠吧,不用简单工厂模式的设计下： 1234567891011121314151617181920212223242526272829303132333435363738/** * 定义一个玩具抽象类 */abstract class AbstractToy &#123; public abstract void produce();&#125;/** * 蜘蛛侠 */class SpiderManToy extends Toy &#123; @Override public void produce() &#123; System.out.println(&quot;我是蜘蛛侠&quot;); &#125;&#125;/** * 钢铁侠 */class IronManToy extends Toy&#123; @Override public void produce() &#123; System.out.println(&quot;我是钢铁侠&quot;); &#125;&#125;/** * 测试类 */public class TestDemo&#123; public static void main(String[] args) &#123; Toy toy = new IronManToy(); toy.produce(); toy = new SpiderManToy(); toy.produce(); &#125;&#125; 这里面存在一个什么问题呢？ 现在我们只有两个玩具而已，如果是三个，四个呢？不断添加一个类来继承我们的抽象类吗？然后不断自己new一个类，这就是高耦合，实现类你要自己编写，业务层你又要自己编写，太麻烦了，看下面示例 12345678910111213141516171819202122** * 玩具工厂类 */class ToyFactory&#123; public static Toy getToy(String name)&#123; if(&quot;IronMan&quot;.equalsIgnoreCase(name))&#123; return new IronManToy(); &#125;else if(&quot;SpiderMan&quot;.equalsIgnoreCase(name))&#123; return new SpiderManToy(); &#125;else&#123; return null; &#125; &#125;&#125;public class TestDemo&#123; public static void main(String[] args) &#123; Toy toy = ToyFactory.getToy(&quot;IronMan&quot;); toy.produce(); toy = ToyFactory.getToy(&quot;spiderman&quot;); toy.produce(); &#125;&#125; 以上代码，将实现类和业务逻辑代码去分开了，业务层只需要去传参数，就可以获取到对应的实现类，如何获取到的细节可以完全不用管理，减少了实业务层和实现层的耦合度，可以分开两个人去对这两个模块进行一个单独的管理，也不会出现问题，业务层只需要传参，工厂那边负责对参数的接收和返回具体的一个实现类以及实现类的编写 以上，业务层已经很好了，但是工厂那一块我们是用了if…else…if这种方法特别不好，写工厂的人，还是和一开始一样，既要自己new具体的实现类，又要编写工厂类，其实本质还是没有发生改变，只是从业务层的麻烦移到工厂上了而已，以上就是我们的简单工厂的做法，所以说，为什么简单工厂只能适合少部分的实例的创建，就是因为一旦管理较多类的情况下，就会导致代码很冗余 工厂方法 定义一个创建对象的接口，让实现这个接口的类来决定实例化哪个类，工厂方法让类的实例化推迟到子类中进行 适用于创建对象需要大量重复代码的时候 应用层不依赖于产品类的实现细节 一个类通过其子类来指定创建哪个对象 重点再说一次：让实现这个接口的类来决定实例化哪个类，工厂方法让类的实例化推迟到子类中进行 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * 定义一个玩具抽象类 */abstract class Toy &#123; public abstract void produce();&#125;/** * 蜘蛛侠 */class SpiderManToy extends Toy &#123; @Override public void produce() &#123; System.out.println(&quot;我是蜘蛛侠&quot;); &#125;&#125;/** * 钢铁侠 */class IronManToy extends Toy &#123; @Override public void produce() &#123; System.out.println(&quot;我是钢铁侠&quot;); &#125;&#125;/** * 玩具工厂类 */abstract class ToyFactory &#123; public abstract Toy getToy();&#125;/** * 蜘蛛侠工厂 */class SpiderManToyFactory extends ToyFactory&#123; @Override public Toy getToy() &#123; return new SpiderManToy(); &#125;&#125;/** * 钢铁侠工厂 */class IronManToyFactory extends ToyFactory&#123; @Override public Toy getToy() &#123; return new IronManToy(); &#125;&#125;/** * 测试类 */public class TestDemo &#123; public static void main(String[] args) &#123; ToyFactory toyFactory = new SpiderManToyFactory(); Toy toy = toyFactory.getToy(); toy.produce(); &#125;&#125; 这里上一个类图： 一共包含三个大角色，产品，工厂，调用者 其中产品里包含了具体的产品实现 工厂里包含了具体的工厂用来创建具体的产品 使用反射123456789101112131415161718192021222324252627282930/** * 玩具工厂类 */class ToyFactory&#123; public static Toy getToy(Class c) &#123; AbstractToy toy = null; try &#123; toy = (Toy) Class.forName(c.getName()).newInstance(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; return toy; &#125;&#125;/** * 测试类 */public class TestDemo&#123; public static void main(String[] args) &#123; Toy toy = ToyFactory.getToy(IronManToy.class); toy.produce(); toy = ToyFactory.getToy(SpiderManToy.class); toy.produce(); &#125;&#125; 在这里，我使用了反射来解决了在业务层只需要传参数而不用关心其实例创建过程的细节，而每次添加新的类的时候，也不需要修改到工厂类的代码 总结这里讲了三种方法，最后进行一个总结 其中，简单工厂适合实例少的场景，优点是简单，易懂，缺点是违反了开闭原则 工厂方法，有点是，适用实例多的场景，遵循开闭原则，缺点是，比较繁琐 使用简单工厂配合反射，可以解决开闭原则的问题，缺点是使用反射会使程序的效率降低","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"Docker-本地镜像发布到阿里云","slug":"Docker-本地镜像发布到阿里云","date":"2019-05-10T19:48:47.000Z","updated":"2019-08-25T00:57:10.599Z","comments":true,"path":"2019/05/11/Docker-本地镜像发布到阿里云/","link":"","permalink":"http://yoursite.com/2019/05/11/Docker-本地镜像发布到阿里云/","excerpt":"","text":"前面讲到了DockerFile DockerFile是什么来的呢？这里重温一下，DockerFile就是用来构建我们的Docker镜像文件 为什么要学DockerFile?因为拉取的镜像往往功能比较单一，我们有时候会需要根据不同的业务来对我们的虚拟机进行一些定制，这时候就有两种方法可选，一种是DockerFile，一种是执行docker commit,其中DockerFile的灵活性更加高但同时也带有一定的学习难度 本次主要讲解的是本地镜像发布到阿里云 为什么要这么做呢？ 这和我们的代码需要被管理是一个道理，而且，好的东西，开源，不也挺好的吗 本地镜像发布到阿里云的流程 图片看上去很复杂，这里大致解释一下，首先找到一个没有被指向的地方开始看，入口就是DockerFile了，DockerFile build之后成为一个镜像，Docker跑起来就是一个容器，还有两个，一个是阿里云，一个是私有云，说白了就是，你推上去的这个镜像，希不希望让别人能够看到的意思 镜像的生成方法 DockerFile docker commit 将本地镜像推送到阿里云首先进入阿里云控制台创建一个镜像仓库用来存放镜像，代码源我这里选择的是本地仓库 点击管理进入 找到 3. 将镜像推送到Registry 接下来就可以在自己仓库里面可以看到自己push后的镜像 将阿里云的镜像拉取到本地直接docker push 自己的镜像 总结这里涉及到的三条命令 用docker登陆阿里云 用docker push到阿里云 用docker pull从阿里云拉取到本地 这里三条的命令都可以在我的仓库管理里面可以清楚查看，也不用过多的解释","categories":[{"name":"运维","slug":"运维","permalink":"http://yoursite.com/categories/运维/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"设计模式-原型","slug":"设计模式-原型","date":"2019-05-05T08:23:10.000Z","updated":"2019-11-18T07:34:36.812Z","comments":true,"path":"2019/05/05/设计模式-原型/","link":"","permalink":"http://yoursite.com/2019/05/05/设计模式-原型/","excerpt":"","text":"原型设计模式什么是原型设计模式呢？可以根据我们的名字来推测就是，首先要有个原型，然后根据原型不断来产生新的东西 普通的一个例子看以下代码 1234567891011class Demo&#123; &#125;class Use&#123; public static void main(String[] args) &#123; List&lt;Demo&gt; list = new ArrayList&lt;Demo&gt;(); for (int i = 0; i &lt; 100; i++) &#123; list.add(new Demo()); &#125; &#125;&#125; 这个代码，逻辑上没什么问题，就是new了100个Demo的实例嘛 但是 我这里new了100次啊，100次啊！！！ 太浪费了CPU内存了，结合我们对单例模式的了解，我们可不可以写成单例？肯定可以啊，但是单例获取的只是一个实例，假设我的业务需求真的需要100个呢？难道真的没办法只能new 100次了吗？？ 原型登场12345678910111213141516171819202122class Demo implements Cloneable&#123; private static Demo demo = new Demo(); //禁止实例化 private Demo()&#123;&#125;; //唯一接口获取实例 public static Demo getDemo() throws CloneNotSupportedException&#123; return demo.clone(); &#125; //重写clone方法 @Override protected Demo clone() throws CloneNotSupportedException &#123; return (Demo)super.clone(); &#125;&#125;public class Use&#123; public static void main(String[] args) throws CloneNotSupportedException &#123; List&lt;Demo&gt; list = new ArrayList&lt;Demo&gt;(); for (int i = 0; i &lt; 1000; i++) &#123; list.add(Demo.getDemo()); &#125; &#125;&#125; 来看看克隆前后的运行的时间对比（System.nanoTime()） 12克隆后：1571500克隆前：270300 为什么克隆后会比克隆前还要久，因为当前初始化一个对象，占用的资源太少了，本来就很快了，使用克隆的话，并不会加快速度，反而会更加慢 假设Demo类里加一行代码 1234private byte[] bytes = new byte[1024*5];克隆后：2940500克隆前：7022000 这里，原型设计模式核心思想是用到了克隆，克隆这个方法其实就是进行了一个内存块的复制，在创建对象成本较大，如初始化占用较长时间、占用大量cpu资源等，新的对象可以通过原型对象复制产生新的对象。 因为我们的原型设计模式设计到内存块的复制，这里就会产生两个问题 浅复制 深复制 了解对象和引用的区别的朋友都知道，如果单纯拷贝一个内存块，是完全不够的，为什么呢？因为引用！所以上述例子是实现不了引用类型的拷贝 浅拷贝看以下例子 1234567891011121314151617181920212223242526272829303132class A&#123; &#125;class Demo implements Cloneable &#123; private static Demo demo = new Demo(); public A a = new A(); // 禁止实例化 private Demo() &#123; &#125;; // 唯一接口获取实例 public static Demo getDemo() throws CloneNotSupportedException &#123; return demo.clone(); &#125; // 重写clone方法 @Override protected Demo clone() throws CloneNotSupportedException &#123; return (Demo) super.clone(); &#125;&#125;public class Use &#123; public static void main(String[] args) throws CloneNotSupportedException &#123; Demo demo1 = Demo.getDemo(); Demo demo2 = Demo.getDemo(); System.out.println(&quot;demo1:&quot; + demo1); System.out.println(&quot;demo2:&quot; + demo2); System.out.println(&quot;demo1:&quot; + demo1.a); System.out.println(&quot;demo2:&quot; + demo2.a); &#125;&#125; 打印结果如下： 1234demo1:demo02.Demo@15db9742demo2:demo02.Demo@6d06d69cdemo1:demo02.A@7852e922demo2:demo02.A@7852e922 出现了一个结果：Object的地址是一样的，说明这里的object是同一个对象，没有实现到拷贝，这就是所谓的浅拷贝，这里上一幅图以供理解 深拷贝12345678910111213141516171819202122232425262728293031323334353637383940class A implements Cloneable&#123; @Override protected A clone() throws CloneNotSupportedException &#123; return (A)super.clone(); &#125;&#125;class Demo implements Cloneable &#123; private static Demo demo = new Demo(); public A a = new A(); // 禁止实例化 private Demo() &#123; &#125;; // 唯一接口获取实例 public static Demo getDemo() throws CloneNotSupportedException &#123; Demo temp = demo.clone(); temp.setA(temp.a.clone()); return temp; &#125; private void setA(A a) &#123; this.a = a; &#125; // 重写clone方法 @Override protected Demo clone() throws CloneNotSupportedException &#123; return (Demo) super.clone(); &#125;&#125;public class Use &#123; public static void main(String[] args) throws CloneNotSupportedException &#123; Demo demo1 = Demo.getDemo(); Demo demo2 = Demo.getDemo(); System.out.println(&quot;demo1:&quot; + demo1); System.out.println(&quot;demo2:&quot; + demo2); System.out.println(&quot;demo1:&quot; + demo1.a); System.out.println(&quot;demo2:&quot; + demo2.a); &#125;&#125; 打印结果如下： 1234demo1:demo02.Demo@15db9742demo2:demo02.Demo@6d06d69cdemo1:demo02.A@7852e922demo2:demo02.A@4e25154f 原型设计模式的优缺点优点 如果创建新的对象比较复杂时，可以利用原型模式简化对象的创建过程，同时也能够提高效率 可以使用深克隆保持对象的状态 原型模式提供了简化的创建结构 缺点 在实现深克隆的时候可能需要比较复杂的代码。 需要为每一个类配备一个克隆方法，而且这个克隆方法需要对类的功能进行通盘考虑，这对全新的类来说不是很难，但对已有的类进行改造时，不一定是件容易的事，必须修改其源代码，违背了“开闭原则”。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"Docker---Docker容器数据卷","slug":"Docker-Docker容器数据卷","date":"2019-04-29T13:41:52.000Z","updated":"2019-08-25T00:56:52.322Z","comments":true,"path":"2019/04/29/Docker-Docker容器数据卷/","link":"","permalink":"http://yoursite.com/2019/04/29/Docker-Docker容器数据卷/","excerpt":"","text":"Docker数据卷使用Docker的时候，我们明白我们跑的都是一个个容器，比如Tomcat，Mysql，Redis等等，既然如此，必须有个东西—–》数据 数据我怎么做一个持久化呢？怎么获取呢？容器奔了我数据就没了吗？ 这时候就有数据卷的诞生 数据卷：一个存放数据的东西，在Docker当中叫做数据卷，可以理解为磁盘，持久存放 数据卷容器：那就是可以理解为存放硬盘的容器，可以理解为移动硬盘 说白点呢，数据是存放在数据卷当中，但是当多个数据卷要进行交互的时候，就需要一个容器来包揽住这些数据卷，这个容器就叫做数据卷容器 数据卷使用-v指令来进行挂载数据卷挂载数据卷就是将本机的某个数据卷（某个文件夹）与容器内共享 docker run -v 宿主机的路径:容器内的路径 镜像名字 12345现在根目录下创建个文件夹$ cd /$ mkdir Doroot执行命令$ docker run -it -v /Doroot:/aaaa --privileged=true centos 注意一下一定要加 –privileged=true 否则容器内部会没有权限 这样子在容器内就能获取到我们宿主机上的数据了，注意一点，此时修改宿主机或者容器内部的数据卷里的文件，都会受到影响，简单来说就是宿主机和容器内布共享数据卷里面的文件 使用DockerFile来添加数据卷以下代码就在容器内挂载了/BBBB，没有指明对应的宿主机位置 123FROM centosVOLUME [&quot;/BBBB&quot;]CMD /bin/bash 然后docker build建立镜像 查看一下容器内挂载的与宿主机对应的位置在哪 123456789101112&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;volume&quot;, &quot;Name&quot;: &quot;b3b394fac32e2e263e360de210b36665d03554425d23fe23ad8ebc54aa304716&quot;, &quot;Source&quot;: &quot;/var/lib/docker/volumes/b3b394fac32e2e263e360de210b36665d03554425d23fe23ad8ebc54aa304716/_data&quot;, &quot;Destination&quot;: &quot;/BBBB&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;&quot; &#125; ], /BBBB对应的宿主机位置为 /var/lib/docker/volumes/ b3b394fac32e2e263e360de210b36665d03554425d23fe23ad8ebc54aa304716/_data 因为我们也说过宿主机和容器内布，是共享的，我们可以亲自去测试一下，这里我就不演示了(如果要测试，记得加上特权模式 –privileged =true，否则会报没权限错误) 数据卷容器建立一个容器挂载数据卷，其他容器通过挂载这个容器（父容器）来实现数据共享，挂载数据卷的容器就是数据卷容器 容器之间传递共享 –volumes-from 123456//先创建一个父容器，挂载宿主机$ docker run -it -v /Doroot:/aaaa --name d01 --privileged=true centos//退出容器$ ctrl + P + Q//创建一个子容器$docker run -it --name d02 --volumes-from dc01 --privileged=true centos 此时d02的centos也挂载了容器卷，实现了容器卷的一个传递 此时数据共享，改变任意一个容器的数据卷信息，另外的容器都会受到影响 总结一下数据卷容器，总结之前先抛出几个问题 d01删除后 d02还有吗？ —-有 d01删除后,新建d03继承d02，d03有吗？ —有 d03继承d01后，删除d01,d02和d03可以共享吗？ —可以 结论：容器之间的配置信息的传递，数据卷的生命周期一直持续到没有容器使用为止","categories":[{"name":"运维","slug":"运维","permalink":"http://yoursite.com/categories/运维/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Docker-DockerFile","slug":"Docker-DockerFile","date":"2019-04-29T12:16:38.000Z","updated":"2019-08-25T00:57:05.747Z","comments":true,"path":"2019/04/29/Docker-DockerFile/","link":"","permalink":"http://yoursite.com/2019/04/29/Docker-DockerFile/","excerpt":"","text":"Docker深入前面讲解了Docker的基本概念以及Docker的安装，还简单跑了一个例子，相信能够对Docker有一个大致的了解 本章将对Docker进一步的探索 DockerFileDockerFile是用来构建Docker镜像的构建文件，是由一系列命令和参数构成的脚本。 DockerFile就是用来构建Docker镜像的，现在DockerHub上面有很多镜像，但是有时候我们会根据需求来自定义镜像，（比如说我们要改造Centos）常用两种方法： 我们在DockerHub上面拉取一个镜像下来，然后进入容器里面进行修改，接着进行docker commit（形成新的一个镜像） 我们自己编写DockerFile文件，因为所有镜像本质都是DockerFile，编写完自己构建出一个镜像 DockerFile和 docker commit 的区别： DockerFile更加灵活，我可以随时根据需求来更改我的文件而docker commit，不具备重复性，什么意思呢，就是说，在DockerFile我不要一个东西，只是删除一行就好了（因为这是构建文件，我把某一块去掉就可以了），但是如果用docker commit就不行了，你安装了好些东西，要删除可能得一个个卸载，而且有时候你想要回复到之前某个点的，就非常困难了，所以：构建镜像不推荐docker commit,请使用DockerFile 构建DockerFile三步骤 编写DockerFile文件 docker build docker run 写一个例子吧： 新建一个DockerFile文件编写以下内容 1234From centosENV mypath /tmpWORKDIR $mypathCMD /bin/bash 执行docker build命令 我这里遇到过一个小问题，很多也遇到了，就是build的时候报错 docker build -f 文件所在的位置 -t 名字：版本号 . 如果文件在当前目录下可以省略-f 1$ docker build -f /dockerFile/DockerFile -t mycentos:2.2 . 重要的事情说三遍 最后面有小数点！！！ 最后面有小数点！！！ 最后面有小数点！！！ 1234//不加小数点报错：$ &quot;docker build&quot; requires exactly 1 argument(s).//没启动docker报错：$ Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? 可以看到当前的目录是tmp，因为我构建镜像的时候就设置了WORKDIR 简单的例子讲完了： 现在看看常用的构建文件命令： FROM 基础镜像，当前镜像是基于哪个镜像，可以想象成类与类之间的继承关系 MAINTAINER，镜像的维护者的姓名和邮箱地址 RUN，容器构建时需要运行的命令 EXPOSE，当前容器对外暴露出的端口 WORKDIR，指定创建容器后，终端默认登陆进来的工作目录 ENV，用来构建镜像过程中设置环境变量 ADD，将宿主机的目录下的文件拷贝到镜像里面并且ADD命令会自动处理URL和解压tar COPY，类似ADD，拷贝文件和目录到镜像中。&lt;源路径&gt; 的文件/目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置 VOLUME，容器数据卷，用于保存和持久化工作 CND，指定一个容器启动时要运行的命令，DockerFile中可以有多个CMD指令，但只有最后一个生效，会被docker run 后面的参数给覆盖 ENTRYPOINT，和CMD一样是指定一个容器启动时要运行的命令，区别在于不会被覆盖，都能执行 ONBUILD，当构建一个被继承的Dockerfile时运行命令，父镜像在被子继承后父镜像的onbuild被触发 这个可能就需要多做才能熟悉其命令了 比如说我们拉取的Centos都是比较纯净的 怎么说呢，什么东西都没有装就对了 比如下面 1234# 继承自centos，在centos基础上安装vimFrom centosRUN yum -y install vimCMD /bin/bash 最后写一个命令稍微复杂一点的 123456789101112131415161718192021222324252627#继承原生TomcatFROM centos#设置作者和邮箱MAINTAINER ymbcxb&lt;353560278@qq.com&gt;#把宿主机当前上下文的c.txt拷贝到容器/usr/local中COPY c.txt /usr/local/cincontainer.txt#把java与tomcat添加到容器中ADD apache-tomcat-9.0.17.tar.gz /usr/local/ADD jdk-8u201-linux-x64.tar.gz /usr/local/#安装VIM编辑器RUN yum -y install vim#设置工作访问时候的落脚点ENV MYPATH /usr/localWORKDIR $MYPATH#配置Java与Tomcat的环境变量ENV JAVA_HOME /usr/local/jdk1.8.0_201ENV CLASSPATH JAVA_HOME/lib/dt.jar:JAVA_HOME/lib/tools.jarENV CATALINA_HOME /usr/local/apache-tomcat-9.0.17ENV CATALINA_BASE /usr/local/apache-tomcat-9.0.17ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin#容器运行时监听的端口EXPOSE 8080#启动时运行tomcat用ENTRYPOINT 和 CMD都可以#ENTRYPOINT [&quot;/usr/local/apache-tomcat-9.0.17/bin/startup.sh&quot;]#CMD [&quot;/usr/local/apache-tomcat-9.0.17/bin/startup.sh&quot;,&quot;run&quot;]#启动tomcat并且打印日志CMD /usr/local/apache-tomcat-9.0.17/bin/startup.sh &amp;&amp; tail - F /usr/local/apache-tomcat-9.0.17/bin/logs/catalina.out 温馨提示：记得将对应的Jar包和Tomcat的包和c.txt放到当前目录下 运行 1$ docker run -it -p 9080:8080 -d mytomcat:1.0 然后在浏览器输入对应的网址就可以看到Tomcat的页面，LinuxIp:9080 执行以下命令进入 1$ docker exec -it cotainerId /bin/bash 查看到是存在cincontainer.txt 总结： 关于命令的学习，还是需要多琢磨琢磨，比如你可以手写一个Centos（安装了openssh）这样子，下次你需要用虚拟机的时候只用开启多一个容器，就够了就可以进行SSH连接来玩，玩坏了不过删容器而已，比传统的下载一个虚拟机镜像要方便多了，大大大大大地提高学习效率。","categories":[{"name":"运维","slug":"运维","permalink":"http://yoursite.com/categories/运维/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Docker初探","slug":"Docker初探","date":"2019-04-23T10:34:02.000Z","updated":"2019-08-25T00:54:53.427Z","comments":true,"path":"2019/04/23/Docker初探/","link":"","permalink":"http://yoursite.com/2019/04/23/Docker初探/","excerpt":"","text":"Docker初识什么是Docker呢？ Docker是一个开源的引擎，可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器。开发者在笔记本上编译测试通过的容器可以批量地在生产环境中部署，包括VMs（虚拟机）、bare metal)、OpenStack 集群和其他的基础应用平台。 简单点理解呢：Docker是一个管理容器的平台 简单介绍一下容器和虚拟机： 虚拟机：虚拟出一套硬件，在其之上的一个运行一个完整的操作系统 容器：容器之间互相隔离，但共享这同一份硬件系统 我们平时会在电脑上安装各种软件，比如微信，QQ。那么我们Docker呢，在想的是把一个系统看成一个软件，我们安装一个软件（本质是一个系统），比如，你现在的操作系统有很多软件，假设，你要换到一台新的电脑是不是要重新安装软件呢，这就很麻烦了，我们可以直接把你的系统当作一个软件，直接一安装就和你旧的电脑一摸一样了。 Docker的好处： 轻：开启速度快，因为不像Vmware那样子有一套虚拟化硬件，所以快 开发运维一致性：不会再对环境不一样起争议 弹性大：这点解释下，比如我现在一台服务器不够用的情况下，理论上是要新建一台服务器，但是新建的过程是很费时间的，因为有各种配置文件，所以，在Docker里，当不够用的时候，不过是安装多一个容器的事情，容器里面什么都做好了 安全：怎么捣鼓都是在容器里，实在不行，删了重建而已，不会影响导别的容器 附上一张图，以供了解： 那么Docker有几个核心概念： 仓库 镜像 容器 简单描述如下： 容器呢就是我们实际运行的东西 镜像呢就是用生成容器 这里可以这么理解为：用面向对象的思想来说，镜像就是类，容器就是实例对象 仓库呢就是用来放镜像的 首先简单跑一个Docker例子首先开启我们的虚拟机,我这里是Centos7，不同虚拟机可能有一点点差别，总体来说差别不大： 安装docker这里要注意一点： Centos 需要内核版本为3.8以及以上才能运行Docker (命令不包含$符号) 12//查看内核版本$ uname -r 12//安装docker$ sudo yum install docker 启动docker123456//启动 docker$ systemctl start docker//将 docker 服务设为开机启动$ systemctl enable docker//查看 docker版本，验证启动成功$ docker -v 拉取一个Centos镜像拉取之前，先要搜索 1$ docker search centos OFFICIAL ：代表官方 这里拉取的时候，默认是很慢的，因为使用的源地址是国外的，速度非常非常慢 为了提高速度，可以换取阿里或者网易的源 我这里使用的是阿里云 首先得注册一个阿里云账号，进入控制台找到 在这里可以看到加速器的地址，由于每个人的加速器地址，我这里就教下怎么配置 123456//打开配置文件$ vi /etc/docker/daemon.json//配置下面这段&#123; &quot;registry-mirrors&quot;: [&quot;https://xxxxx.mirror.aliyuncs.com&quot;]&#125; 接下来开始正式拉取镜像 1$ docker pull centos 12//查看所有镜像$ docker images 什么！！！只有两百M？对的，就是这么小，为什么，因为没有虚拟出一套物理硬件，单纯在硬件之上的层次，就这么小 默认的就是拉取最新版 如果想拉6的怎么办 指定一下就可以了 1$ docker pull centos:centos6 那么我怎么知道标签是多少呢？？？ 去官网https://hub.docker.com/_/centos查就可以了 最后一步！！！ 创建容器12//docker run 镜像名字:标签,这里 -it代表交互$ docker run -it centos:latest 此时就跑起来了 这就跑起来了一个建立在你虚拟机上的一个centos，而且秒开 你可以在这个界面里输入一些Linux指令也是可以行的，如果要退出 有两种方式 1234//第一种 退出容器，且杀死容器，说白了就容器没了$ exec //第二种，退出容器，且容器后台运行，可以重新进入$ ctrl+p+q 12//检查所有容器的状态$ docker ps -a 最后列举一下Docker镜像和容器常用的基本命令 镜像的操作命令： 删除单个镜像 docker rmi name 删除多个镜像 docker rmi name1 name2 删除全部镜像 docker rmi -f $(docker images -qa) 运行一个镜像 docker run [OPTIONS] imagename OPTIONS说明 –name=”容器新名字”: 为容器指定一个名称 -d: 后台运行容器，并返回容器ID，也即启动守护式容器 -i：以交互模式运行容器，通常与 -t 同时使用 -t：为容器重新分配一个伪输入终端，通常与 -i 同时使用 -P: 随机端口映射 -p: 指定端口映射，有以下四种格式ip:hostPort:containerPort ip::containerPort hostPort:containerPort containerPort 容器的操作命令： 启动镜像 docker run images 查看哪些容器在运行 docker ps 查看所有容器 docker ps -a 停止运行中的容器 docker stop id/name 删除一个容器 docker rm id 删除全部容器 docker rm -f $(docker ps -a -q) 重新进入挂起的容器 docker attach id（直接进入容器启动命令的终端，不会启动新的进程） dicker exec -t 是在容器中打开新的终端，并且可以启动新的进程 从容器内拷贝文件到宿主机上 docker cp 容器id名字:路径 宿主机路径 提交容器副本使之成为新的镜像 docker commit 本章就到此结束了，算入个门吧，复杂的东西，后面再讲","categories":[{"name":"运维","slug":"运维","permalink":"http://yoursite.com/categories/运维/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"JVM---类加载全过程","slug":"JVM-类加载全过程","date":"2019-04-21T17:35:09.000Z","updated":"2019-08-25T01:00:18.231Z","comments":true,"path":"2019/04/22/JVM-类加载全过程/","link":"","permalink":"http://yoursite.com/2019/04/22/JVM-类加载全过程/","excerpt":"","text":"类加载先上图 那么类加载器就是位于Java虚拟机中，故名思意，类加载器就是用来加载类的 具体分析类的加载机制： JVM把class文件加载到内存，并且对数据进行校验，解析和初始化，最终形成JVM可以直接使用的Java类型的过程 加载：将字节码加载到内存 校验：分析是否存在不满足Java规范 准备：将类的变量即静态变量分配内存并设置类变量的初始值（举个例子 public static int i = 3; 在这个过程之后赋值成0，使用初始值） 解析：将虚拟机常量池的符号引用替换为直接引用（举个例子,int b = a; 这个可以理解为符号引用，直接引用是什么意思呢，将这种符号转换成对应的内存地址） 初始化：执行类构造器方法(区分开构造器方法)，(举个例子，上面在准备的阶段，将i的值赋值为0，此时才赋值为3) 类的引用类的引用：分两种，一种是主动引用，一种是被动引用，（会发生类的初始化） 类的主动引用 执行new这个动作的时候(A a = new A()) 调用类的静态成员的时候和静态方法(这个就很明显了，没有执行构造器方法，但是却已经初始化了（类构造器）) 使用反射机制 main方法所在的类 初始化一个类的时候，如果其父类未被初始化，就会先初始化其父类 类的被动引用 当访问一个静态域时，只有真正声明这个域的类才会被初始化 通过子类引用父类的静态变量，不会导致子类初始化 通过数组定义类引用，不会出发此类的初始化 引用常量不会触发此类的初始化（常量在编译期间就存入常量池） 下图解释一下 12345678910111213141516171819202122232425262728class A&#123; static int a = 3; static final int b = 4; static&#123; System.out.println(&quot;A的静态块&quot;); &#125;&#125;class B extends A&#123; static&#123; System.out.println(&quot;B的静态块&quot;); &#125;&#125;class C&#123; static&#123; System.out.println(&quot;C的静态块&quot;); &#125; public static void main(String [] args)&#123; //主动引用 A a = new A();//1 System.out.println(A.a);//2 Class.forName(&quot;&quot;);//3 B b = new B();//4 //被动引用 System.out.println(B.a);//不会加载子类B，会加载父类A，1，2 A a = new A[10];//3 System.out.println(B.b);//4 &#125;&#125; 类加载器的层次结构 引导类加载器(BootStrap Class Loader)，它用来加载 Java 的核心库(JAVA_HOME/jre/lib/rt.jar,或sun.boot.class.path路径下的内容)，非Java代码编写 扩展类加载器(Extensions Class Loader),用来加载 Java 的扩展库(JAVA_HOME/jre/ext/*.jar，或java.ext.dirs路径下的内容) 应用程序类加载器(Application Class Loader),它根据 Java 应用的类路径（classpath，java.class.path 路径下的内容）来加载 Java 类。一般来说，Java 应用的类都是由它来完成加载的 自定义类加载器，开发人员可以通过继承 java.lang.ClassLoader类的方式实现自己的类加载器，以满足一些特殊的需求。 12345678public class Demo &#123; public static void main(String[] args) &#123; System.out.println(Demo.class.getClassLoader());//AppClassLoader System.out.println(Demo.class.getClassLoader().getParent());//ExtClassLoader System.out.println(Demo.class.getClassLoader().getParent().getParent());//null &#125;&#125; 类的代理模式 代理模式：交给其他加载器来加载指定的类 双亲委托机制：加载某个类的时候，首先让父加载器去加载器，如果父加载器能加载就父加载器加载，不然才自己加载,补个伪代码,方便理解 123456789//加载类，类名class，使用classLoader加载器void loadClass(Class class,ClassLoader)&#123; //判断有无父加载器 if()&#123; getClass(class,classLoader.getParent()); &#125; //去加载 loadClass(class);&#125; 这样子有个什么问题呢？比如说Object类，String类，这都是引导类去加载的，所以，你就不可能手写一个Object类，替代掉原来的Object类，因为双亲委托机制，一定会让引导类先加载，加载到了就不会加载手写的，保证了一定的安全性。 补充：不是所有的类加载都是采用双亲委托机制，比如Tomcat服务器 自定义类加载器自定义类加载器就是使用自己手写的类加载器去加载，写个例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293import java.io.ByteArrayOutputStream;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;import java.io.InputStream;/** * 自定义类记载其 * 加载本地文件的类 */public class FileSystemClassLoader extends ClassLoader &#123; private String rootDir; /** * * @param rootDir 加载的类所在根目录 */ public FileSystemClassLoader(String rootDir) &#123; this.rootDir = rootDir; &#125; /** * 重写ClassLoader的findClass */ @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; //判断类是否已经被类加载器加载 Class&lt;?&gt; c = findLoadedClass(name); if(c!=null) &#123; return c; &#125;else &#123; //如果还没被加载，在这里使用双亲委托机制，让父加载器去加载 ClassLoader parent = this.getParent(); try &#123; c = parent.loadClass(name);//给父类加载 &#125;catch(ClassNotFoundException e) &#123; e.printStackTrace(); &#125; if(c!=null) &#123; return c; &#125; else &#123; byte[] classData = getClassData(name);//加载字节码 if(classData == null) &#123; throw new ClassNotFoundException(); &#125;else &#123; c = defineClass(name, classData, 0,classData.length);//把字节码转化为类 &#125; &#125; &#125; return c; &#125; /** * * @param className 类名 * @return 字节数组（字节码） */ private byte[] getClassData(String className) &#123; String path = rootDir +&quot;/&quot; + className.replace(&apos;.&apos;, &apos;/&apos;)+&quot;.class&quot;; InputStream is = null; ByteArrayOutputStream baos = new ByteArrayOutputStream(); try &#123; is = new FileInputStream(path); byte [] buffer = new byte[1024]; int len = 0; while((len = is.read()) != -1) &#123; baos.write(buffer,0,len); &#125; &#125;catch(FileNotFoundException e) &#123; e.printStackTrace(); return null; &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;finally &#123; if(is!=null) &#123; try &#123; is.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if(baos!=null) &#123; try &#123; baos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; return baos.toByteArray(); &#125;&#125; 12345678910111213public class Demo &#123; public static void main(String[] args) &#123; FileSystemClassLoader loader = new FileSystemClassLoader(&quot;f:/java&quot;); try &#123; Class&lt;?&gt; c = loader.loadClass(&quot;HelloWorld&quot;); System.out.println(c); &#125; catch (ClassNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; 线程上下文类加载器根据上面的类加载器的代理模式会遇到一些问题：比如说，Java提供了很多SPI（服务提供接口），例如JDBC，JDBC的接口是Oracle写的，但是JDBC的实现类是由多个产商写的，比如Mysql，Oracle等 SPI接口是Java的核心库的一部分，是由引导类加载器载的；SPI 实现的 Java 类一般是由系统类加载器来加载的。引导类加载器是无法找到 SPI 的实现类的，因为它只加载 Java 的核心库。它也不能代理给系统类加载器，因为它是系统类加载器的祖先类加载器。也就是说，类加载器的代理模式无法解决这个问题。 为了解决这个问题，Java设计团队只好引入了一个不太优雅的设计：线程上下文类加载器（Thread Context ClassLoader）。线程上下文类加载器是从 JDK 1.2 开始引入的。类 java.lang.Thread中的方法getContextClassLoader()和 setContextClassLoader(ClassLoader cl)用来获取和设置线程的上下文类加载器。如果没有通过 setContextClassLoader(ClassLoader cl)方法进行设置的话，线程将继承其父线程的上下文类加载器。Java 应用运行的初始线程的上下文类加载器是应用程序类加载器。在线程中运行的代码可以通过此类加载器来加载类和资源。 有了线程上下文类加载器，就可以做一些“舞弊”的事情了，JNDI服务使用这个线程上下文类加载器去加载所需要的SPI代码，也就是父类加载器请求子类加载器去完成类加载器的动作，这种行为实际上就是打通了双亲委派模型的层次结构来逆向使用类加载器，已经违背了双亲委派模型的一般性原则。 线程上下文类加载器是为了抛弃双亲委派加载链模式Demo： 12345678910public class ThreadClassLoader&#123; public static void main(String[] args) &#123; ClassLoader loader = ThreadClassLoader.class.getClassLoader(); System.out.println(loader); ClassLoader loader2 = Thread.currentThread().getContextClassLoader(); System.out.println(loader2); Thread.currentThread().setContextClassLoader(new FileSystemClassLoader(&quot;F:/&quot;) ); System.out.println(Thread.currentThread().getContextClassLoader()); &#125;&#125;","categories":[{"name":"Java虚拟机","slug":"Java虚拟机","permalink":"http://yoursite.com/categories/Java虚拟机/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}]},{"title":"Web登陆其实没那么简单","slug":"Web登陆其实没那么简单","date":"2019-04-11T13:15:01.000Z","updated":"2019-08-25T00:52:42.542Z","comments":true,"path":"2019/04/11/Web登陆其实没那么简单/","link":"","permalink":"http://yoursite.com/2019/04/11/Web登陆其实没那么简单/","excerpt":"","text":"一个简单的HTML例子看看用户信息安全标准的HTML语法中，支持在form表单中使用input标签来创建一个HTTP提交的属性，现代的WEB登录中，常见的是下面这样的表单： 12345&lt;form action = &quot;http://localhost:8080/Application/login&quot; method = &quot;POST&quot;&gt; 用户名：&lt;input id=&quot;username&quot; name=&quot;username&quot; type=&quot;text&quot; /&gt; 密码：&lt;input id=&quot;password&quot; name=&quot;password&quot; type=&quot;password&quot; /&gt; &lt;button type=&quot;submit&quot;&gt;登陆&lt;/button&gt;&lt;/form&gt; form表单会在提交请求时,会获取form中input标签存在name的属性，作为HTTP请求的body中的参数传递给后台，进行登录校验。 例如我的账号是user1，密码是123456，那么我在提交登录的时候会给后台发送的HTTP请求如下（Chrome或者FireFox开发者工具捕获，需开启Preserve log）： 可以发现即便password字段是黑点，但是本机仍以明文的形式截获请求。 HTTP协议传输直接暴露用户密码字段在网络传输过程中，被嗅探到的话会直接危及用户信息安全，以Fiddler或Wireshark为例，发现捕获的HTTP报文中包含敏感信息： 使用加密算法能保证密码安全吗？WEB前端可以通过某种算法，对密码字段进行加密后，在将密码作为Http请求的内容进行提交，常见的包括对称和非对称加密。 对称加密:采用对称密码编码技术，它的特点是文件加密和解密使用相同的密钥加密。 非对称加密:需要两个密钥，公开密钥（publickey）和私有密钥（privatekey）。公开密钥与私有密钥是一对，如果用公开密钥对数据进行加密，只有用对应的私有密钥才能解密；如果用私有密钥对数据进行加密，那么只有用对应的公开密钥才能解密。 3.1 使用对称加密加密解密在前后台协商后，似乎是个不错的办法，比如，前台使用一个字符串位移+字符串反转的简单方法（举个例子，当然不能这么简单）。 那么这样简单的方法似乎可以混淆原密码，并且轻松由后台进行相反操作复原。但是这有两个缺点： 前后端加密解密需要同时修改代码； 前端加密无非是写在JS里，但是JS有风险被直接破解从而识别加密方法。 3.2非对称加密HTTPS就一定是安全的吗？非对称加密有着公钥私钥的存在，公钥可以随意获取，私钥是用来对公钥解密的本地存储，通过公私钥的机制似乎可以保证传输加密并且乃至现在还在使用的HTTPS就是基于这个原理。但是HTTPS就一定安全吗？HTTP存在两种可能的风险： HTTPS可以保证传输过程中的信息不被别人截获，但是细细思考下，HTTPS是应用层协议，下层采用SSL保证信息安全，但是在客户端和服务端，密文同样是可以被截获的； HTTPS报文在传输过程中，如果客户端被恶意引导安装“中间人”的WEB信任证书，那么HTTPS中的“中间人攻击”一样会将明文密码泄露给别人。 结论是，无论HTTP还是HTTPS，密码必须密文传输想想HTTPS也不能一定保障用户密码信息，那么就应该考虑在应用层之上再继续对密码进行保护，也就是编写代码来进行控制，而不依赖特定协议，比较容易想到的就是利用不可逆加密散列函数MD5(string)，用户在注册输入密码的时候，就存储MD5(password)值，并且在WEB端先进行MD5(password)，然后将密码传输至后台，与数据库中的密文进行比较（PS：MD5函数在指定位数的情况下，对相同字符串运算值相同）。优点比较明显： 保证了用户数据库内部的密码信息安全； 传输过程中无论如何都不会使得用户的密文被破解出原密码； 简单高效，执行以及编码难度都不大，各种语言都提供MD5支持，开发快。 那太好了！这样可以省下HTTPS的钱了，真是这样吗？回到开头的例子：用户输入的用户名是：user1，密码是：123456，那么不管在什么协议之下，可以看到实际发送的HTTP/HTTPS报文在MD5处理后是这样的： 没错，加密登录成功了。但是，当我们庆祝密码安全的时候，发现账户的钱突然不翼而飞。这是为什么呢？黑客却笑的很开心：因为他们并不一定要获取到你的密码明文，如果直接截获你的密码密文，然后发送给服务器不是一样可以登录吗？因为数据库里的不也是MD5(password)的一样的密文吗？HTTP请求被伪造，一样可以登录成功，从而攫取其他的数据或者转走余额。 这怎么办?其实并不难，有很多种解决方法？其实原理都是类似的：那就是服务器缓存生成随机的验证字段，并发送给客户端，当客户端登录时，把这个一并字段传给服务器，用于校验。 方案一：验证码MVC场景。控制器将把数据的Model封装到View中，这种存在Session的连接方式，允许了在Session中存取信息。那么我们可以利用一些开源的验证码生成工具，例如JAVA中的Kaptcha，在服务端存放生成一个验证码值以及一个验证码的生成图片，将图片以Base64编码，并返回给View，在View中解码Base64并加载图片，并于用户下次登录时再进行比对。 方案二：token令牌前后端分离场景。现在非常流行的前后端分离的开发模式大大提高了项目的开发效率。职责、分工明确，但是由于HTTP是无状态的（就是这一次请求并不知道上一次请求的内容），当用户登录时，根据用户的username作为key，生成随机令牌（例如UUID）作为value缓存在Redis中，并且将token返回给客户端，当客户端登录时，将完成校验，并且删除Redis中的那条缓存记录。 那么每次从服务器中获取认证的token，确实能保证HTTP请求是由前端传回来的了，因为token在每次登陆后都会删除并被重置，会导致黑客尝试重放账号密码数据信息来登陆的时候导致无法成功登陆。 总而言之，就是我拿到了账号以及密码的密文也登陆不了，因为，如果请求不包含后台认证的令牌token，是个非法请求。 可是还别高兴的太早，当心数据被篡改密码也加密了，黑客看不到明文了。加上Token了，登陆过程也没法再被截获重放了。可是想想这种情况，你在进行某宝上的网络支付，需要账号，密码，金额，token这四个字段进行操作，然后支付的时候你付了1块钱买了一袋包邮的小浣熊干脆面，某宝结算结束后，你发现你的账户余额被扣了1万元。这又是怎么回事呢？ 因为即便黑客不登录，不操作，一样要搞破坏：当请求路由到黑客这边的时候，截获数据包，然后也不需要登录，反正账号密码都是对的，token也是对的，那么把数据包的字段改改，搞破坏就可以了，于是把money改成了1万，再传给服务器，作为受害者就莫名其妙踩了这个坑。可这该怎么解决呢？其实原理类似于HTTPS里的数字签名机制，首先科普下什么是数字摘要以及数字签名： 什么是“数字摘要”我们在下载文件的时候经常会看到有的下载站点也提供下载文件的“数字摘要“，供下载者验证下载后的文件是否完整，或者说是否和服务器上的文件”一模一样“。其实，数字摘要就是采用单项Hash函数将需要加密的明文“摘要”成一串固定长度（128位）的密文，这一串密文又称为数字指纹，它有固定的长度，而且不同的明文摘要成密文，其结果总是不同的，而同样的内容信息其摘要必定一致。 因此，“数字摘要“叫”数字指纹“可能会更贴切一些。“数字摘要“是HTTPS能确保数据完整性和防篡改的根本原因。 数字签名–水到渠成的技术假如发送方想把一份报文发送给接收方，在发送报文前，发送方用一个哈希函数从报文文本中生成报文摘要,然后用自己的私人密钥对这个摘要进行加密，这个加密后的摘要将作为报文的”签名“和报文一起发送给接收方，接收方首先用与发送方一样的哈希函数从接收到的原始报文中计算出报文摘要，接着再用发送方的公用密钥来对报文附加的数字签名进行解密，如果这两个摘要相同、那么接收方就能确认报文是从发送方发送且没有被遗漏和修改过！这就是结合“非对称密钥加解密”和“数字摘要“技术所能做的事情，这也就是人们所说的“数字签名”技术。在这个过程中，对传送数据生成摘要并使用私钥进行加密地过程就是生成”数字签名“的过程，经过加密的数字摘要，就是”数字签名“。 因此，我们可以在WEB端对之前案例中提到的username+MD5(password)+token通过签名，得到一个字段checkCode，并将checkCode发送给服务器，服务器根据用户发送的checkCode以及自身对原始数据签名进行运算比对，从而确认数据是否中途被篡改，以保持数据的完整性。 总结看似非常简单的WEB登录，其实里面也存在着非常多的安全隐患。这些安全完善的过程是在一个实际WEB项目中遇到的，上面的分析演化是在应对项目安全的检查中所提出的解决方案，多少会有很多不足的地方，希望一起交流探讨，共同进步！ 补充1：JS加密函数存在被破解问题： 12如果黑客通过阅读前端js源码,发现加密算法,是否意味他可以构造可以被服务端解密的checkCode 来欺骗服务端呢 ? 回答: 1摘要或加密JS算法不直接以静态文件的形式存在浏览器中，而是让WEB端去请求Server，服务器可以根据随机令牌token值决定返回一个相应随机的加密策略，以JS代码响应的方式返回，在异步请求响应中，加载JS摘要算法，这样客户端就可以动态加载数字摘要策略，保证无法仿造。 补充2：MD5存在隐患的问题问题： 1用MD5、SHA256 处理密码的过时了。。。现在 PBKDF、bcrypt 都在过时中。 回答： 123456789本文重点侧重于方法思路的介绍，并不一定是要使用MD5函数，可以使用其他的方式。MD5存在隐患，之前确实没有考虑太多，不过非常感谢园友指出，确实是这样的，主要思想是：对于MD5的破解，实际上都属于【碰撞】。比如原文A通过MD5可以生成摘要M，我们并不需要把M还原成A，只需要找到原文B，生成同样的摘要M即可。设MD5的哈希函数是MD5()，那么：MD5(A) = MMD5(B) = M任意一个B即为破解结果。B有可能等于A，也可能不等于A。大概意思也就是，截获了MD5加密后的密文，一样可以，找到一个不是原密码，但是加密后可以登陆成功的“伪原文”。 CSDN有一篇关于MD5风险的博客写的非常好，推荐一下：MD5算法如何被破解 从中可以看到一点，MD5函数确实能被反向“破解”，但是这个“破解”只是找到一个经过MD5运算后得到相同结果的原文，并非是用户的明文密码。但是这样会被破解登录的可能，确实是需要采用更完善的算法进行加密，再次感谢！","categories":[{"name":"技术分享","slug":"技术分享","permalink":"http://yoursite.com/categories/技术分享/"}],"tags":[]},{"title":"一条SQL语句的执行过程","slug":"一条SQL语句的执行过程","date":"2019-04-09T08:33:37.000Z","updated":"2019-08-25T00:47:19.329Z","comments":true,"path":"2019/04/09/一条SQL语句的执行过程/","link":"","permalink":"http://yoursite.com/2019/04/09/一条SQL语句的执行过程/","excerpt":"","text":"Mysql基本架构示意图 大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。 Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL5.5.5 版本开始成为了默认存储引擎。 也就是说，你执行 create table 建表的时候，如果不指定引擎类型，默认使用的就是InnoDB。不过，你也可以通过指定存储引擎的类型来选择别的引擎，比如在 create table语句中使用 engine=memory, 来指定使用内存引擎创建表。不同存储引擎的表数据存取方式不同，支持的功能也不同 连接器连接器是连接我们客户端和服务器首先经过的第一个功能模块，当我们连接我们的Mysql的时候，首先就会执行一条语句 1mysql -h[ip地址] -P[端口号] -u[用户名] -p[密码] 这条语句做的事情就是让本机连接上远端或者本地的Mysql服务器，从而进行管理Mysql 此时连接器就是用来验证我们的用户名和密码是否正确的，正确则可以得到操作的权限，其实就是和我们网站的第一个入口—&gt;登陆是一个意思，进行用户验证 如果连接了之后,没有别的操作，则当前连接就正处于一个空闲状态 当连接上数据库的时候，会保持一个连接，这个连接是长连接，直到连接关闭，否则一直保持连接，中途所有的客户端请求，都由该连接进行工作。短连接只能保证客户端在某个时间段的请求由该连接进行工作，下次查询将会使用一个新的连接 使用长连接也会由问题： 因为长连接保持连接，所以长时间下来，当连接数很多的时候，所占用的内存就会很多 解决办法如下： 1.定期断开长连接 2.Mysql5.7版本之后，每次执行一个比较大的操作后，通过执行mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验证，速度会比较快 查询缓存我们首先可以查看mysql关于缓存的配置 query_cache_type: 查询缓存类型,是否打开缓存 可选项 1、0(OFF)：关闭 Query Cache 功能，任何情况下都不会使用 Query Cache； 2、1(ON)：开启 Query Cache 功能，但是当SELECT语句中使用SQL_NO_CACHE提示后，将不使用Query Cache； 3、2(DEMAND)：开启Query Cache 功能，但是只有当SELECT语句中使用了SQL_CACHE 提示后，才使用Query Cache。 备注1: 如果query_cache_type为on而又不想利用查询缓存中的数据，可以用下面的SQL： 1SELECT SQL_NO_CACHE * FROM my_table WHERE condition; 如果值为2，要使用缓存的话，需要使用SQL_CACHE开关参数： 1SELECT SQL_CACHE * FROM my_table WHERE condition; 但是大多情况下，建议不要使用查询缓存，因为查询缓存的失效非常频繁，任意一个表的更新操作，这个表上的所有查询缓存都会被清空了，所以查询缓存的命中率非常低，除非表中基本上只会涉及到查操作而很少更新操作的时候可以考虑使用查询缓存的方式 Mysql8.0没有查询缓存的功能，已被彻底抛弃 分析器往上说的，如果开启了缓存机制的，就在缓存里查，否则就要走分析器了，所谓分析器，就是判断你输入的这条语句到底是什么意思，是要做查操作呢还是更新操作呢，还是语法有错误呢，都是由分析器去做的事情 优化器经过了分析器，Mysql就知道你要做的是查还是更新还是什么，此时，到优化器，顾名思义，就是如何优化你查的方式 比如： 1select * from t1 join t2 using(ID) where t1.c = 10 and t2.d =20 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2里面 d 的值是否等于 20。 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1里面 c 的值是否等于 10。 这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。 执行器通过分析器，知道要做什么了，通过优化器知道选择什么的方式做了，执行器就是根据上面的两个结论去做，调用InnoDB引擎去取表中的数据 总结这就是Mysql的逻辑架构，对一个Sql语句完整执行流程的各个阶段有一个初步的认识","categories":[{"name":"Mysql","slug":"Mysql","permalink":"http://yoursite.com/categories/Mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://yoursite.com/tags/Mysql/"}]},{"title":"Centos搭建git私服","slug":"Centos搭建git私服","date":"2019-03-26T12:14:48.000Z","updated":"2019-08-25T01:00:35.453Z","comments":true,"path":"2019/03/26/Centos搭建git私服/","link":"","permalink":"http://yoursite.com/2019/03/26/Centos搭建git私服/","excerpt":"","text":"Centos搭建GIT私服安装gitCentos默认自带Git 可以通过以下命令进行查看 1git --version 默认是1.8 创建用户123groupadd gitadduser git -g gitpassword git 先创建一个用户组 再在这个用户组里面创建一个用户 再给用户设置密码 创建authorized_keys文件1234567cd /home/gitmkdir .sshchmod 700 .sshtouch .ssh/authorized_keyschmod 600 .ssh/authorized_keyscd /homechown -R git:git git 要注意的是文件权限和所属用户。 (后续的git clone如果需要密码，很有可能是git用户没有访问authorized_keys文件的权限) 客户端创建密钥并上传1ssh-keygen -t rsa -C &quot;your_email&quot; 该命令会产生两个文件: id_rsa对应私钥，id_rsa.pub对应公钥。 将id_rsa.pub中的内容写到服务器的authorized_keys文件中。 如果有多个客户端，那么在authorized_keys文件中，一行保存一个客户端的公钥。 创建git仓库为了方便管理，所有的git仓库都置于同一目录下，假设为/home/gitrepo， 123cd /homemkdir gitrepochown git:git gitrepo 接下来创建git仓库：test.git 12cd gitrepogit init --bare test.git 把仓库所属用户改为git 1chown -R git:git test.git 注意每次新建的仓库，都要修改仓库的所属用户 git私服搭建完毕push 和 clone示例 打开git bash 1234git clone git@ip:/home/gitrepo/test.gitgit remote add origin git@ip:/home/gitrepo/test.gitgit push -u origin","categories":[{"name":"技术分享","slug":"技术分享","permalink":"http://yoursite.com/categories/技术分享/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"}]},{"title":"Git-进阶","slug":"Git-进阶","date":"2019-01-31T13:58:03.000Z","updated":"2019-08-25T00:54:02.594Z","comments":true,"path":"2019/01/31/Git-进阶/","link":"","permalink":"http://yoursite.com/2019/01/31/Git-进阶/","excerpt":"","text":"Git进阶本地分支管理一个项目如果是你一个人开发，那就没什么，反正就一份项目，你想做哪个模块就做哪个模块但是这样子开发效率慢啊 要是两个人开发，开发效率提高一倍，那么怎么合并起来呢？？？ 如果说两个人做的地方，刚好互不影响，那很简单，学过数学的人都知道，直接合并起来嘛 问题来了！！！ 如果两个人做的地方有重复怎么办？？？一行一行对比，然后一行行改?还是说一个人做完再让另外一个人做？那这和一个人做有什么区别 这就太麻烦了 所以引入了分支管理！这是个好东西 创建分支根据上图我们可以知道，我们首先是有一个主干的(称为master分支) 我们每次提交，会多一个新的节点 提交越多，master分支也会边长 创建分支的代码：(新的分支名字为dev) 1git branch dev 我们可以看到一个*标记在master，这代表着我们操控着master分支假设我们要切换分支： 1git checkout dev (我们也有一个更加方便的代码，创建分支同时切换到该分支上)： 1git checkout -b dev 接下来，我们执行一次提交 然后我又返回到master里再提交一次 合并分支首先切换到master分支将dev分支合并到master分支下 1git merge dev 删除分支1git branch -d dev GitHub上实现分支管理上面说这么多，其实为了熟悉一下命令，还有通过一些图来表明，每个操作，到底发生了什么 接下来才是实际开发中会用到的，结合GitHub上讲解 首先我拉取一下我Github，我仓库里只有一个A.txt,里面有句Hello 我新建两个文件夹用来模拟两台电脑 接着做如下操作 然后把文件都添加到本地仓库中 接着在Git2里推送到github上的分支下12git checkout -b devgit push -u origin dev 这样子在Github上就有分支了 接着Git去上传自己的到github的master下1git push -u origin master 在创建一个文件夹Git3 分别把两个分支拉下来 1234567git clone 路径进入仓库git checkout dev git clone -b dev(分支名) 路径git checkout mastergit merge devgit push -u origin master","categories":[{"name":"技术分享","slug":"技术分享","permalink":"http://yoursite.com/categories/技术分享/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"}]},{"title":"Git入门","slug":"Git入门","date":"2019-01-12T16:13:30.000Z","updated":"2019-08-25T00:53:54.616Z","comments":true,"path":"2019/01/13/Git入门/","link":"","permalink":"http://yoursite.com/2019/01/13/Git入门/","excerpt":"","text":"版本控制器：首先了解Git之前要明白另外一样东西，那就是版本控制系统 什么叫版本控制系统呢？ 版本控制是指对软件开发过程中各种程序代码、配置文件及说明文档等文件变更的管理，是软件配置管理的核心思想之一。 程序员写代码，学会一种版本控制器是必不可少的技能 写代码，少不了的就是不断的修改源代码，但是代码是一种很神奇的东西，你会发现，原本只有一个BUG的改着改着突然发现BUG数量不仅仅没有减少而且还增加，此时的心里感受，哈哈，应该要吐血，那么怎么办？我原来的代码也被我改掉了说白了就是已经没有的备份，那就凉了~ 好的，小白曾经是这么干的，比如说，出现BUG吗，那我要修改，修改之前，先拷贝一份现在的源文件，然后再去改，这样子就稳了 当然小白一开始做得还是很开心的，因为很简单啊，写的代码也不多，而且往往只是一个文件，随着小白的进步当中，代码往往牵涉到多个文件的修改，这样子也是可以ctrl+c和ctrl+v的，但是一是一个文件夹里密密麻麻的项目，二是，看着一堆副本一副本二，简直就是头疼啊！！！ 好啦，现在开始讲我们的版本控制器了 简单来说呢，版本控制器就是可以更加方便地对我们的代码进行管理 进入正题： Git:Git：分布式版本控制系统 那么有分布式那么就会有集中式，没错！ SVN就是集中式版本控制系统，本章不做介绍 Git很火的呢，怎么说，GitHub就是基于Git的基础上的 至于分布式版本系统控制系统和集中式版本控制系统，等我把Git和SVN讲解完再进行一个比较 Git的安装Git的安装可以参考官网https://git-scm.com/downloads直接下载后安装后就可以了，很简单 安装后在开始菜单中找到Git Bash鼠标右键也会有 Git的入门我首先创建了一个文件夹，专门进行讲解文件夹名字为Git，首先创建一个文件，名字为a.txt，内容为AAA 接下来进行提交 在文件夹里面右键 git Bash 初始化首先我们要初始化一个仓库，这个仓库是放在我们本机的 1git init 默认为隐藏的，要设置查看隐藏文件就可以看得到多了一个文件夹 添加将文件添加到仓库里 1git add a.txt 提交将文件提交到仓库里 1git commit -m &quot;version 1.0&quot; -m后面是本次提交的一个说明，就是提交的是什么东西(原则上可以省略，但是建议不要) git add 和 git commit的区别首先git add添加自己指定的文件git commit不可以，一次性将所有提交，不可指定文件 git add是先将文件添加到暂存区，git commit将暂存区里的文件一次性提交到仓库里！！！ 基本上简单的一个Git流程的上传部分就介绍完了 下载文件既然前面我们讲完了如何将文件上传到自己的仓库，那么我们现在要用到的情况下，怎么从仓库下载下来呢？？？ 现在为了模仿以下真实环境 我进行多了两次提交 第一次提交之前，在a.txt增加了一行BBB 第二次提交之前，在a.txt增加了一行CCC 接下来我们就要去查看记录了 1git log 当前TXT是这样子的 现在要恢复啦！！！见证奇迹的时刻——- 1git reset --hard HEAD^ 再打开文本，发现已经恢复了，用git log去查看的时候发现也确实少了一条记录HEAD代表的是当前版本HEAD^代表的是当前版本的上一个版本HEAD^^代表的是上上版本 HEAD~N 代表的是当前邦本的上N个版本 远程仓库GitHub是提供Git仓库托管服务的,所以首先要有一个GitHub账号 自行注册GitHub的账号，这个问题，不在这里描述了 其次注册完账号还有做一些事情 先创建一个SSH KEY 1ssh-keygen -t rsa -C &quot;邮箱&quot; 在C盘本地账号下有一个文件夹.ssh 里面有两个文件，分别是id_rsa.pub 和 id_rsa分别对应着公钥和私钥 登陆GitHub，打开Settings里有一个选项，SSH and GPG keys添加自己机子的公钥 因为Git支持SSH协议，所以添加了SSH KEY之后可以防止别人冒充来对我们的仓库进行恶意修改 添加文件到GitHub首先在GitHub里面创建一个Repository 比如说我创建了一个gitTest 然后回到git Bash添加远程仓库1git remote add origin git@github.com:用户名/仓库名字.git 那么对应删除远程仓库的命令：1git remote rm origin 现在我们添加成功之后，接下来就是要上传到远程仓库里面去了 1git push -u origin master 接着去刷新我们的仓库，发现文件已经提交上去了 GitHub上下载文件这个也很简单对应着一条命令就可以了 1git clone git@github.com:用户名/仓库名字.git 好了，本章对Git的介绍就讲到这里，这里只是入门，后面会有一些更加多的内容介绍，敬请关注~","categories":[{"name":"技术分享","slug":"技术分享","permalink":"http://yoursite.com/categories/技术分享/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"}]},{"title":"Python爬取网易云音乐","slug":"Python爬取网易云音乐","date":"2019-01-12T10:20:26.000Z","updated":"2019-08-30T06:58:33.205Z","comments":true,"path":"2019/01/12/Python爬取网易云音乐/","link":"","permalink":"http://yoursite.com/2019/01/12/Python爬取网易云音乐/","excerpt":"","text":"Python爬取网易云音乐首先放网址： https://music.163.com/ 通过Network我们可以找到我们的音乐url存放的位置 那么我们就简单啦，知道Ajax的请求页面，我们当然就可以直接爬取了，但是： 这个FormData好像不简单，那我怎么请求呢？ 第一直觉，就感觉是被加密了，不愧是网易云，有一套呢 那么肯定就是和JS脱离不了关系了，找到JS，然后保存到本地进行分析一下 把代码保存到本地，进行一些操作 加密用到了四个参数，那么我们可以打印一下这四个参数 那么问题来了，这JS文件在我的本地，我怎么让网站进行加载呢？ 对的，这里要运用到一个工具，Fiddler4 这是个什么东西呢 它能够记录并检查所有你的电脑和互联网之间的Http通讯，设置断点，查看所有的进出Fiddler的数据（cookie,html,css,js） 大概就是，在客户端和服务器之间创建一个代理服务器来对之间进行交互通讯信息进行监控 下载安装完成之后还要对Fiddler4进行配置： Tools—》Options 更详细的介绍，这里就不多说了 那么大概界面是这样的 接着我们打开网易云官网 把core.js拖拉都右边，钩上相应的选项，在最下面找到要替换的JS，最后点击一下save 就能发现上面的路径变了，上图是我已经替换好的了 接下来：在网易云上搜索一首歌，打开控制台 你会发现，居然这样子了： 打印成功了！ 再看XHR里面 我们的重点是url这个，所以我们只用关注第一个打印的，显然ids就是歌曲的序号 多试几组可以看出来一个问题就是： 后三个参数是不用管的 那么歌曲的序号又要怎么获取呢？最终找到的结果是在 然而这个页面也是加密的，很强，没事 我们再看看后台打印的东西 span class = “s-fc7”又是什么东西呢？ 经过测试，发现这是固定的值 整个Json不同的地方在于s，传入歌名就可以了 呼 offset是偏移量，与翻页数有关系 好了，接下来又得去看我们的JS代码，去分析加密过程了 1234567891011121314151617181920212223242526272829303132333435363738function a(a) &#123; var d, e, b = &quot;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789&quot;, c = &quot;&quot;; for (d = 0; a &gt; d; d += 1) e = Math.random() * b.length, e = Math.floor(e), c += b.charAt(e); return c &#125; function b(a, b) &#123; var c = CryptoJS.enc.Utf8.parse(b) , d = CryptoJS.enc.Utf8.parse(&quot;0102030405060708&quot;) , e = CryptoJS.enc.Utf8.parse(a) , f = CryptoJS.AES.encrypt(e, c, &#123; iv: d, mode: CryptoJS.mode.CBC &#125;); return f.toString() &#125; function c(a, b, c) &#123; var d, e; return setMaxDigits(131), d = new RSAKeyPair(b,&quot;&quot;,c), e = encryptedString(d, a) &#125; function d(d, e, f, g) &#123; var h = &#123;&#125; , i = a(16); return h.encText = b(d, g), h.encText = b(h.encText, i), h.encSecKey = c(i, e, f), h &#125; function e(a, b, d, e) &#123; var f = &#123;&#125;; return f.encText = c(a + e, b, d), f &#125; window.asrsea = d 首先看函数d 函数d首先有个i，这个i是一个随机的十六位字符串， 然后进行了两次加密，第一次是第一个参数和第四个参数进行加密，把结果返回出来后与i字符串进行第二次加密，这个encText就是我们的params 而通过我们的刚才打印结果来看，后三个参数是固定的，然而i是随机的，也就是我们可以固定一个参数（可以想象成就是每次随机刚好就是随机到我的字符串），也就是说h.encSecKey=c(i,e,f)也是固定的，那也没什么好看的了 最主要的还是我们的params参数的第一次加密，因为第二次加密是在第一次加密的结果和一个固定的字符串，所以也没有讨论的必要了 首先看看我们的加密算法： 12345678910#AES加密算法def AES_encrypt(text, key, iv): pad = 16 - len(text) % 16 if type(text)==type(b&apos;&apos;): text = str(text, encoding=&apos;utf-8&apos;) text = text + pad * chr(pad) encryptor = AES.new(key, AES.MODE_CBC, iv) encrypt_text = encryptor.encrypt(text) encrypt_text = base64.b64encode(encrypt_text) return encrypt_text 这里，要安装一下Crypto模块，不然会报错，模块找不到 接下来就是源代码了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596from Crypto.Cipher import AESimport requestsimport base64import osimport codecsimport jsonfrom pypinyin import lazy_pinyinfrom urllib.request import urlretrieve# 后三个参数和i的值（随机的十六位字符串）b = '010001'c = '00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7'd = '0CoJUm6Qyw8W8jud'#随机的十六位字符串def createSecretKey(size): return (''.join(map(lambda xx: (hex(ord(xx))[2:]), str(os.urandom(size)))))[0:16]#AES加密算法def AES_encrypt(text, key, iv): pad = 16 - len(text) % 16 if type(text)==type(b''): text = str(text, encoding='utf-8') text = text + str(pad * chr(pad)) encryptor = AES.new(key.encode(\"utf8\"), AES.MODE_CBC, iv.encode(\"utf8\")) encrypt_text = encryptor.encrypt(text.encode(\"utf8\")) encrypt_text = base64.b64encode(encrypt_text) return encrypt_text#得到第一个加密参数def Getparams(a,SecretKey): #0102030405060708是偏移量，固定值 iv = '0102030405060708' h_encText = AES_encrypt(a,d,iv) h_encText = AES_encrypt(h_encText,SecretKey,iv) return h_encText#得到第二个加密参数def GetSecKey(text, pubKey, modulus): # 因为JS做了一次逆序操作 text = text[::-1] rs = int(codecs.encode(text.encode('utf-8'), 'hex_codec'), 16) ** int(pubKey, 16) % int(modulus, 16) return format(rs, 'x').zfill(256)#得到表单的两个参数def GetFormData(a): SecretKey = createSecretKey(16) params = Getparams(a,SecretKey) enSecKey = GetSecKey(SecretKey,b,c) data = &#123; \"params\":str(params,encoding='utf-8'), \"encSecKey\":enSecKey &#125; return datadef getOnePatam(): # 查询id的url url = 'https://music.163.com/weapi/cloudsearch/get/web?csrf_token=' #伪装头部 head = &#123; 'Host': 'music.163.com', 'Origin':'https://music.163.com', 'Referer':'https://music.163.com/search/', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36', &#125; print(\"输入你想要下载的歌手\") song_name = input() #第一个参数 song_name = ''.join(lazy_pinyin(song_name)) key = '&#123;hlpretag:\"\",hlposttag:\"&lt;/span&gt;\",s:\"'+song_name+'\",type:\"1\",csrf_token:\"\",limit:\"30\",total:\"true\",offset:\"0\"&#125;' FormData = GetFormData(key) html = requests.post(url,headers=head,data=FormData) result = json.loads(html.text) return result['result']['songs']#下载器：def download(name,id): # 获取歌曲的url的路径 song_url = \"https://music.163.com/weapi/song/enhance/player/url?csrf_token=\" # 伪装头部 headers = &#123; 'Host': 'music.163.com', 'Origin': 'https://music.163.com', 'Referer': 'https://music.163.com/', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36' &#125; # 把上个页面查询到的id放到第二个页面的第一个参数上 a =str(&#123;'ids': \"[\"+str(id)+\"]\", 'br': 320000, 'csrf_token': \"\"&#125;) FormData = GetFormData(a) response = requests.post(song_url,data = FormData,headers=headers) json_dict = json.loads(response.content) song_url=json_dict['data'][0]['url'] print(song_url) folder = os.path.exists('songs') if not folder: os.makedirs('songs') path = os.path.join('songs',name+\".mp3\") urlretrieve(song_url,filename=path)if __name__ == '__main__': song_list = getOnePatam() for i in song_list: name = i['name'] id = i['id'] download(name,id) 以下是效果图 这次程序重点的是对加密的网页，学会如何去处理 有一点要强调一下，因为如果是中文，会导致加密的时候字符串长度不匹配的问题，所以只能用拼音，所以这里加了一个中文转拼音的库，pypinyin的lazy_pinyin()的方法","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/categories/爬虫/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"Solr集群版的搭建","slug":"Solr集群版的搭建","date":"2019-01-10T10:35:59.000Z","updated":"2019-08-25T00:48:17.256Z","comments":true,"path":"2019/01/10/Solr集群版的搭建/","link":"","permalink":"http://yoursite.com/2019/01/10/Solr集群版的搭建/","excerpt":"","text":"Solr集群版的搭建搭建Solr集群 首选需要配置zookeeper集群 先搭建四个tomcat 1234cp -r apache-tomcat-7.0.47 /home/quan/app/solr-cloud/tomcat01cp -r apache-tomcat-7.0.47 /home/quan/app/solr-cloud/tomcat02cp -r apache-tomcat-7.0.47 /home/quan/app/solr-cloud/tomcat03cp -r apache-tomcat-7.0.47 /home/quan/app/solr-cloud/tomcat04 1234vim tomcat01/conf/server.xmlvim tomcat02/conf/server.xmlvim tomcat03/conf/server.xmlvim tomcat04/conf/server.xml 给不同的Tomcat修改端口，tomcat01为81，tomcat02为82,以此类推，主要修改以下三个地方 把单机版的Solr放到Tomcat下 1234cp -r solr /home/app/solr-cloud/tomcat01/webapps/cp -r solr /home/app/solr-cloud/tomcat02/webapps/cp -r solr /home/app/solr-cloud/tomcat03/webapps/cp -r solr /home/app/solr-cloud/tomcat04/webapps/ 再复制四个solrhome到solr-cloud目录下 再修改每个solrhome下的solr.xml 注意写自己的端口号和IP地址 修改每个tomcat的solr的web.xml 修改catalina文件 1234vim tomcat01/bin/catalina.shvim tomcat02/bin/catalina.shvim tomcat03/bin/catalina.shvim tomcat04/bin/catalina.sh tomcat运行catalinda.sh脚本命令 配置JAVA_OPTS 1JAVA_OPTS=&quot;-DzkHost=192.168.25.130:2181,192.168.25.130:2182,192.168.25.130:2183&quot; 在solr文件下的example目录下的script目下的cloud-scripts目录下有个zkcli.sh 上传配置文件至zookeeper的命令 1./zkcli.sh -zkhost 192.168.25.130:2181,192.168.25.130:2182,192.168.25.130:2183 -cmd upconfig -confdir /home/quan/app/solr-cloud/solrhome01/collection1/conf/ -confname myconf 进入solr-cloud的zookeeper01中的bin目录下进行连接zookeeper 1./zkCli.sh -server 192.168.25.130:2182 检查一下 1./zkCli.sh -server 192.168.25.130:2182 myconf就是我们上传的 内容里面有我们上传的solr文件 整个搭建就完成了 开启四个Tomcat 1234/home/quan/app/solr-cloud/tomcat01/bin/startup.sh/home/quan/app/solr-cloud/tomcat02/bin/startup.sh/home/quan/app/solr-cloud/tomcat03/bin/startup.sh/home/quan/app/solr-cloud/tomcat04/bin/startup.sh SolrCloud创建Collection的命令 1http://192.168.25.130:8180/solr/admin/collections?action=CREATE&amp;name=collection2&amp;numShards=2&amp;relicationFactor=2 SolrCloud删除Collection的命令 1http://192.168.25.130:8180/solr/admin/collections?action=DELETE&amp;name=collection1","categories":[],"tags":[{"name":"Solr","slug":"Solr","permalink":"http://yoursite.com/tags/Solr/"}]},{"title":"设计模式-单例","slug":"设计模式-单例","date":"2019-01-06T23:52:39.000Z","updated":"2019-08-25T00:48:13.814Z","comments":true,"path":"2019/01/07/设计模式-单例/","link":"","permalink":"http://yoursite.com/2019/01/07/设计模式-单例/","excerpt":"","text":"设计模式（Design Pattern）是一套被反复使用、多数人知晓的、经过分类的、代码设计经验的总结。 使用设计模式的目的：为了代码可重用性、让代码更容易被他人理解、保证代码可靠性。 设计模式使代码编写真正工程化；设计模式是软件工程的基石脉络，如同大厦的结构一样。 说白了就是：就是使用设计模式，代码会更好。 单例模式：就是单个实例 那么单例模式有什么好处呢？ 很简单，首先，都是单个实例了，那么就可以怎样 一、实例控制 单例模式会阻止其他对象实例化其自己的单例对象的副本，从而确保所有对象都访问唯一实例。 二、灵活性 因为类控制了实例化过程，所以类可以灵活更改实例化过程。 但是还是有缺点的： 一、开销 虽然数量很少，但如果每次对象请求引用时都要检查是否存在类的实例，将仍然需要一些开销。可以通过使用静态初始化解决此问题。 二、可能的开发混淆 使用单例对象（尤其在类库中定义的对象）时，开发人员必须记住自己不能使用new关键字实例化对象。因为可能无法访问库源代码，因此应用程序开发人员可能会意外发现自己无法直接实例化此类。 三、对象生存期 不能解决删除单个对象的问题。在提供内存管理的语言中（例如基于.NET Framework的语言），只有单例类能够导致实例被取消分配，因为它包含对该实例的私有引用。在某些语言中（如 C++），其他类可以删除对象实例，但这样会导致单例类中出现悬浮引用。。 单例模式根据实例化对象时机的不同分为两种 饿汉式 上代码 12345678910public class Single&#123; /*私有化，防止外部创建实例*/ private Single&#123;&#125; /*设置静态属性私有，防止外部通过类名访问*/ private static Single single = new Single(); /*设置静态方法*/ public static Single getInstance()&#123; return single; &#125;&#125; 这个很简单的 上面有个问题，当类加载的时候，就会实例化Single（），假如我加载完类，一段时间没用，而实例却已经早就创建了，着就会造成一个浪费。 通过这个问题，我们可以想，当我们需要实例的时候我们才去实例化，而不是通过类加载，那么我们可以这样子做 懒汉式 12345678910111213public class Single&#123; /*私有化，防止外部创建实例*/ private Single&#123;&#125; /*设置静态属性私有,先设空，需要的时候再实例化*/ private static Single single=null; /*设置静态方法*/ public static Single getInstance()&#123; if(single==null)&#123; single = new Single() &#125; return single &#125;&#125; 这样子，就没什么问题了 但是现在再思考一个问题，万一是多线程的时候呢？这样就有问题的了 这样子我们就需要加一个锁 12345678910111213141516171819public class Single&#123; /*私有化，防止外部创建实例*/ private Single&#123;&#125; /*设置静态属性私有,先设空，需要的时候再实例化*/ private static Single single=null; /*设置静态方法*/ public static Single getInstance()&#123; /*两重判空是为了提高效率*/ if(single==null)&#123; /*上锁*/ synchronized(Single.Class)&#123; if (single==null) &#123; single = new Single() &#125; &#125; &#125; return single; &#125;&#125; 这样子整个单例模式就做得比较好了","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"Java线程","slug":"Java线程","date":"2018-12-28T19:56:18.000Z","updated":"2019-08-25T00:53:45.313Z","comments":true,"path":"2018/12/29/Java线程/","link":"","permalink":"http://yoursite.com/2018/12/29/Java线程/","excerpt":"","text":"什么是线程？ 线程是一个程序里的不同执行路径 一般的程序是从一个入口出发，沿着唯一的路径走到终点，而线程则使唯一的路径变成多条路径 以下是单线程操作 123456789101112131415161718192021class A extends Thread&#123; public void run() &#123; while(true) &#123; System.out.println(&quot;hello world&quot;); &#125; &#125;&#125;public class Thread1 &#123; public static void main(String[]args) &#123; A aa=new A(); aa.run(); while(true) &#123; System.out.println(&quot;hello JAVA&quot;); &#125; &#125;&#125; 因为aa.run没有执行完毕，下面的while循环就不会执行，所以就是一直输出“hello world“再看看多线程操作 123456789101112131415161718192021class A extends Thread&#123; public void run() &#123; while(true) &#123; System.out.println(&quot;hello world&quot;); &#125; &#125;&#125;public class Thread1 &#123; public static void main(String[]args) &#123; A aa=new A(); aa.start(); while(true) &#123; System.out.println(&quot;hello JAVA&quot;); &#125; &#125;&#125; Thread中的start方法就是创建一个线程，并且自动调用run方法，直接调用run方法是不会创建一个线程的。执行一个线程，其实就是执行一个线程里面的run方法，一个Thread对象不能调用两次start方法，否则会抛出异常。把aa.run改成aa.start结果就是两个循环交替执行，这就是多线程。单线程就是一条路径，从头到尾执行。多线程就是有多条路径，每次都可以走不同的路径。 执行aa.start并不代表aa对象的线程就立刻执行，而是得到了能够被CPU执行的资格，也就是就绪的状态。创建线程的第二种方式： 12345678910111213141516171819202122class A implements Runnable&#123; public void run() &#123; while(true) &#123; System.out.println(&quot;hello world&quot;); &#125; &#125;&#125;public class Thread2 &#123; public static void main(String[]args) &#123; A aa=new A(); Thread th=new Thread(aa); th.start(); while(true) &#123; System.out.println(&quot;hello JAVA&quot;); &#125; &#125;&#125; Thread常用方法： setName（String）设置名字 currentThread（）返回正在执行线程的对象 getName（）返回线程的名字 12345678910111213141516171819class A extends Thread&#123; public void run() &#123; System.out.println(&quot;hello world&quot;); System.out.println(Thread.currentThread().getName()); &#125;&#125;public class Thread1 &#123; public static void main(String[]args) &#123; A aa=new A(); aa.start(); System.out.println(&quot;hello JAVA&quot;); System.out.println(Thread.currentThread().getName()); &#125;&#125; 123456789101112131415161718class A extends Thread&#123; public void run() &#123; System.out.println(&quot;hello world&quot;); System.out.println(Thread.currentThread().getName()); &#125;&#125;public class Thread1 &#123; public static void main(String[]args) &#123; A aa=new A(); aa.setName(&quot;123&quot;); aa.start(); System.out.println(&quot;hello JAVA&quot;); System.out.println(Thread.currentThread().getName()); &#125;&#125; Thread的sleep方法sleep()方法导致了程序暂停执行指定的时间，让出cpu该其他线程，但是他的监控状态依然保持者，当指定的时间到了又会自动恢复运行状态。要捕获异常！ 123456789101112131415161718192021222324class A extends Thread&#123; public void run() &#123; for(int i=0;i&lt;10;i++) &#123; System.out.println(Thread.currentThread().getName()+i); try&#123; Thread.sleep(1000); &#125; catch(Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;public class Thread1 &#123; public static void main(String[]args) &#123; A aa=new A(); aa.start(); &#125;&#125; Thread的join方法：如a.join（）；暂停当前正在执行的线程，直到a的线程运行终止之后当前线程才有机会得到执行，注意：不是暂停a对象的线程，而是当前运行的线程12345678910111213141516171819202122232425262728class A extends Thread&#123; public void run() &#123; for(int i=0;i&lt;10;i++) &#123; System.out.println(Thread.currentThread().getName()+i); &#125; &#125;&#125;public class Thread1 &#123; public static void main(String[]args) &#123; A aa=new A(); aa.start(); try&#123; aa.join(); &#125; catch(Exception e) &#123; e.printStackTrace(); &#125; for(int i=0;i&lt;10;i++) &#123; System.out.println(Thread.currentThread().getName()+i); &#125; &#125;&#125; **Thread的优先级： getPriority:获取优先级 setPriority:设置优先级 Java提供一个线程调度器来监控程序中启动后进入就绪状态的所有线程。 线程调度器用数字表现，范围从一到十，一个线程默认是5。 通常优先级高的比优先级低的要先执行，但并不是一定的！因为实际开发中并不单纯依赖优先级来决定优先级的运行顺序** 1234567891011121314151617181920212223242526272829class A implements Runnable&#123; public void run() &#123; for(int i=0;i&lt;10;i++) &#123; System.out.println(&quot;A&quot;+i); &#125; &#125;&#125;class B implements Runnable&#123; public void run() &#123; for(int i=0;i&lt;10;i++) &#123; System.out.println(&quot;B&quot;+i); &#125; &#125;&#125;public class Thread3 &#123; public static void main(String[]args) &#123; Thread t1=new Thread(new A()); Thread t2=new Thread(new B()); t1.setPriority(Thread.MIN_PRIORITY); t2.setPriority(Thread.MAX_PRIORITY); t1.start(); t2.start(); &#125;&#125; 优先级越高！越容易被CPU先调用！ 线程的同步卖票系统！ 假如有三个地方，A,B,C同时卖票 假如代码写成这样 12345if(票数大于0)&#123; 买票 票数-1&#125; 当A发现票大于0的时候，本应该执行下一步，假如此时CPU切换的B线程的时候，发现票数大于0（因为在A线程里面，票数没有减一），当在B中发现票数大于0之后，假如CPU又切换到C线程里面，发现票数还是大于0（同理）假如票只有一张，那么此时就相当于一张票被卖了三次。这将产生错误！ 123456789101112131415161718192021222324252627282930class A implements Runnable&#123; private int tickets=100; public void run() &#123; while(true) &#123; if(tickets&gt;0) &#123; System.out.println(Thread.currentThread().getName()+&quot;正在卖出第&quot;+tickets+&quot;张票&quot;); tickets--; &#125; else &#123; break; &#125; &#125; &#125;&#125;public class Thread4 &#123; public static void main(String[]args) &#123; A a=new A(); Thread t1=new Thread(a); t1.start(); A b=new A(); Thread t2=new Thread(b); t2.start(); &#125;&#125; 以上代码运行结果： 每张票都被卖出去两次！！！这是不合理的 导致这个的原因是a对象和b对象都有一个属于自己的tickets 100 那么接下来看以下程序 123456789101112131415161718192021222324252627282930class A implements Runnable&#123; static int tickets=100; public void run() &#123; while(true) &#123; if(tickets&gt;0) &#123; System.out.println(Thread.currentThread().getName()+&quot;正在卖出第&quot;+tickets+&quot;张票&quot;); tickets--; &#125; else &#123; break; &#125; &#125; &#125;&#125;public class Thread4 &#123; public static void main(String[]args) &#123; A a=new A(); Thread t1=new Thread(a); t1.start(); A b=new A(); Thread t2=new Thread(b); t2.start(); &#125;&#125; 把票数改成静态的结果是这样的： 那么来分析一下这个结果是为什么，当Thread-0发现票数是100的时候执行卖出操作，然后立刻切换的线程1然后发现还是100但是没有执行卖出操作又转换为线程0，此时减一然后就变成99、98、97、96、95、这个时候立刻切换成线程1执行卖出操作，打印出来。 简单来说：CPU会在线程之间来回切换！好的，重点来了！Synchronized—同步1234567891011121314151617181920212223242526272829303132class A implements Runnable&#123; static int tickets=100; public void run() &#123; while(true) &#123; synchronized(this) &#123; if(tickets&gt;0) &#123; System.out.println(Thread.currentThread().getName()+&quot;正在卖出第&quot;+tickets+&quot;张票&quot;); tickets--; &#125; else &#123; break; &#125; &#125; &#125; &#125;&#125;public class Thread4 &#123; public static void main(String[]args) &#123; A a=new A(); Thread t1=new Thread(a); Thread t2=new Thread(a); t1.start(); t2.start(); &#125;&#125; 结果如下： synchronized 关键字，代表这个方法加锁,相当于不管哪一个线程（例如线程A），运行到这个方法时,都要检查有没有其它线程B（或者C、 D等）正在用这个方法(或者该类的其他同步方法)，有的话要等正在使用synchronized方法的线程B（或者C 、D）运行完这个方法后再运行此线程A,没有的话,锁定调用者,然后直接运行。它包括两种用法：synchronized 方法和 synchronized 块。","categories":[{"name":"技术分享","slug":"技术分享","permalink":"http://yoursite.com/categories/技术分享/"}],"tags":[{"name":"线程","slug":"线程","permalink":"http://yoursite.com/tags/线程/"}]},{"title":"PHP手动搭建环境","slug":"PHP手动搭建环境","date":"2018-12-28T19:23:13.000Z","updated":"2019-08-25T00:54:08.293Z","comments":true,"path":"2018/12/29/PHP手动搭建环境/","link":"","permalink":"http://yoursite.com/2018/12/29/PHP手动搭建环境/","excerpt":"","text":"php环境手动搭建php下载路径https://windows.php.net/download（注意一点的是要下线程安全的Thread Safe） Apache下载路径http://httpd.apache.org/docs/current/platform/windows.html Mysql下载路径https://dev.mysql.com/downloads/mysql/ Apache的安装 这里注意，请先把你的Apache的目录放置好再进行安装，否则安装后再移动位置，会出问题进入Apache目录下的conf目录修改httpd.conf大概在38行附近修改成如下：路径使用自己Apache的安装位置12Define SRVROOT &quot;E:\\Apache24&quot; ServerRoot &quot;$&#123;SRVROOT&#125;&quot; 用管理员模式开启CMD进入Apache的bin目录下 命令行httpd -k install进行安装可以使用httpd -t进行测试，是否安装成功 注意事项如果失败，导致原因有可能是： 端口被占用 解决办法： netstat -aon | findstr :80 查看端口是否正在被监听，如果被监听了，有两种方法1. 修改Apache的端口，打开Apache的conf目录下修改httpd.conf,查找Listen关键字 找到 Listen 80 修改到你想设置的端口即可 2. 停止正在监听的服务，打开资源管理器，找到对应的PID，停止运行 这里默认上面三个已经下载好而且已经安装好了 正题：Apache和PHP整合在Apache的conf目录下的httpd.conf文件中加入下面三行代码 12345678#加载PHP模块LoadModule php7_module &quot;E:/php7/php7apache2_4.dll&quot;#当执行后缀为php的文件，就去找这个模块执行AddType Application/x-httpd-php .php#载入php配置文件PHPIniDir &quot;E:/php7&quot; 路径选择你们的位置 在php目录下拷贝php.ini-development改名为php.ini在文件里面进行如下修改(目录为个人的目录)1extension_dir = &quot;E:/php7/ext&quot; 在apache里面有个htdocs目录在里面写入一个php文件比如:test.php123&lt;?php phpinfo(); ?&gt; 在浏览器输入localhost/test.php 如果能够正确显示则配置完成","categories":[{"name":"技术分享","slug":"技术分享","permalink":"http://yoursite.com/categories/技术分享/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://yoursite.com/tags/PHP/"}]},{"title":"Solr单机版的搭建","slug":"Solr单机版的搭建","date":"2018-12-27T00:35:59.000Z","updated":"2019-08-25T00:52:50.798Z","comments":true,"path":"2018/12/27/Solr单机版的搭建/","link":"","permalink":"http://yoursite.com/2018/12/27/Solr单机版的搭建/","excerpt":"","text":"Solr Solr是一个开源搜索平台，用于构建搜索应用程序。 它建立在Lucene(全文搜索引擎)之上。 Solr是企业级的，快速的和高度可扩展的。 使用Solr构建的应用程序非常复杂，可提供高性能。 solr的一些突出的特点： Restful APIs − 要与Solr通信，并非一定需要有Java编程技能。相反，您可以使用restful服务与它通信。可使用文件格式(如xml，json和.CSV)在Solr中作为输入文档，并以相同的文件格式获取结果。 全文搜索 - Solr提供了全文搜索所需的所有功能，例如令牌，短语，拼写检查，通配符和自动完成。 企业准备 - 根据企业/组织的需要，Solr可以部署在任何类型的系统(大或小)，如独立，分布式，云等。灵活和可扩展 - 通过扩展Java类并相应配置，可以轻松地定制Solr的组件。 NoSQL数据库 - Solr也可以用作大数据量级的NOSQL数据库，可以沿着集群分布搜索任务。 管理界面 - Solr提供了一个易于使用，用户友好，功能强大的用户界面，使用它可以执行所有可能的任务，如管理日志，添加，删除，更新和搜索文档。 高度可扩展 - 在使用Solr与Hadoop时，我们可以通过添加副本来扩展其容量。 以文本为中心并按相关性排序 - Solr主要用于搜索文本文档，结果根据与用户查询的相关性按顺序传送 Solr单机版的搭建：解压solr和tomcat12tar zxf solr-4.10.3.tgz.tgztar -zxf apache-tomcat-7.0.47.tar.gz 把solr的war包放到tomcat的webapp目录下1cp solr-4.10.3/dist/solr-4.10.3.war /home/quan/taotao/apache-tomcat-7.0.47/webapps/solr.war cp solr-4.10.3/example/lib/ext/* /home/quan/taotao/apache-tomcat-7.0.47/webapps/solr/WEB-INF/lib/123配置一下solrhome vim /home/quan/taotao/apache-tomcat-7.0.47/webapps/solr/WEB-INF/web.xml12修改solr的web.xml vim /home/quan/taotao/apache-tomcat-7.0.47/webapps/solr/WEB-INF/web.xml` 再重新启动一次Tomcat 在浏览器里访问一下 localhost:8080/solr会出现以下界面 整个Solr服务就启动完成了","categories":[{"name":"技术分享","slug":"技术分享","permalink":"http://yoursite.com/categories/技术分享/"}],"tags":[{"name":"Solr","slug":"Solr","permalink":"http://yoursite.com/tags/Solr/"}]},{"title":"线程和进程","slug":"线程和进程","date":"2018-12-20T12:59:30.000Z","updated":"2019-08-25T00:47:36.903Z","comments":true,"path":"2018/12/20/线程和进程/","link":"","permalink":"http://yoursite.com/2018/12/20/线程和进程/","excerpt":"","text":"计算机所有可运行的软件，通常也包括操作系统，被组织成若干顺序进程，简称进程。一个进程就是一个正在执行程序的实例 单核CPU也就是单个核心的CPU，每次只能执行一个进程，由于CPU在各进程之间快速切换，所以每个进程所执行的时间是不确定的。 举个例子： 假设你在看着食谱做美食，那么你就相当于CPU，食谱就是程序，而做美食的材料就是输入数据，进程就是，你在阅读食谱取食材以及制作美食的这一系列动作，假设你在做美食时候突然来了个电话，你可能会先熄火，然后脑海里知道自己现在做到哪个位置（保存当前状态），然后去接电话，处理完了后，你可能才回来厨房想想刚才做到哪里了，然后继续之前继续做 一个进程就是某种类型的一个活动，它有程序、输入、输出以及状态。单个处理器可以被多个进程共享，CPU使用了某种调度算法决定何时停止一个进程的工作，并且转向另一个进程提供服务。 进程的创建：有四种主要的事件1、系统初始化 前台进程 守护进程2、正在运行的程序执行了创建进程的系统调用3、用户请求创建一个新进程4、一个批处理作业的初始化 进程的终止1、正常退出2、出错退出3、严重错误4、被其他进程给杀死 只可以有一个父进程，但可以有零个或者多个子进程 进程有三种状态：运行态，就绪态，阻塞态 ​ 为了实现进程模型，操作系统维护着一张表格（一个结构数组）即进程表，每个进程占用一个进程表的一个项，这张表包含了许多信息，比如程序计数器，堆栈指针，内存分配状况，等等从而保证了该进程被断掉后重新启动的时候，能够保存之前的信息。 线程为什么需要线程？ 首先，有了线程，我们可以不必考虑终端、定时器、和上下文的切换只需考虑并行进程。其次，线程比进程更加轻量级，速度会比用进程效率要提高很多。 每个单核处理器在某个时刻也是只能够执行一个线程的，这和进程是一样的，线程是CPU处理的基本单位，我们前面讨论的进程，是进程单线程模型。 同样的，线程也是有阻塞态、运行态、就绪态。 为了实现可移植的线程程序，IEEE在IEEE标准中定义了线程的标准，它定义的线程包叫做pthread 实现线程包有两种方法，第一种把整个线程包放在用户空间 从内核的角度上管理的就是单线程进程的模型，这样子尽管系统不支持线程，也可以进行实现 用C语言实现1234567891011121314151617181920212223#include&lt;pthread.h&gt;#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#define NUMBER 10void *hello(void *id)&#123; printf(&quot;Thread-----%d\\n&quot;,id); pthread_exit(NULL);&#125;int main()&#123; pthread_t threads[NUMBER]; int status; int i; for(i=0;i&lt;NUMBER;i++)&#123; printf(&quot;Main---Creating thread%d\\n&quot;,i); status = pthread_create(&amp;threads[i],NULL,hello,(void*)i); if(status!=0)&#123; printf(&quot;ErrorCode----&gt;%d\\n&quot;,status); exit(-1); &#125; &#125; return 0;&#125; 在用户空间管理线程的时候会创建一个运行时的系统，由这个系统进行管理，每个进程都需要其专用的线程表，用来跟踪该进程中的线程。这些表和进程表相似，不过它仅仅记录的是各个线程的属性，如每个线程的程序计数器，堆栈指针，寄存器和状态。因为切换线程的时候不需要陷入到内核空间，不需要有上下文的切换，也不需要对内存高速缓存进行刷新，这就使得线程调度非常快捷。 用户线程还有一个优点，允许每个进程有自己定制的调度算法，这样子就有一个很好的可扩展性 如果某个线程阻塞了，就会导致整个进程的阻塞 在内核中实现线程 此时不再需要运行时的系统了，另外，每个进程中也没有线程表，内核中有用来记录系统中所有线程的线程表了，当某个线程希望创建一个新的线程的时候，就会进行一个系统的调用，这个系统调用通过对线程表的更新完成线程创建的工作。 内核的线程表里保存了每个线程的信息，这些信息和在用户空间中的线程是一样的，但是现在保存在内核中 由于在内核中创建线程的代价比较大，所以某些系统会采取一种方式：回收线程，当某个线程被撤销的时候，就标记为不可运行的，但是其内核数据的结构没有收到影响。再次创建一个新的线程的时候，就把这个线程给重新启动就可以了，这样子可以减少多次系统调用来开辟新的线程 混合实现 使用内核级的线程，然后将用户级的线程与某些或者全部内核线程多路复用，采取这种方式：开发人员就可以决定有多少个内核级的线程和多少个用户级线程即使多路复用，这个模型可以带来最大的灵活度 进程之间的通信比如说一个购票系统： 进程A和进程B，此时：进程A去买票（一共十张）：首先读出票数，把票数减一，然后把减一后的数据放回去 假设在第二个步骤的时候发生CPU 的切换，进程B去读票数，那么去读的时候此时票数还是10，然后进行减一后，放回去，再切回线程A，放回去，这里就有个问题：同一张票给了两个进程去卖了！这明显是不科学的。 为了有效避免进程的竞争问题这里需要做的就是互斥，那么什么是互斥呢？就是A在访问的时候，禁止B访问，这样子就能够进行一个有效的防止竞争了。 这里面设计很多种方法： 屏蔽中断：每个进程在刚刚进入临界区后立刻屏蔽所有中断，并在就有离开之前再打开终端，屏蔽中断后，时钟中断也会被屏蔽，CPU只有发生时钟中断或者其他中断才会进行切换，这样子，在屏蔽中断之后，CPU将不会进行切换 锁变量：共享一个变量（锁），初始值为0，当一个进程进入临界区的时候，就测试这把锁，如果该锁为0则进程把锁设置为1，然后进入，如果进入的时候锁的值为1，则等待，当出去临界区的时候再把锁的值改为0 严格轮换法： 严格轮换法同样也是针对一个临界区设置一个变量,假设为Turn。以两个进程为例子: 当Turn为0时,Process 0才能能进入临界区,否则等待。等Process 0离开临界区后,将Turn设置为1. 当Turn为1时,Process 1才能进入临界区,否则等待。等Process 1离开临界区后,将Turn设置为0.","categories":[{"name":"技术分享","slug":"技术分享","permalink":"http://yoursite.com/categories/技术分享/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://yoursite.com/tags/操作系统/"}]}]}